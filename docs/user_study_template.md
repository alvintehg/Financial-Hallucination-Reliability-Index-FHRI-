# User Study Template: LLM Financial Chatbot with Hallucination Detection

## Study Overview

**Study Title:** Evaluating User Trust and Satisfaction in an LLM-powered Financial Advisor with Hallucination Detection

**Study Duration:** 15-20 minutes per participant

**Target Participants:** 5-10 users with basic financial knowledge (students, early career professionals, retail investors)

**Study Goals:**
1. Measure user trust in the chatbot's responses
2. Evaluate user satisfaction with hallucination detection features
3. Assess perceived usefulness and reliability
4. Gather qualitative feedback on system improvements

---

## Study Protocol

### Pre-Study Briefing (2 minutes)

Provide participants with the following information:

> **Welcome!** You will be testing a financial chatbot that uses Large Language Models (LLMs) to answer investment-related questions. The system includes:
> - **Hallucination Detection**: Flags uncertain or potentially incorrect answers
> - **Contradiction Detection**: Warns when answers contradict previous responses
> - **Confidence Indicators**: Shows how confident the system is in its answers
>
> Your task is to interact with the chatbot naturally, ask financial questions, and provide feedback on your experience.

### Task Scenarios (10 minutes)

Ask participants to complete the following tasks using the chatbot:

#### Scenario 1: Company Performance Query
- **Task:** "Ask about Apple's recent quarterly performance"
- **Observe:** Does the user notice confidence scores? Do they trust the answer?

#### Scenario 2: Comparative Analysis
- **Task:** "Compare Microsoft and Apple's Q3 2024 performance"
- **Observe:** Does the user find the comparison helpful? Any hallucination flags?

#### Scenario 3: Financial Concept Explanation
- **Task:** "Ask what portfolio diversification means"
- **Observe:** Is the explanation clear? Does the user trust the definition?

#### Scenario 4: Follow-up Question (Contradiction Test)
- **Task:** "Ask a follow-up question that might contradict the previous answer"
- **Example:** "What about the energy sector performance?" (after asking about S&P 500)
- **Observe:** Does contradiction detection work? Is it helpful?

#### Scenario 5: Out-of-Knowledge Query
- **Task:** "Ask about something the system might not know"
- **Example:** "What was Tesla's stock price yesterday?"
- **Observe:** Does the system admit lack of knowledge? Hallucination detection?

---

## Questionnaire

### Part A: Demographic Information

1. **What is your age range?**
   - [ ] 18-24
   - [ ] 25-34
   - [ ] 35-44
   - [ ] 45-54
   - [ ] 55+

2. **What is your background?**
   - [ ] Student (Finance/Business)
   - [ ] Student (Non-Finance)
   - [ ] Early Career Professional
   - [ ] Experienced Professional
   - [ ] Retail Investor
   - [ ] Other: ___________

3. **How familiar are you with financial concepts?**
   - [ ] Not familiar (1)
   - [ ] Slightly familiar (2)
   - [ ] Moderately familiar (3)
   - [ ] Very familiar (4)
   - [ ] Expert (5)

4. **Have you used financial chatbots or robo-advisors before?**
   - [ ] Yes, frequently
   - [ ] Yes, occasionally
   - [ ] Yes, once or twice
   - [ ] No, never

---

### Part B: Trust and Reliability (5-point Likert Scale)

**Instructions:** Rate the following statements on a scale from 1 (Strongly Disagree) to 5 (Strongly Agree).

| # | Statement | 1 (Strongly Disagree) | 2 (Disagree) | 3 (Neutral) | 4 (Agree) | 5 (Strongly Agree) |
|---|-----------|----------------------|--------------|-------------|-----------|-------------------|
| 1 | I trust the chatbot's financial advice | [ ] | [ ] | [ ] | [ ] | [ ] |
| 2 | The chatbot's answers seem accurate and reliable | [ ] | [ ] | [ ] | [ ] | [ ] |
| 3 | The hallucination detection feature increases my trust | [ ] | [ ] | [ ] | [ ] | [ ] |
| 4 | The confidence scores help me judge answer quality | [ ] | [ ] | [ ] | [ ] | [ ] |
| 5 | I would use this chatbot for investment research | [ ] | [ ] | [ ] | [ ] | [ ] |
| 6 | The contradiction warnings are helpful | [ ] | [ ] | [ ] | [ ] | [ ] |
| 7 | The chatbot admits when it doesn't know something | [ ] | [ ] | [ ] | [ ] | [ ] |

---

### Part C: Usability and Satisfaction (5-point Likert Scale)

| # | Statement | 1 (Strongly Disagree) | 2 (Disagree) | 3 (Neutral) | 4 (Agree) | 5 (Strongly Agree) |
|---|-----------|----------------------|--------------|-------------|-----------|-------------------|
| 8 | The chatbot is easy to use | [ ] | [ ] | [ ] | [ ] | [ ] |
| 9 | The responses are clear and understandable | [ ] | [ ] | [ ] | [ ] | [ ] |
| 10 | The system responds quickly enough | [ ] | [ ] | [ ] | [ ] | [ ] |
| 11 | The interface is intuitive | [ ] | [ ] | [ ] | [ ] | [ ] |
| 12 | Overall, I am satisfied with this chatbot | [ ] | [ ] | [ ] | [ ] | [ ] |

---

### Part D: Feature-Specific Questions

13. **Did you notice the hallucination detection feature during your interaction?**
    - [ ] Yes, I noticed it and understood what it means
    - [ ] Yes, I noticed it but wasn't sure what it meant
    - [ ] No, I didn't notice it

14. **Did any of your queries trigger a hallucination warning?**
    - [ ] Yes, and it seemed accurate (the answer was indeed uncertain)
    - [ ] Yes, but it seemed like a false alarm (the answer looked fine)
    - [ ] No, no hallucination warnings appeared
    - [ ] I'm not sure

15. **Did you encounter any contradiction warnings?**
    - [ ] Yes, and it was helpful
    - [ ] Yes, but it seemed unnecessary
    - [ ] No, no contradiction warnings appeared
    - [ ] I'm not sure

16. **How useful are the confidence scores (entropy-based indicators)?**
    - [ ] Very useful (5)
    - [ ] Somewhat useful (4)
    - [ ] Neutral (3)
    - [ ] Not very useful (2)
    - [ ] Not useful at all (1)

17. **Compared to a regular chatbot without hallucination detection, how much do you trust this system?**
    - [ ] Much more trustworthy (+2)
    - [ ] Somewhat more trustworthy (+1)
    - [ ] About the same (0)
    - [ ] Less trustworthy (-1)
    - [ ] Much less trustworthy (-2)

---

### Part E: Open-Ended Feedback

18. **What did you like most about the chatbot?**

    _____________________________________________________________
    _____________________________________________________________
    _____________________________________________________________

19. **What did you like least or find confusing?**

    _____________________________________________________________
    _____________________________________________________________
    _____________________________________________________________

20. **Did you encounter any errors, incorrect answers, or unexpected behavior?**

    _____________________________________________________________
    _____________________________________________________________
    _____________________________________________________________

21. **How could the hallucination detection feature be improved?**

    _____________________________________________________________
    _____________________________________________________________
    _____________________________________________________________

22. **Would you recommend this chatbot to others? Why or why not?**

    _____________________________________________________________
    _____________________________________________________________
    _____________________________________________________________

23. **Any additional comments or suggestions?**

    _____________________________________________________________
    _____________________________________________________________
    _____________________________________________________________

---

## Post-Study Debrief (2 minutes)

Thank the participant and explain:

> **Thank you for participating!** This study helps us understand how users interact with AI-powered financial advisors and whether hallucination detection features improve trust and usability.
>
> **Note:** This is a research prototype. Always consult licensed financial advisors for investment decisions.

---

## Data Analysis Plan

### Quantitative Analysis

1. **Trust Score:** Average of Q1-Q7 (Part B)
2. **Usability Score:** Average of Q8-Q12 (Part C)
3. **Feature Usefulness:** Q16 scores
4. **Comparative Trust:** Q17 scores

### Statistical Tests

- **Descriptive Statistics:** Mean, median, standard deviation for all Likert scale questions
- **Trust Comparison:** t-test or Wilcoxon signed-rank test for Q17 (comparing trust with vs. without detection)
- **Correlation Analysis:** Correlation between familiarity (Q3) and trust (Q1-Q7)
- **Feature Effectiveness:** Chi-square test for Q13-Q15 (detection feature usage)

### Qualitative Analysis

- **Thematic Coding:** Categorize open-ended responses (Q18-Q23) into themes:
  - Positive aspects (e.g., "transparency", "accuracy", "easy to use")
  - Negative aspects (e.g., "confusing warnings", "slow responses")
  - Improvement suggestions (e.g., "better explanations", "more data")

---

## Sample Size Justification

**Target:** 5-10 participants

**Rationale:**
- For qualitative usability studies, 5 users can identify ~85% of usability issues (Nielsen Norman Group)
- For preliminary trust evaluation, 8-10 participants provide sufficient insights for FYP-level research
- Larger studies (n>30) are ideal for publication but not required for undergraduate projects

---

## Google Forms Template Link

**Option 1: Create in Google Forms**

1. Go to [Google Forms](https://forms.google.com)
2. Create a new form titled "LLM Financial Chatbot User Study"
3. Copy questions from Parts A-E above
4. Set up Likert scale questions as "Multiple choice grid"
5. Set up open-ended questions as "Paragraph text"

**Option 2: Microsoft Forms**

Same process using [Microsoft Forms](https://forms.office.com)

---

## Consent Form (Include at Beginning)

> **Research Study Consent Form**
>
> **Study Title:** Evaluating User Trust in an LLM-powered Financial Advisor with Hallucination Detection
>
> **Researcher:** [Your Name], [University Name]
>
> **Purpose:** This study evaluates user experience with an AI-powered financial chatbot prototype.
>
> **Procedures:** You will interact with the chatbot for 10 minutes and complete a questionnaire (5 minutes).
>
> **Risks:** Minimal. This is not real financial advice. No personal or financial data will be collected.
>
> **Benefits:** You will help improve AI transparency and trustworthiness in financial applications.
>
> **Confidentiality:** Your responses are anonymous. No identifying information will be collected.
>
> **Voluntary Participation:** You may withdraw at any time without penalty.
>
> **Contact:** For questions, contact [Your Email]
>
> - [ ] I consent to participate in this study
> - [ ] I do NOT consent

---

## Tips for Conducting the Study

1. **Pilot Test:** Test the questionnaire with 1-2 friends first to catch issues
2. **Think-Aloud Protocol:** Ask participants to verbalize their thoughts while using the chatbot
3. **Screen Recording:** Record interactions (with consent) for qualitative analysis
4. **Neutral Facilitation:** Don't bias participants by explaining how features "should" work
5. **Note Observations:** Record body language, confusion, surprises during interaction
6. **Time Management:** Keep sessions under 20 minutes to maintain engagement

---

## Expected Results for FYP Report

Based on this study, you should be able to report:

1. **Trust Scores:** Mean trust score with/without hallucination detection (with statistical test)
2. **Feature Adoption:** % of users who noticed and understood hallucination warnings
3. **User Satisfaction:** Overall satisfaction score (Q12 average)
4. **Qualitative Themes:** 3-5 key themes from open-ended responses
5. **Recommendations:** User-driven improvements for future iterations

---

**Good luck with your user study!** ðŸŽ“
