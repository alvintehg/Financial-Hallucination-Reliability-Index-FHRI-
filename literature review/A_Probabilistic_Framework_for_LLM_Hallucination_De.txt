=== Page 1 ===
A Probabilistic Framework for LLM Hallucination
Detection via Belief Tree Propagation
Bairu Hou
UC Santa Barbara
bairu@ucsb.eduYang Zhang
MIT-IBM Watson AI Lab
yang.zhang2@ibm.comJacob Andreas
MIT CSAIL
jda@mit.eduShiyu Chang
UC Santa Barbara
chang87@ucsb.edu
Abstract
This paper focuses on the task of hallucination detection, which aims to determine
the truthfulness of LLM-generated statements. To address this problem, a popular
class of methods utilize the LLM‚Äôs self-consistencies in its beliefs in a set of logi-
cally related augmented statements generated by the LLM, which does not require
external knowledge databases and can work with both white-box and black-box
LLMs. However, in many existing approaches, the augmented statements tend to be
very monotone and unstructured, which makes it difficult to integrate meaningful
information from the LLM beliefs in these statements. Also, many methods work
with the binarized version of the LLM‚Äôs belief, instead of the continuous version,
which significantly loses information. To overcome these limitations, in this paper,
we propose Belief Tree Propagation ( BTP ROP), a probabilistic framework for LLM
hallucination detection. BTP ROP introduces a belief tree of logically related state-
ments by recursively decomposing a parent statement into child statements with
three decomposition strategies, and builds a hidden Markov tree model to integrate
the LLM‚Äôs belief scores in these statements in a principled way. Experiment results
show that our method improves baselines by 3%-9% (evaluated by AUROC and
AUC-PR) on multiple hallucination detection benchmarks. Code is available at
https://github.com/UCSB-NLP-Chang/BTProp .
1 Introduction
Large language models (LLMs) have shown impressive performance across various domains. Existing
LLMs are recognized for their ability to follow human instruction and generate outputs that are both
grammatically correct and contextually relevant. However, a notable limitation is their occasional
divergence from factual accuracy and real-world knowledge. This issue, commonly referred to as
‚Äúhallucination‚Äù, significantly undermines the reliability and trustworthiness of LLMs.
Various methods have been developed to detect hallucinated statements generated by LLMs. One
naive solution is to directly prompt the LLM to output confidence scores for the truthfulness of the
statements (referred as ‚Äúmodel beliefs" in this paper and prior work [ 1‚Äì3]). However, due to the
LLM‚Äôs limited knowledge and poor calibration of the confidence estimation, such beliefs may be
mistaken. To resolve this problem, one important line of methods bases the truthfulness decision
on the LLM‚Äôs internal knowledge as well as logical consistency[ 1‚Äì7]. More specifically, these
approaches typically involve two steps. First, generate an augmented set of statements that are related
to the target statements. The augmented statements can be premises or contradictory statements of the
target statements [ 3,8], or extended passages in the same topic of the target statements [ 4].Second,
ask the LLM to make correctness judgments for every statement in the augmented set, and then
integrate these judgments to determine the truthfulness of the target statement based on the logical
relationships between the augmented statements and the target statement.
Preprint. Under review.arXiv:2406.06950v1  [cs.CL]  11 Jun 2024

=== Page 2 ===
‚ë†The top of Mount Everest has lower atmospheric pressure compared to sea level.‚ë°The freezing point of water can be as low as -40¬∞C at the top of Mount Everest.‚ë¢Lower pressure can decrease the boiling point of water, not the freezing point‚ë£The low pressure does not alter the fundamental chemical properties of water that determine its freezing point.‚ë§The freezing point of water can be as low as -2¬∞C at the top of Mount Everest.
‚ì™At the top of Mount Everest, where the atmospheric pressure is much lower than at sea level, the freezing point of water is as low as -40¬∞C.
An Example Belief TreeFHallucinated belief0.9TFTTF0.91.01.00.90.9‚ì™‚ë†‚ë°‚ë¢‚ë£‚ë§Hidden variable (Truth value)Observed variable (confidence)F0.9Figure 1: An example constructed belief tree.To intuitively understand the idea be-
hind such methods, consider the sim-
plest case where there are two con-
tradictory statements. If the LLM
believes both statements are true, it
would lead to a logical inconsistency
and indicate that one of the decisions
is flawed. By having a large number
of augmented statements, we could
then gather rich information about
the LLM‚Äôs beliefs and correct the po-
tential decision mistakes by resolv-
ing the logical inconsistencies. Com-
pared to other hallucination detection
methods, such as retrieval-augmented
methods [ 9‚Äì11] and decisions based
on LLM‚Äôs internal states [ 12‚Äì14],
such consistency-based methods do
not rely on external datasets and can work with both black-box and white-box LLMs, and thus gains
wide popularity in the research community.
With the same intuition, we propose Belief Tree PROPagation ( BTP ROP), a probabilistic framework
that leverages logical consistency across model‚Äôs beliefs for LLM hallucination detection. Specifically,
given a statement from the LLM‚Äôs generation, BTP ROP first generates a belief tree of logically
connected statements by recursively decomposing a parent statement into child statements. As
the example shown in Figure 1, the root node of the belief tree is one sentence generated by
the LLM about the freezing point of water from the FELM [15] dataset. Each child node is either
supportive or contradictory to its parent node. To diversify the statements and the logical relationships,
BTP ROP introduces three types of decomposition paradigms. After that, BTP ROP builds a directed
graphical model (a ‚Äúhidden Markov tree") [ 16,17] in which the LLM‚Äôs confidence scores are modeled
as observed variables, and the ground-truth truthfulness of the statements as the corresponding hidden
variables. Thus the truthfulness decision is turned into a standard principled probabilistic inference
problem. By estimating the emission probabilities and transition probabilities properly, we can do
inference in this hidden Markov tree model to correct the potential wrong beliefs of LLMs.
Compared to existing hallucination detection methods [ 4,7] that directly sample additional responses
from the LLM as augmentations, our method organize more diverse statements into a tree structure
to better utilize the model‚Äôs beliefs. Furthermore, our method can integrate the model beliefs in a
principled and probabilistic manner. Experiment results show that our method achieves state-of-the-
art performance, improving the hallucination detection performance (AUROC and AUC-PR) by 3% -
9% on FELM-Science [15] and FactCheckGPT [18] datasets.
2 Related Work
Hallucinations in LLMs. Hallucination has become a prominent topic in the research of LLMs [ 12,
19‚Äì22]. In existing literature, hallucination typically refers to the phenomenon where LLMs generate
nonfactual, nonsensical, or unfaithful outputs. Hallucinations have been observed in various tasks
such as text summarization [ 23‚Äì26], question answering [ 21], and machine translation [ 27‚Äì30].
Prior work [ 31,32,32] characterizes hallucination into two main types: factuality hallucination
and faithfulness hallucination. Factuality hallucinations refer to outputs that are inconsistent with
real-world facts or cannot be verified. In comparison, faithfulness hallucinations shift to the generated
content that deviates from the user‚Äôs instruction or user-provided contextual information. In this
paper, we mainly focuses on factuality hallucinations and aim to better leverage the LLM‚Äôs intrinsic
capabilities to detect such nonfactual statements within their generations.
Hallucination detection. Existing approaches to detect factuality hallucination can be divided
into two categories. The retrieval-based methods [ 10,11,33‚Äì38] involve comparing the LLM‚Äôs
output with information retrieved from a reliable knowledge base to assess its factuality. However,
these methods depend heavily on external knowledge bases, making them less flexible.To detect
2

=== Page 3 ===
hallucinations when an external knowledge base is inaccessible, another category of methods lever-
ages the model‚Äôs intrinsic capabilities. Among these intrinsic methods, some approaches leverage
the reasoning ability of LLMs to detect and reduce hallucinations [ 15,20] via chain-of-thought
prompting [ 39]. Uncertainty-based methods perform token-level or sentence-level uncertainty quan-
tification [ 18,40] and use the uncertainty to predict whether the model‚Äôs generation is non-factual.
When the model‚Äôs parameters and hidden states are accessible, probing these hidden states has also
been shown to be effective in detecting hallucinations [ 12‚Äì14]. The sampled-based methods (or
consistency-based methods) [ 4,6,7] sample multiple additional responses to the user prompt and
then perform consistency checks between the augmented responses and the original model generation.
Inconsistencies are then used to detect hallucinations. Our method also aims to detect hallucinations
using the model‚Äôs intrinsic capabilities and is closely related to the consistency-based methods. In-
stead of sampling additional responses and performing unstructured consistency checks, our method
generates logically-related statements organized in a tree structure. We further propose a probabilistic
framework based on the hidden Markov tree model to check consistency and detect hallucinations.
Improving factuality via logical consistency. A growing body of research explores improving
LLMs‚Äô factuality using the logical consistency across their beliefs [ 1‚Äì3,5,8,41,42]. Starting from an
initial text ( e.g., a statement requiring truthfulness assessment or a question), these methods identify
additional texts that are logically connected to the initial text and organize them in tree or graph
structures. The inconsistencies of model‚Äôs beliefs on the factual correctness of these texts are resolved
based on the inferential relations among these texts. One main limitation is that they treat the beliefs
(LM-generated truth values or probabilities) as calibrated, which is generally not the case. Motivated
by this, we build a probabilistic framework to integrate the model‚Äôs beliefs and figure out the most
possible and reliable correctness evaluation of the initial text.
3 Methodology
In this section, we will describe the details of the proposed method, which leverages the inconsistency
in model beliefs for hallucination detection.
3.1 Problem Formulation
Given a statement vfrom the LLM-generated text, e.g.,‚ÄòA star‚Äôs temperature is determined by the
amount of mass and energy it has‚Äô , and an LLM, our primary goal is to use the LLM to gauge the
correctness of the statement. Many previous works use simple prompting techniques to directly
elicit the LLM‚Äôs belief of the statement, whose output would typically be in the form of confidence
scores between [0,1]. For example, one approach is to directly ask the LLM whether it believes this
statement to be true. By sampling multiple answers from the LLM, we can obtain the confidence
score as the percentage of the answers that say ‚Äòyes‚Äô .
Denote Svas LLM‚Äôs confidence score regarding statement vreflecting the LLM‚Äôs original belief of
the statement. Then Svshould be close to one if the LLM believes the statement is true, and zero
otherwise. However, due to LLM‚Äôs limited knowledge and capability, Svdoes not always align with
the actual truthfulness of the statement ‚Äì the scores could be high for some false statements, and low
for true ones. In this paper, we will seek to improve the truthfulness judgment with the LLM over its
confidence score Sv.
3.2 Our Method: An Intuitive Overview
The basic idea of our approach is to construct a belief tree , denoted as T, where the root node is the
target statement, each child node is a statement logically related to the parent node, and each edge
represents the logical connection between the two nodes. We then obtain the confidence scores of all
the nodes and use the logical consistency among the scores to correct any potential mistakes in the
scores.
Figure 2 shows an example, where the target statement is ‚ÄòA star‚Äôs temperature is determined by
the amount of mass and energy it has‚Äù . This statement is true but assume that the LLM produces
a low confidence score, 0.1, for the statement. To correct this error, we can construct a belief tree
Tby generating two child nodes from the target node, where the first, which is true, is entailed by
3

=== Page 4 ===
the target statement and the second, which is false, contradicts the target statement. Assume that the
LLM correctly assigns a high confidence score to the former and a low score to the latter. In this
case, we can easily spot that the belief of the target node is logically inconsistent with those of the
child nodes, and that increasing the confidence score of the target node would resolve the logical
inconsistency, hence correcting the mistake.
Stars with lower mass will have lower temperatures than more massive ones.External factors such as a surrounding nebula determines the temperature.0.90.1A star's temperature is determined by the amount of mass and energy it has.0.1EntailsContradictsPotential Hallucination
Figure 2: Motivating example for the proposed method.Of course, the example above shows the
simplest case where the LLM‚Äôs beliefs in
the child nodes are all correct, whereas, in
reality, the child nodes‚Äô beliefs may as well
be wrong. To mitigate these additional er-
rors, we can construct a much larger belief
tree with greater depth and many different
statements. Assuming the errors in the con-
fidence scores are sporadic, we can then expect to correct most of these errors, in both the target
and child nodes, by taking into account all the confidence scores and their logical consistency. An
example of such a full belief tree is shown in Figure 3.
As implied by the discussion above, our algorithm consists of two components: ‚ù∂Constructing the
belief tree, and ‚ù∑inferring the truthfulness of the target statement by integrating the confidence
scores across the belief tree. Section 3.3 will discuss the former and Sections 3.4-3.6 the latter.
3.3 Belief Tree Construction
Since the belief tree Tconsists of nodes as statements and edges as the logical relationships between
parent and child statements, the construction of the belief tree iterates between the following two
steps. Step 1: Given a statement as the parent node, generate a set of logically connected statements
as the children nodes. Step 2: Determine the logical connection between the children nodes and
parent node. The iterative process starts with the target statement as the root node, and terminates
when the maximum tree depth is reached. Each step is detailed below.
Generating Child Statements The success of our algorithm hinges on the diversity of the generated
statements. Hence, we instruct the LLM to choose from three types of statement generation strategies.
Strategy 1: Statement Decomposition. For statements containing multiple facts/claims, we can
decompose them into individual sub-statements. Figure 1 shows an example where the target
statement ( Node 0 ) contains multiple facts (the atmosphere pressure level and the freezing point of
water on Mound Everest), which is then decomposed into two statements, one on the atmosphere
pressure level and one on the freezing point of water. Statement decomposition can be achieved by
prompting the LLM with in-context examples, as listed in Appendix A.1.
Strategy 2: Supportive and Contradictory Premises. In this strategy, the LLM is instructed to
generate a set of premises that are each supportive or contradictory to the parent claim. When
verifying the truthfulness of Node 2 in Figure 1 about the freezing point of water, we can prompt the
model to generate contradictory premises ( Node 3 andNode 4 ) that implies the limited influence of
low pressure on the freezing point of water. By leveraging these generated premises, we can indeed
correct the model‚Äôs wrong belief on Node 2 since the model‚Äôs confidence scores on these generated
premises are high in this example.
Strategy 3: Statement Correction. In this strategy, the LLM is instructed to generate what it believes
to be corrected versions of the parent statement. The Node 5 in Figure 1 shows one example of
the statement correction process, where the statement is about the freezing point of water. Then
the corrected statements would be almost the same as the parent statement ( Node 2 ), except that
the actual temperature is replaced with alternatives that LLM believes may be true. This strategy is
implemented via a three-step prompting process. First, the LLM is prompted to generate a question
about the key information in the statement. For example, the key information in Node 2 in Figure 1
is the freezing point. Then, the LLM is instructed to answer the generated question, and multiple
answers will be sampled. Finally, the LLM is prompted to generate a corrected statement from the
original statement if it is wrong according to each sampled answer. Note that there can be multiple
corrected statements because the LLM may produce different answers when sampled multiple times.
4

=== Page 5 ===
Also, the corrected statements may include the parent statement itself if the LLM believes the parent
statement is a likely answer. More details about this process can be found in Appendix A.1.
To select the most appropriate strategies for each parent claim, we introduce the following routing
procedure. First, we always start by asking the LLM to attempt the statement decomposition
strategy. If the decomposition returns multiple statements, which indicates that the decomposition is
meaningful and successful, we would use these decomposed statements as the child statements. If the
decomposition returns only a single statement, indicating that the parent statement cannot be further
decomposed, we will then prompt the LLM to select between strategies 2 and 3 with a list of rules
and examples. The detailed prompts will be listed in Appendix A.1.
Determining Logical Relationships Given a pair of parent and child statements, uandv, we
consider the following four logical relationships: ‚ù∂Equivalence, u‚áîv, ifuentails vandventails
u;‚ù∑entailment, u‚áív, ifuentails vbutvis neutral to u;‚ù∏reverse entailment, u‚áêv, ifuis
neutral to vbutventails u; and‚ùπcontradiction, u‚áí ¬¨v(or, equivalently, v‚áí ¬¨u). Note that we
do not consider the completely neutral relationship because any statements determined as completely
neutral to their parent statement will be removed.
For the decomposition strategy (strategy 1), the child node statements are, by construction, jointly
equivalent to the parent statement. Formally, denote uas the parent node and C(u)as the set of its
child nodes, then their logical relationship is determined as u‚áî ‚à© v‚ààC(u)v.
For strategies 2 and 3, since each child statement is independently generated, we only need to
determine their individual relationship to the parent statement, rather than the joint relationship. To
this end, we leverage an off-the-shelf natural language inference (NLI) model to infer the entailment,
neutrality, or contradiction between the statements. For each pair of parent statement uand an
individual child statement v, we derive two NLI relationships, one by setting uas the premise and
vas the hypothesis, and the other with uandvswitched. The two NLI outputs are then mapped to
the aforementioned logical relationships ‚Äì (entail, entail) mapped to equivalence ;(entail, neutral)
mapped to entailment ;(neutral, entail) mapped to reverse entailment ; and any results containing
contradict in either direction mapped to contradiction . As mentioned, if the NLI module returns
(neutral, neutral) , the corresponding child statement will be discarded.
Prior Belief Estimation The last step of the belief tree construction is to estimate the model‚Äôs belief
(i.e., whether the statement is true) on each node. We adopt the simple prompting method discussed
in Section 3.1 to obtain LLM‚Äôs confidence scores. Specifically, we directly probe the LLM with the
prompt ‚Äò True or False? {target statement} ‚Äô and use the next token prediction probabilities
of the words ‚ÄòTrue‚Äô and‚ÄòFalse‚Äô to compute the model confidence. We normalize the prediction
logits of the two words to get the confidence score of that statement. The only exception is in the
case of statement correction, where the LLM is already instructed to output alternative statements it
believes to be true. As a result, we simply set the confidence score of each generated statement to
1. Our empirical analysis shows that this would greatly reduce the number of LLM queries without
deteriorating the performance.
3.4 A Hidden Markov Tree Model
After the belief tree is constructed, the next question is how to utilize the confidence scores across the
tree to better determine the truthfulness of the root statement, namely the target statement. To this
end, we introduce a hidden Markov tree model and frame the truthfulness estimation problem as a
probabilistic inference problem.
Figure 3 shows an example hidden Markov tree model built upon the belief tree, where there are two
layers of variables. The upper layer consists of the confidence score of each statement, i.e.,{Su},
which is estimated during the belief tree construction process. The lower layer consists of the binary
variables representing the actual correctness of each statement, denoted as {Zu}.Zu= 1if statement
vis correct and Zu= 0otherwise. {Su}are observed variables while {Zu}are hidden variables.
Given the above hidden Markov tree model, determining the truthfulness of the target statement can
be cast as computing the posterior probability, i.e.,p(Z0= 1|{Su}), which means the truthfulness
of the root node given the confidence scores on all the nodes in this belief tree. According to the
probability graph in figure 3, we need to estimate the following probabilities.
5

=== Page 6 ===
ùëç!ùëç"ùëç#ùëç$ùëç%An HMM on Belief Tree
ùëùùëç!=1ùë†&Prior beliefPosterior beliefBelief prop.ùëç!Ground-truth factuality (hidden)LLM‚Äôs prior belief (observed)ùëùùëçùíû"|ùëç"Transition probabilityùëùùëÜ"|ùëç"Emission probabilityùëùBelief propagationPosterior beliefùëÜ!ùëÜ"ùëÜ#ùëÜ$ùëÜ%ùëÜ!Figure 3: An example hidden Markov tree model.The first set is the emission probability ,
p(Su|Zu), which characterizes the LLM‚Äôs confi-
dence score of the statement given its underly-
ing truthfulness. The emission probability cap-
tures the quality of LLM‚Äôs original confidence
score. If the confidence score is accurate, then
p(Su|Zu= 1) will concentrate its probability
mass towards 1, and p(Su|Zu= 0) will con-
centrate its probability mass towards 0. Other-
wise, the conditional distributions will be more
spread.
The second set of distribution is the transition
probability ,p(ZC(u)|Zu), where ZC(u)represent
the set of truthfulness variables of all the child
nodes of u. The transition probability is the joint probability of the truthfulness of the child statements
given that of the parent statement, which captures the logical relationships among these statements.
The third set of distribution is the prior distribution of hidden variables, i.e.,p(Zu=z), which refers
to the probability of an arbitrary statement ugenerated by the LLM being true or false. We use
uniform prior (both 0.5 for z= 1andz= 0) in this paper.
Therefore, the process of computing the posterior probability contains two steps: ‚ù∑the estimation
step, to first estimate these probabilities on a held-out dataset. ‚ù∏the inference step, to compute the
posterior probability for each testing example. Section 3.5 will discuss how to determine the emission
and transition probabilities and Section 3.6 how to infer p(Z0= 1|{Su}).
3.5 Determining Conditional Probabilities
Determining Emission Probabilities The emission probabilities, p(Su|Zu), can be estimated from
labeled datasets. Specifically, given a dataset of statements with truthfulness labels, we can run the
LLM to obtain the confidence scores of all the statements. We can then estimate p(Su|Zu= 1) by
obtaining the empirical distribution of the confidence scores within the statements labeled as true,
andp(Su|Zu= 0) within the statements labeled as false. To obtain the empirical distribution of the
continuous confidence scores, we first quantize the support [0,1]into multiple bins, and count the
histogram of the confidence scores falling into each bin. The boundaries of each bin are listed in
Table 1.
Table 1: Empirical estimation of the emission probability on
Wikibio-GPT3 dataset [4].
Su‚àà[0.0,0.2) [0 .2,0.4) [0 .4,0.7) [0 .7,0.9) [0 .9,1.0]
Zu=True 0.12 0.05 0.10 0.08 0.65
Zu=False 0.30 0.10 0.15 0.13 0.32Determining Transition Proba-
bilities The transition probability,
p(ZC(u)|Zu), is a multivariate
Bernoulli distribution with the
support {0,1}m, where mis the size
ofC(u). As discussed in Section 3.3,
different statement generation strate-
gies have different logical relationship properties. For the statement decomposition strategy, the child
statements are always jointly equivalent to the parent statement. Therefore, when the parent claim is
true, i.e.,Zu= 1, we know that all of the child statements must be true, and thus p(ZC(u)=z|Zu= 1)
equals one when zis an all-one vector and zero when ztakes other values. Conversely, when
the parent claim is false, i.e.,Zu= 0, we can infer that at least one child statement is wrong, so
p(ZC(u)=z|Zu= 0) is set to zero when zis an all-one vector and uniformly set to 1/(2m‚àí1)when
ztakes other values.
For the other two strategies, the child statements of the same parent statement are considered
independent conditional on the parent statement. Therefore, we can decompose the probability
p(ZC(u)|Zu)intoQ
v‚ààC(u)p(Zv|Zu), where each p(Zv|Zu)is the transitional probability between
parent statement uand one child statement v.p(Zv|Zu)is determined based on the four different
possible logical relationships between uandv(recall the discussion in Section 3.3), which is listed in
Table 2, where ptandpfare two hyper-parameters.
6

=== Page 7 ===
Table 2: Transition probability given different logical relationships between two nodes. ptandpfare
set to 0.5in this paper.
u‚áîv u‚áív u‚áêv u‚áí ¬¨v
u=True u=False u=True u=False u=True u=False u=True u=False
v=True 1.0 0.0 1.0 pt pt 0.0 0.0 pt
v=False 0.0 1.0 0.0 pf pf 1.0 1.0 pf
3.6 Inferring the Posterior Probability of Z0
Inferring the posterior probability p(Z0= 1|{Su})is a standard inference problem in hidden Markov
tree models and we can directly apply the standard upward-downward algorithm [16] to efficiently
compute this probability. For brevity, we will only describe the gist of the algorithm here. More
details can be found in Appendix A.3 and previous work [16, 17].
The algorithm introduces an auxiliary conditional distribution Œ≤(z, u)‚â°p(ST(u)|Zu=z), where
z‚àà {0,1}andST(u)represents the confidence scores of all the nodes in the sub-tree whose root node
isu. By definition, the posteior probability of our interest, p(Z0= 1|{ST(0)}), can be represented as
Œ≤(z= 1,0)/(Œ≤(z= 0,0) +Œ≤(z= 1,0)). According to the Bayesian rule, we can derive a recursive
relationship between Œ≤(z, u)of a parent node uand those of the child nodes:
Œ≤(z, u) =p(Su|Zu=z)¬∑X
ZC(u)‚àà{0,1}mp(ZC(u)|Zu=z)Y
v‚ààC(u)Œ≤(Zv, v), (1)
where mis the size of C(u)(namely the number of child statements to node u). Note that the first
term on the RHS is the emission probability, and the first term inside the summation is the transition
probability, which are both already known. Therefore, Equation 1 provides a way to compute Œ≤(z,0)
recursively from the leaf nodes back to the root node. First, we compute the Œ≤(z, u)of the leaf
nodes as Œ≤(z, u) =p(Su|Zu=z). Then, we use Equation 1 to recursively compute the Œ≤(z, u)of a
parent node ufrom their child nodes, until the root node is reached. The entire process is essentially
propagating and merging the beliefs in the confidence scores from sub-trees upward to the parent
node. By the time we reach the root node, we have gathered the information of all the confidence
scores. Hence this process is also referred to as a belief propagation process.
We summarize the whole algorithm in Appendix A.4.
4 Experiments
In this section, we conduct empirical evaluations on commonly-used hallucination detection bench-
marks to demonstrate the effectiveness of the proposed method.
4.1 Experiment Configurations
Datasets We follow the previous work of hallucination detection [ 4,15] and use the follow-
ing datasets for evaluation: Wikibio-GPT3 [4],FELM-Science [15], and FactCheckGPT [43].
Wikibio-GPT3 mainly consists of biography articles generated by LLMs, whereas FELM-Science
andFactCheckGPT cover a broader range of topics such as physics, chemistry, and computer science.
Evaluation settings and metrics Following the default settings on each dataset, we conduct
hallucination detection at sentence-level on Wikibio-GPT3 andFactCheckGPT and segment-level
onFELM-Science . Our evaluation encompasses a comprehensive set of metrics, including the area
under the receiver operator characteristic curve (AUROC), area under the precision-recall curve
(AUC-PR), F1 score, and detection accuracy. As F1 score and detection accuracy evaluation require a
decision threshold, we search for the optimal threshold to maximize the F1 score for each method
and compute the two metrics. Note that when computing these metrics, we consider hallucinated
examples as positive instances, following the default configurations of these datasets. Moreover,
Wikibio-GPT3 andFELM-Science exhibit significant class imbalances, so the detection accuracy
on these datasets serve merely as reference points.
Baselines We include the following baselines for comparison. The implementation details are
available in Appendix A.1. (1) Prior confidence , which directly queries the model‚Äôs confidence
7

=== Page 8 ===
Table 3: Hallucination detection performance of different methods. We report AUROC, ROC-PR, F1
score, and detection accuracy(Acc) for all methods with two different backbone models. The best
results are highlighted in bold .
Method Backbone AUROC AUC-PR F1 Acc Backbone AUROC AUC-PR F1 Acc
Wikibio-GPT3
Prior ConfidenceGPT-3.5
-Turbo73.1 85.7 84.5 76.3Llama3-8B
Instruct71.0 85.7 85.5 75.9
Chain-of-thought 71.3 83.4 85.2 76.4 72.0 84.4 85.3 75.7
SelfCheckGPT 82.6 91.3 86.6 80.0 77.0 86.8 86.1 76.8
BTP ROP 80.7 90.4 87.6 80.4 74.0 86.2 86.1 77.5
FELM-Science
Prior Confidence
GPT-3.5
-Turbo75.5 37.2 42.1 81.6
Llama3-8B
Instruct76.1 38.2 44.0 80.6
Chain-of-thought 56.3 19.0 28.6 72.5 61.6 25.7 32.6 71.2
SelfCheckGPT 77.4 45.7 51.3 84.4 77.8 39.5 49.1 76.5
Maieutic-Prompting - - 27.2 82.6 - - 22.9 79.2
BTP ROP 79.1 52.3 56.5 81.6 77.8 48.2 51.3 82.8
FactCheckGPT
Prior Confidence
GPT-3.5
-Turbo76.0 52.6 53.6 71.5
Llama3-8B
Instruct71.9 47.6 50.8 60.5
Chain-of-thought 66.5 38.9 47.7 66.4 72.5 43.3 53.2 66.1
SelfCheckGPT 74.9 49.7 54.4 74.0 72.5 46.4 51.9 65.9
Maieutic-Prompting - - 35.0 72.2 - - 39.8 66.8
BTP ROP 79.4 54.3 60.2 75.3 73.9 49.1 55.3 72.6
regarding the truthfulness of each sentence or segment. (2) Chain-of-thought , which prompts
the model to first generate a reasoning process before deciding the truthfulness. We adopt the
Chain-of-thought prompting method from the official FELM [15] dataset, with slight modifications for
sentence-level and segment-level hallucination detection. (3) SelfCheckGPT [4], which samples
additional responses from the model and use the inconsistency between each response and the target
statement for hallucination detection. Among the multiple variants, we choose SelfCheckGPT-prompt
for comparison, which achieves the best performance. We sample 20 responses for hallucination
detection following its default configuration. (4) Maieutic-Prompting [8], which first builds a belief
tree via backward-chaining and then infers the truth-value of the original statement that resolves the
inconsistencies.
Implementation details We evaluate our method along with baselines using both closed-sourced
and open-sourced models, including GPT-3.5-turbo-0125 andLlama-3-8B-Instruct . For our
method, we set the maximum belief tree depth to 2. We employ greedy decoding during belief
tree construction and prior belief estimation. The exception is the statement correction strategy,
where we sample 5corrected statements using temperature 0.7. Additionally, since each statement
inFactCheckGPT is manually processed to ensure it contains only one property or fact, we do not
apply statement decomposition when build the belief tree for statements from FactCheckGPT . We
use the first 120 examples in the Wikibio-GPT3 dataset to estimate the emission probability in our
method, and validate it on the remaining examples and two other datasets. More implementation
details can be found in Appendix A.1.
4.2 Experiment Results
Overall comparison We evaluate the effectiveness of BTP ROP with the experiment results in
Table 3. We highlight the following observations. First, our method achieves the best performance on
FELM-Science andFactCheckGPT datasets across different backbones, demonstrating the superior-
ity of our method. BTP ROP improves upon the best baselines by 3% - 9% on AUROC and ROC-PR.
The only exception is the Wikibio-GPT3 dataset, where the SELFCHECK GPT is more effective
for detection hallucinated outputs in biographies generated by LLMs. Second, compared to SELF-
CHECK GPT which leverages contradictions between the target statement and the sampled responses
for hallucination detection, our method is more effective on detecting hallucinated responses related
to scientific knowledge. Both FELM-Science andFactCheckGPT datasets contain a significant
proportion of questions on scientific knowledge, and our method achieves the best performance on
them. Third, chain-of-thought prompting is less effective in hallucination detection, especially on the
FELM-Science dataset. This finding aligns with the experimental results in the original FELM dataset
paper. Our experiments show that the model tends to regard the input sentence as true most of the
8

=== Page 9 ===
time, leading to sub-optimal performance. Finally, the Maieutic-prompting method, which is originall
designed to verify the correctness of statements related to commonsense reasoning, is less effective
for hallucination detection. Even on the FELM-Science andFactCheckGPT datasets which contain
many statements about scientific knowledge, it remains less effective compared to other methods.
0.0 0.2 0.4 0.6 0.8
Relative Time Cost0.250.300.350.400.45AUC-PR
FELM-Science
SelfCheckGPT
BTProp
0.0 0.2 0.4 0.6 0.8
Relative Time Cost0.350.380.410.440.470.50AUC-PR
FactCheckGPT
SelfCheckGPT
Chain-of-thought
BTProp
Figure 4: Performance-efficiency comparison.Deployment efficiency One con-
cern on our method is the belief
tree construction would be time-
consuming and inefficient. There-
fore, we compare the time cost
of our method with the base-
lines on the FELM-Science and
FactCheckGPT datasets, where
our method significantly outper-
form baselines. We use the open-
sourced Llama-3-8b-Instruct
model as the backbone, and eliminate CoT prompting method on FELM-Science dataset due to its
poor performance. We visualize the trade-off between hallucination detection performance and the
time cost in Figure 4 of different methods. The baseline methods, including SELFCHECK GPT and
CoT prompting, samples additional 20 responses for hallucination detection by default. We vary the
number of samples and test the corresponding performance to visualize the trade-off. As depicted in
the graph, the baseline performance shows diminishing returns in performance improvement with
increased computational complexity. As time cost increases, the performance growth of the two
baseline models gradually levels off, eventually reaching a point where additional time does not
translate into significantly better performance. In comparison, our method can effectively trades
off increased time cost for significantly improved performance. Despite its higher computational
complexity, it achieves a marked improvement in performance metrics, underscoring its efficiency
and effectiveness in hallucination detection compared to the baseline models.
Qualitative results We visualize several examples of the constructed belief trees to demonstrate
how the inconsistencies in model‚Äôs beliefs help detect the hallucination. Due to space limit, we put
these examples and the corresponding analyses in Appendix A.2.
Table 4: Performance of BTP ROP given different child
node generation strategies.
AUROC AUC-PR F1 Acc
BTP ROP 79.1 52.3 56.5 81.6
Decomposition only 67.9 31.2 43.3 79.6
Premise only 70.2 30.7 40.4 78.6
Correction only 77.4 47.5 53.0 81.5
Prior Conf 75.5 37.2 42.1 81.6Ablation study: belief tree construc-
tion We introduce three strategies to
build the belief tree: statement decom-
position, generating supportive and con-
tradictory premises, and statement cor-
rection. We perform ablation study to
demonstrate the necessity to introduce
these different strategies in Table 4 using
gpt-3.5-turbo-0125 as the backbone
model. Specifically, we evaluate the perfor-
mance of our method when only use one child node generation strategies on the FELM-Science
dataset. We highlight the following observations. First, only applying one child node generation
strategy is sub-optimal. There exist a significant gap between using one strategy and using them all.
Second, statement correction tends to contribute most to the performance improvement, but when
combining it with other strategies, the performance can still be further improved.
5 Conclusion
In this paper, we propose BTP ROP, a method that leverages the model‚Äôs intrinsic capability for
hallucination detection. Given a statement from the LLM-generated texts, our method organizes
the model‚Äôs intrinsic beliefs on neighboring statements in a belief tree. By introducing the hidden
Markov tree model, we convert the the hallucination detection into a posterior probability estimation
problem and propose corresponding solutions to solve it. Experiment results have demonstrate the
effectiveness of our method. The future direction would be how to further improve the belief tree
construction method to make it more effective and efficient.
9

=== Page 10 ===
6 Acknowledgements
The work of Bairu Hou and Shiyu Chang was partially supported by National Science Foundation
(NSF) Grant IIS-2207052 and NSF Grant IIS-2302730. The computing resources used in this work
were partially supported by the Accelerate Foundation Models Research program of Microsoft.
7 Broader Impact
In this paper, our primary goal is to develop an algorithm that can integrate the model‚Äôs internal
beliefs on logically-connected statements to detect hallucinations in texts generated by the LLM. Our
method is designed to improve the trustworthiness of LLMs and make fully use of their intrinsic
capabilities. Therefore, our method is less likely to introduce the unintended risks. We also assess the
experiments to ensure they are devoid of any harmful content or adverse impacts.
8 Limitation
While BTP ROP improves the hallucination detection performance and enhance the trustworthiness
of LLMs, it involves collecting a set of augmented statements (the belief tree) to perform inference.
Therefore, the main limitation of our method is its high time cost. Constructing a belief tree
necessitates multiple queries to the large language model, leading to significant delays. Additionally,
the time complexity of generating child nodes increases exponentially as more layers are added to
the tree. A potential solution to this issue could be to develop a mechanism that selectively expands
nodes within the belief tree, thereby optimizing the process.
References
[1]Eric Mitchell, Joseph Noh, Siyan Li, Will Armstrong, Ananth Agarwal, Patrick Liu, Chelsea
Finn, and Christopher D Manning, ‚ÄúEnhancing self-consistency and performance of pre-trained
language models through natural language inference,‚Äù in Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing , 2022, pp. 1754‚Äì1768.
[2]Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit
Bansal, and Srinivasan Iyer, ‚ÄúMethods for measuring, updating, and visualizing factual beliefs
in language models,‚Äù in Proceedings of the 17th Conference of the European Chapter of the
Association for Computational Linguistics , 2023, pp. 2714‚Äì2731.
[3]Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schuetze, and
Peter Clark, ‚ÄúLanguage models with rationality,‚Äù in The 2023 Conference on Empirical Methods
in Natural Language Processing , 2023.
[4]Potsawee Manakul, Adian Liusie, and Mark Gales, ‚ÄúSelfcheckgpt: Zero-resource black-box
hallucination detection for generative large language models,‚Äù in The 2023 Conference on
Empirical Methods in Natural Language Processing , 2023.
[5]Afra Feyza Aky√ºrek, Ekin Aky√ºrek, Leshem Choshen, Derry Wijaya, and Jacob Andreas,
‚ÄúDeductive closure training of language models for coherence, accuracy, and updatability,‚Äù arXiv
preprint arXiv:2401.08574 , 2024.
[6]Zouying Cao, Yifei Yang, and Hai Zhao, ‚ÄúAutohall: Automated hallucination dataset generation
for large language models,‚Äù arXiv preprint arXiv:2310.00259 , 2023.
[7]Niels M√ºndler, Jingxuan He, Slobodan Jenko, and Martin Vechev, ‚ÄúSelf-contradictory hal-
lucinations of large language models: Evaluation, detection and mitigation,‚Äù in The Twelfth
International Conference on Learning Representations , 2023.
[8]Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras,
and Yejin Choi, ‚ÄúMaieutic prompting: Logically consistent reasoning with recursive expla-
nations,‚Äù in Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing , 2022, pp. 1266‚Äì1279.
10

=== Page 11 ===
[9]Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia
Tsvetkov, and Hannaneh Hajishirzi, ‚ÄúFine-grained hallucination detection and editing for
language models,‚Äù arXiv preprint arXiv:2401.06855 , 2024.
[10] Jiawei Zhang, Chejian Xu, Yu Gai, Freddy Lecue, Dawn Song, and Bo Li, ‚ÄúKnowhalu:
Hallucination detection via multi-form knowledge based factual checking,‚Äù arXiv preprint
arXiv:2404.02935 , 2024.
[11] Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing Zheng, and Xuan-Jing Huang, ‚ÄúHallu-
cination detection for generative large language models by bayesian sequential estimation,‚Äù in
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing ,
2023, pp. 15361‚Äì15371.
[12] Amos Azaria and Tom Mitchell, ‚ÄúThe internal state of an llm knows when it‚Äôs lying,‚Äù in The
2023 Conference on Empirical Methods in Natural Language Processing , 2023.
[13] Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou, and Yiqun Liu,
‚ÄúUnsupervised real-time hallucination detection based on the internal states of large language
models,‚Äù arXiv preprint arXiv:2403.06448 , 2024.
[14] Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping
Ye, ‚ÄúInside: Llms‚Äô internal states retain the power of hallucination detection,‚Äù in The Twelfth
International Conference on Learning Representations , 2023.
[15] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian
He, ‚ÄúFelm: Benchmarking factuality evaluation of large language models,‚Äù in Thirty-seventh
Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2023.
[16] Matthew S Crouse, Robert D Nowak, and Richard G Baraniuk, ‚ÄúWavelet-based statistical signal
processing using hidden markov models,‚Äù IEEE Transactions on signal processing , vol. 46, no.
4, pp. 886‚Äì902, 1998.
[17] J-B Durand, Paulo Goncalves, and Yann Gu√©don, ‚ÄúComputational methods for hidden markov
tree models-an application to wavelet trees,‚Äù IEEE Transactions on Signal Processing , vol. 52,
no. 9, pp. 2551‚Äì2560, 2004.
[18] Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan
Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy
Baldwin, et al., ‚ÄúFact-checking the output of large language models via token-level uncertainty
quantification,‚Äù arXiv preprint arXiv:2403.04696 , 2024.
[19] Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and William B
Dolan, ‚ÄúA token-level reference-free hallucination detection benchmark for free-form text
generation,‚Äù in Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , 2022, pp. 6723‚Äì6737.
[20] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz,
and Jason Weston, ‚ÄúChain-of-verification reduces hallucination in large language models,‚Äù
arXiv preprint arXiv:2309.11495 , 2023.
[21] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen, ‚ÄúHalueval: A
large-scale hallucination evaluation benchmark for large language models,‚Äù in Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing , 2023, pp.
6449‚Äì6464.
[22] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith, ‚ÄúHow language model
hallucinations can snowball,‚Äù arXiv preprint arXiv:2305.13534 , 2023.
[23] Esin Durmus, He He, and Mona Diab, ‚ÄúFeqa: A question answering evaluation framework
for faithfulness assessment in abstractive summarization,‚Äù in Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics , 2020, pp. 5055‚Äì5070.
11

=== Page 12 ===
[24] QIU Yifu, Yftah Ziser, Anna Korhonen, Edoardo Ponti, and Shay B Cohen, ‚ÄúDetecting and
mitigating hallucinations in multilingual summarisation,‚Äù in The 2023 Conference on Empirical
Methods in Natural Language Processing , 2023.
[25] Meng Cao, Yue Dong, and Jackie Chi Kit Cheung, ‚ÄúHallucinated but factual! inspecting the
factuality of hallucinations in abstractive summarization,‚Äù in Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2022, pp.
3340‚Äì3354.
[26] Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel,
‚ÄúEvaluating the factual consistency of large language models through news summarization,‚Äù in
Findings of the Association for Computational Linguistics: ACL 2023 , 2023, pp. 5220‚Äì5255.
[27] Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David Sussillo, ‚ÄúHallucina-
tions in neural machine translation,‚Äù 2018.
[28] Nuno M Guerreiro, Elena V oita, and Andr√© FT Martins, ‚ÄúLooking for a needle in a haystack:
A comprehensive study of hallucinations in neural machine translation,‚Äù arXiv preprint
arXiv:2208.05309 , 2022.
[29] Nuno Miguel Guerreiro, Duarte M Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch,
Pierre Colombo, and Andr√© FT Martins, ‚ÄúHallucinations in large multilingual translation
models,‚Äù Transactions of the Association for Computational Linguistics , vol. 11, 2023.
[30] David Dale, Elena V oita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi,
Cynthia Gao, Lo√Øc Barrault, and Marta Costa-juss√†, ‚ÄúHalomi: A manually annotated benchmark
for multilingual hallucination and omission detection in machine translation,‚Äù in Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing , 2023, pp.
638‚Äì653.
[31] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,
Andrea Madotto, and Pascale Fung, ‚ÄúSurvey of hallucination in natural language generation,‚Äù
ACM Computing Surveys , vol. 55, no. 12, pp. 1‚Äì38, 2023.
[32] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qian-
glong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al., ‚ÄúA survey on hallucination in
large language models: Principles, taxonomy, challenges, and open questions,‚Äù arXiv preprint
arXiv:2311.05232 , 2023.
[33] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston, ‚ÄúRetrieval augmenta-
tion reduces hallucination in conversation,‚Äù in Findings of the Association for Computational
Linguistics: EMNLP 2021 , 2021, pp. 3784‚Äì3803.
[34] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi, ‚ÄúFactscore: Fine-grained atomic evaluation of
factual precision in long form text generation,‚Äù in Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing , 2023, pp. 12076‚Äì12100.
[35] I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Gra-
ham Neubig, Pengfei Liu, et al., ‚ÄúFactool: Factuality detection in generative ai‚Äìa tool augmented
framework for multi-task and multi-domain scenarios,‚Äù arXiv preprint arXiv:2307.13528 , 2023.
[36] Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam, ‚ÄúWikichat: Stopping the hallucination
of large language model chatbots by few-shot grounding on wikipedia,‚Äù in Findings of the
Association for Computational Linguistics: EMNLP 2023 , 2023, pp. 2387‚Äì2413.
[37] Siqing Huo, Negar Arabzadeh, and Charles LA Clarke, ‚ÄúRetrieving supporting evidence for
llms generated answers,‚Äù arXiv preprint arXiv:2306.13781 , 2023.
[38] Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett, and Eunsol Choi, ‚ÄúComplex claim
verification with evidence retrieved in the wild,‚Äù arXiv preprint arXiv:2305.11859 , 2023.
12

=== Page 13 ===
[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al., ‚ÄúChain-of-thought prompting elicits reasoning in large language models,‚Äù
Advances in neural information processing systems , vol. 35, pp. 24824‚Äì24837, 2022.
[40] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu, ‚ÄúA stitch in
time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence
generation,‚Äù arXiv preprint arXiv:2307.03987 , 2023.
[41] Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pi-
patanangkura, and Peter Clark, ‚ÄúExplaining answers with entailment trees,‚Äù in Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing , 2021, pp.
7358‚Äì7370.
[42] Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark, ‚ÄúEntailer: Answering questions with
faithful and truthful chains of reasoning,‚Äù arXiv preprint arXiv:2210.12217 , 2022.
[43] Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr
Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein,
Aditya Pillai, et al., ‚ÄúFactcheck-gpt: End-to-end fine-grained document-level fact-checking and
correction of llm output,‚Äù arXiv preprint arXiv:2311.09000 , 2023.
[44] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
Joseph E. Gonzalez, Hao Zhang, and Ion Stoica, ‚ÄúEfficient memory management for large
language model serving with pagedattention,‚Äù in Proceedings of the ACM SIGOPS 29th
Symposium on Operating Systems Principles , 2023.
[45] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou, ‚ÄúSelf-consistency improves chain of thought reasoning in
language models,‚Äù in The Eleventh International Conference on Learning Representations ,
2022.
13

=== Page 14 ===
A Appendix / supplemental material
A.1 Implementation Details
Data preprocessing We preprocess the data in Wikibio-GPT3 [4] and FELM-Science [15] before
evaluating our method and baselines. The data preprocessing includes two steps. First, we perform
decontextualization since the sentences in a response generated by the LLM might be context-
dependent and contain pronouns or noun phrases that cannot be understood without full context. Take
the first data instance from Wikibio-GPT3 for demonstration, which is a biography of ‚ÄúJohn Russell
Reynolds‚Äù generated by an LLM. The second sentence within the biography requiring hallucination
detection is ‚ÄúHe was born in London, the son of a barrister, and was educated at Eton College and
Trinity College, Cambridge.‚Äù Without the context, we cannot identify the truthfulness of this sentence
due to the pronoun. Therefore, we first decontextualize the sentences in the two datasets by prompting
gpt-3.5-turbo-0125 . The prompt is available in Figure 9.
Second, we further manually clean up the dataset by filtering out some sentences that are not check-
worthy but still annotated as ‚Äútrue‚Äù in the datasets. For example, the FELM contains some sentences
such as ‚ÄúSure!‚Äù and ‚ÄúIf you have any further questions or concerns, please let me know.‚Äù. These
sentences are annotated as ‚Äútrue‚Äù and will be counted into performance evaluation. Since both two
datasets contains not too many examples, we manually filter out these sentences to exclude them in
our evaluation. The datasets after our preprocessing and filtering are available in the supplemental
material and will be made open-sourced.
Implementation of our method We use the vLLM [ 44] to perform the inference of
Llama-3-8B-Instruct . We use a single NVIDIA A100 80GB PCIe GPU to evaluate the per-
formance and report the time cost in Figure 4. For the emission probability estimation (Table 1), we
use the first 120 examples in the Wikibio-GPT3 dataset (50% of it) to compute the empirical estima-
tion of the emission probability. The model we use is gpt-3.5-turbo-0125 . Then we transfer the
estimated emission probabilities to the remaining examples in Wikibio-GPT3 and other datasets for
performance evaluation. For Llama-3-8B-Instruction , we also use the same estimated emission
probabilities. Additionally, since we set the model confidence on child nodes generated by statement
correction as 1.0 rather than probe the model‚Äôs confidence, we maintain an individual emission
probability for those child nodes ( p(Su= 1|Zu= 1) = 0 .8andp(Su= 1|Zu= 0) = 0 .2). During
the belief tree construction process, we add several constraints to boost the efficiency. Specifically,
we only apply statement decomposition to the root node, assuming the child nodes generated by the
LLM is atomic statements that only contain one aspect of information. Also, we do not expand nodes
generated by statement correction, as they are homogeneous to their parent node.
Implementation of baselines We follow the default configuration of each baselines. For SELF-
CHECKGPT , we sample 20 additional responses for hallucination detection. Similarly, we also sample
20 different answers for chain-of-thought prompting and aggregate them using self-consistency [ 45].
For Maieutic-Prompting, we use their prompts for the CREAK datasets to generate belief trees in our
evaluation.
A.2 Examples of Belief Trees
0: AdieleAfigbo(1941‚Äì2006) was a Nigerian historian and professor of African history at the University of Nigeria, Nsukka.1: AdieleAfigbowas a Nigerian historian2: AdieleAfigbowas a professor of African history at the University of Nigeria, Nsukka4: AdieleAfigbolived from 1937 to 20065: AdieleAfigbolived from 1937 to 20091.001.001.001.001.00Statement decompositionStatement correctionLabel: False
3: AdieleAfigbolived from 1941 to 20060.975: AdieleAfigbolived from 1937 to 20031.00
Figure 5: Belief tree example.We visualize several constructed belief trees as
well as how our method leverage the inconsis-
tencies in model‚Äôs beliefs for hallucination de-
tection. We start with a simple example from
theWikibio-GPT3 dataset [ 4] shown in Fig-
ure 5. Starting from the root node about Adiele
Afigbo , the first layer contains 3 child nodes
generated by statement decomposition. How-
ever, despite the statement is wrong, the model
(gpt-3.5-turbo-0125 ) assign a high confi-
dence score to both the original statement and
the child nodes decomposed from the root node.
Then, our method further generate child nodes
14

=== Page 15 ===
for node 1,2, and 3. We display the child nodes of node 3 in the figure, which is generated by
statement correction. With the three different statements about the date of death of that person, we
decrease the model‚Äôs confidence on node 3 and finally correct the model‚Äôs belief on the root node.
Note that in statement correction, we directly set the confidence of the generated child nodes as
1.0. For this example, if we query the model‚Äôs confidence scores on node 4, 5, and 6, we will get
confidence scores 0.97, 0.91, 0.88, respectively, which will still contradicts to their parent node.
0: Water can freeze at temperatures slightly below 0¬∞C at higher elevations, where the atmospheric pressure is lower.1: Water can freeze at temperatures slightly below 0¬∞C at higher elevations
2: Freezing of water at lower temperatures is influenced by lower atmospheric pressur3: The temperature required for water to freeze remains consistent regardless of elevation4: Scientific experiments have consistently shown that water freezes at 0¬∞C under normal conditions.5: The freezing point of water decreases with increasing atmospheric pressure, not lower atmospheric pressure6: The concept of lower atmospheric pressure causing water to freeze at lower temperatures is a misconception.0.990.99
0.100.970.990.190.06Statement decompositionContradictory premises
Contradictory premisesLabel: False
Figure 6: Belief tree example.Another example of the belief tree
is shown in Figure 6, which mainly
consists of child nodes generated by
supportive and contradictory premises.
At the root node, the model assigns a
high confidence score to the statement
about freezing point of water from the
FELM dataset [ 15]. After the statement
decomposition, the inconsistency is
triggered due to model‚Äôs low confi-
dence on node 2. Furthermore, if
we continuously generate premises for
node 1, we get two additional child
nodes (node 3 and 4) that are contra-
dictory to their parent node. However,
the model still assigns a high confi-
dence to node 3 and 4. Within the
belief propagation framework, the conditional probability of node 1 being true given the observations
on node 3 and 4 will be decreased. Similarly, this effect will propagate to the root node and lead to a
low posterior probability of the root node being true.
0: Linux adoption has been relatively limited compared to other operating systems like Windows and macOS.1: Linux has a strong presence in server environments, powering a significant portion of servers worldwide.2: Linux is the foundation for Android, which is the most widely used operating system for mobile devices3: Android is built on the Linux kernel, utilizing its open-source code as the basis for its operating system.0.370.990.990.99supportive premisesLabel: Truesupportive premises
Figure 7: Belief tree example.We then show one failure case of
our method in Figure 7. The
target statements comes from the
FactCheckGPT dataset [ 43]. Our
method first generation two support-
ive premises with high confidence.
These two child nodes are indeed
reasonable and can support the opin-
ion that ‚ÄúLinux operating system has
a widespread adoption.‚Äù, thus con-
tradicting to the root statement and
further decreasing the model‚Äôs confi-
dence on the root node. We hypothesize that the original statement, ‚ÄúLinux adoption has limited
compared to other operating systems like Windows and macOS‚Äù lacks sufficient specificity and could
be interpreted from multiple perspectives. For example, it could refer to market share in personal
computing versus cloud computing domains, making the ground-truth label nondeterministic.
A.3 Inferring the Posterior Probability of Z0
The posterior probability p(Z0= 1|{Su})can be computed by:
p(Z0= 1|{Su}) =p(Z0= 1,{Su})
p(Z0= 1,{Su}) +p(Z0= 0,{Su}), (2)
where {Su}={su}refers to the set of all observed variables in the belief tree.
Therefore, the key of the inference on the belief tree is to compute the two joint probabilities p(Z0=
1,{Su})andp(S1=False ,¬ØX1=¬Øx1). Following the conditional independence assumption in the
hidden Markov model, the truthfulness ( i.e,, states of hidden variables) of each node is determined by
its parent node in the tree and the transition probability. Also, the model‚Äôs confidence ( i.e., states of
observed variables) on each node is determined by the state of the corresponding variable and the
emission probability. With such an assumption, they can be decomposed as:
p(Z0= 1,{Su}) =p({Su}|Z0= 1)‚àóp(Z0= 1), (3)
15

=== Page 16 ===
where p(Z0= 1) is the prior probability of the statement being true (note that we set it to be 0.5 in
this paper). p({Su}|Z0= 1) is the conditional probability of all observed variables (exclude the root
node) given the root node being true. Recall that we introduce a notation Œ≤(z, u) =p(ST(u))|Zu=z
to represent such conditional probabilities, where ST(u)refers to the confidence scores (observed
variables) of all nodes on the subtree rooted at node u. Therefore, the conditional probability
p({Su}|Z0= 1) can be denoted as Œ≤(1,0)(node 0, hidden variable Z0= 1).
Without loss of generality, we discuss below how to compute Œ≤(z, u)for an arbitrary node uin the
tree. The computation for Œ≤(1,0)andŒ≤(0,0)can be performed in exactly the same way. Specifically,
we can further decompose Œ≤(z, u)as:
p(ST(u))|Zu=z) =Ô£±
Ô£≤
Ô£≥Y
v‚ààC(u)p(ST(v)|Zu=z)Ô£º
Ô£Ω
Ô£æp(Su|Zu=z). (4)
The probability p(ST(v)|Zu=z)can be decomposed as:
p(ST(v)|Zu=z) =X
k‚àà{0,1}p(ST(v)|Zv=k)‚àóp(Zv=k|Zu=z)
=X
k‚àà{0,1}Œ≤(v, k)‚àóp(Zv=k|Zu=z).(5)
Therefore, the conditional probability Œ≤(z, u) =p(ST(u))|Zu=z)can be computed in a recursive
manner:
Œ≤(z, u) =Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥Y
v‚ààC(u)X
k‚àà{0,1}Œ≤(v, k)‚àóp(Zv=k|Zu=z)| {z }
Transition ProbabilityÔ£º
Ô£¥Ô£Ω
Ô£¥Ô£æp(Su|Zu=z)|{z}
Emission Probability. (6)
There are two types of probabilities in the above equation. First, p(Su|Zu=z)is the ‚Äúemission
probability" at node u. Second, p(Zv=k|Zu=z)is the ‚Äútransition probability" from node uto its
child node v. The recursive computation in Equation 6 will starts from the leaf nodes. When vis the
leaf node in the tree, Œ≤(v, k)is actually p(Sv|Zv=k), which is the emission probability at node v.
Such a process propagates from bottom to up. Finally, we can compute Œ≤(1,0)as well as the joint
probability p(Z0= 1|{Su})according to Equation 3.
Beyond conditional independence: transition probability for statement decomposition. The
above computation of the posterior probability is based on the assumption of conditional independence.
Assume the node uhave two child nodes, v1andv2. Then we consider the transition probabilities
from utov1and from utov2independently. However, this does not hold for the child nodes
generated by statement decomposition, in which the truthfulness of the child nodes are influenced by
their parent node simultaneously. If uis true, then we know that both two child nodes are also true.
In contrast, if uis false, we can only infer that at least one of the child nodes are false. To handle
such a case, we revise the probability computation in Equation 4 accordingly. Assume node uhasm
child nodes. The probability Œ≤(z, u) =p(ST(u))|Zu=z)can be computed as:
p(ST(u))|Zu=z)p(Su|Zu=z)¬∑X
ZC(u)‚àà{0,1}mp(ZC(u)|Zu=z)Y
v‚ààC(u)Œ≤(Zv, v),(7)
which is the Equation 1 in the main paper.
A.4 Summary of the Algorithm
We summarize the pipeline of our method in Algorithm 1. The root node of the belief tree is the
given statement u0. During the belief tree construction process, we maintain a set Nthat contains
all current leaf nodes of the belief tree. We recursively expand each leaf node u‚àà N by its child
nodesC(u)and add these new child nodes to the belief tree if they are logically-connected to the
current leaf node. These generated child nodes then become new leaf nodes. They will also be added
toNand be expanded until the maximum depth is reached. Given the constructed belief tree and
the emission and transition probabilities estimated from a held-out dataset, the posterior probability
Œ≤(Z0= 1|ST(0))can be then computed in a recursive manner, which will be further used to predict
the truthfulness of the given statement.
16

=== Page 17 ===
A.5 License of the Datasets Used
Algorithm 1 BTP ROP Algorithm
1:Input: Statement u0, maximum tree depth dmax
2:Output: The posterior probability p(Z0= 1|ST(0)).
3:
4: Initialize the belief tree Twith root node u0
5: Initialize the leaf node set N={u0}
6:whileN Ã∏=‚äòdo ‚ñ∑Belief tree construction
7: Pop an element ufromN
8: Generate child nodes C(u)foru
9: forNode v‚àà C(u)do
10: Add vtoT
11: Add vtoNifdu< dmax
12: end for
13:end while
14:
15:function GETBETA(u,z)‚ñ∑Compute Œ≤(z, u)in Eq. 1
16: ifC(u) =‚äòthen
17: return p(su|Zu=z)
18: end if
19: forv‚àà C(u)do
20: Œ≤(v,0) = GETBETA(v,0)
21: Œ≤(v,1) = GETBETA(v,1)
22: end for
23: Compute Œ≤(u, z)according to Eq. 1
24: return Œ≤(u, z)
25:end function
26:
27:Œ≤(1,0)= G ETBETA(u= 0,z= 1)
28:Œ≤(0,0)= G ETBETA(u= 0,z= 0)
29:p(Z0= 1|ST(0)) =Œ≤(1,0)/(Œ≤(1,0) +Œ≤(0,0))FactCheckGPT is under Apache-2.0 li-
cense. FELM is under CC-BY-NC-SA-4.0
license, and Wikibio-GPT3 is under CC-
BY-SA-3.0 license. We have cited them
accordingly in the main paper.
A.6 Prompt
In this section, we list all the prompt used
in this paper, including belief tree construc-
tion, prior confidence estimation, and data
preprocessing.
Belief tree construction Given a state-
ment from the LLM output, we use the
following prompt gpt-3.5-turbo-0125
to perform statement decomposition , which
is shown in Figure 10. In the instruc-
tion, we specify the requirements to ex-
tract check-worthy claims and provide the
model with several examples. We also in-
clude an additional special example, where
the given sentence is actually a subjective
opinion, to prevent the model from de-
composing sentences that are actually not
check-worthy. We use a similar prompt for
Llama-3-8b-Instruction as shown in
Figure 11.
To generate supportive and contradictory
premises , we prompt the model as follows:
If the model judge the given statement as
true, then it needs to generate several expla-
nations to its judgment, which form the sup-
portive premises. In contrast, if the model
believes the given statement is false, then it will generate explanations to its judgment, which form the
contradictory premises. The prompts we used are listed in Figure 12 and Figure 13. We first prompt
the model with the prompt in Figure 12. If the model judges the statement as true and generates
supportive premises, then these premises will be returned and contradictory premises will not be
generated. If the model judges the statement as false, then we prompt the model with the prompt in
Figure 13 for contradictory premises. Although there might be better prompting strategies to generate
such premises, finding the optimal prompts and prompting strategies is out of the scope of this paper.
Therefore, we leave it for future work.
Finally, to generate child nodes via statement correction , we adopt the following pipeline. First, we
prompt the model using prompts listed in Figure 14 and Figure 15 to generate a question about the key
pieces of information in the statement. After that, we feed the generated question to the LLM again
without any other prompt to get its answer. Finally, we ask the model to revise the original statement
according to the ‚Äúbackground knowledge", which is the answer generated by itself. By doing this,
we expect to better utilize the inconsistency across model‚Äôs beliefs for child node generation. The
prompt for the last step is shown in Figure 16
To select the most appropriate strategies for each node, we use the following prompts to ask the LLM
to output the most suitable strategies, which is displayed in Figure 17 and Figure 18.
Prior confidence estimation When prompt gpt-3.5-turbo-0125 for the confidence score
on the truthfulness of a statement, we use the following prompt: True or False?
{target_statement} . For Llama-3 model, we find it sometimes refuse to judge the truthful-
17

=== Page 18 ===
ness and simply output it requires additional context. To enforce the model to output the confidence
score, we change the prompt accordingly:
Are the following statements true or false ? For each of the following
statements , determine whether it is true or false . Provide a response
of ‚ÄôTrue ‚Äô if the statement is correct , or ‚ÄôFalse ‚Äô if the statement is
incorrect .
Remember :
- You ** do not ** have access to additional information or external
data .
- If verifying the statement requires such external data or context
, regard it as false .
- Directly output " True " or " False " without adding any markers .
Figure 8: Prompt for confidence estimation ( Llama-3-8B-Instruct ).
** Rewrite Texts for Clarity **
In this task , you will receive one paragraph and one target statement
extracted from it. The target statement is context - dependent , which
makes the statement hard for us to understand without context and check
its truthfulness . Therefore , your task is to rewrite the statement to
reduce context dependency . Specifically ,
- Pronoun resolution : Replacing pronouns like "this ," "the ," "that ,"
"he ," "she ," and " they " with specific nouns or names they refer to in
the original paragraph . You should always use the full names .
- If the target statement only use the first / last name to refer to
the main entity , replace the first / last name with the full name of the
entity if available .
Note : do not modify the semantics of the sentence . Do not add new
information or your own descriptions into the statement .
** Input / Output Format **
The input will be provided with the format as below :
Original paragraph : <the original text >
Target statement : <the target statement needing rewrite >
Format your output as:
Output : <the target statement after rewrite >
Figure 9: Prompt for data preprocessing.
18

=== Page 19 ===
** Fact - Checking Claims Extraction :**
** Objective :** Analyze the provided statement to extract and list all
distinct factual claims that require verification . Each listed claim
should be verifiable and not overlap in content with other claims
listed .
** Instructions :**
1. ** Identify Factual Claims **:
- Identify parts of the statement that assert specific , verifiable
facts , including :
- Statistical data and measurements .
- Historical dates and events or other information .
- Direct assertions about real - world phenomena , entities , events ,
statistics
- Conceptual understandings and theories .
- In every claim , alway use the ** full names ** when referring to any
concept , person , or entity . ** Avoiding the use of pronouns or indirect
references ** that require contextual understanding .
2. List each verifiable claim separately . Ensure that each claim is
distinct and there is no overlap in the factual content between any two
claims .
- If a single claim is repeated in different words , list it only
once to avoid redundancy .
3. ** Output :**
- If there are multiple check - worthy claims , list each one clearly
and separately .
- If there is only one check - worthy claim , output just that one
claim .
- If no part of the statement contains verifiable facts (e.g.,
purely subjective opinions , hypothetical scenarios ), output the
following message : " Claim 1: No check - worthy claims available ."
** Output Format **:
Your output should be organized as follows :
Claim 1: <the first claim >
Claim 2: <the second claim >
Claim 3: <the third claim (if necessary )>
...
** Examples :**
Statement : According to recent data , China has surpassed the United
States in terms of GDP when measured using Purchasing Power Parity ( PPP
), and India is projected to overtake China by 2030.
Claim 1: China has surpassed the United States in terms of GDP when
measured using Purchasing Power Parity ( PPP ).
Claim 2: India is projected to overtake China in terms of GDP by 2030."
Statement : The world ‚Äôs largest desert is Antarctica , and it is larger
than the Sahara .
Claim 1: The world ‚Äôs largest desert is Antarctica .
Claim 2: Antarctica is larger than the Sahara .
Statement : I think pizza is the best food ever !
Claim 1: No check - worthy claims available .
Statement : The software ‚ÄôPhotoshop ‚Äô was released by Adobe Systems in
1988.
Claim 1: The software ‚ÄôPhotoshop ‚Äô was released by Adobe Systems in
1988.
Figure 10: The prompt for statement decomposition using gpt-3.5-turbo-0125 .
19

=== Page 20 ===
** Fact - Checking Claims Extraction :**
** Objective :** Analyze the provided statement to extract and list all
distinct factual claims that require verification . Each listed claim
should be verifiable and not overlap in content with other claims
listed .
** Instructions :**
1. Identify parts of the statement that assert specific , verifiable
facts . In every claim , alway use the ** full names ** when referring to
any concept , person , or entity . ** Avoiding the use of pronouns or
indirect references ** that require contextual understanding .
2. List each verifiable claim separately . Ensure that each claim is
distinct and there is no overlap in the factual content between any two
claims .
3. Exclude any statements that are purely hypothetical , assume
theoretical scenarios , or are speculative in nature . These do not
contain verifiable factual claims . For example , statements involving
assumptions (" Let ‚Äôs assume ...") , theoretical discussions (" consider
whether ...") , or purely speculative scenarios should not be considered
as containing verifiable claims .
4. If the statement does not contain any verifiable facts , output the
following message : " Claim 1: No check - worthy claims available ."
** Output Format **:
Your output should be organized as follows :
Claim 1: <the first claim >
Claim 2: <the second claim >
Claim 3: <the third claim (if necessary )>
...
** Examples :**
Statement : According to recent data , China has surpassed the United
States in terms of GDP when measured using Purchasing Power Parity ( PPP
), and India is projected to overtake China by 2030.
Claim 1: China has surpassed the United States in terms of GDP when
measured using Purchasing Power Parity ( PPP ).
Claim 2: India is projected to overtake China in terms of GDP by 2030."
Statement : The world ‚Äôs largest desert is Antarctica , and it is larger
than the Sahara .
Claim 1: The world ‚Äôs largest desert is Antarctica .
Claim 2: Antarctica is larger than the Sahara .
Statement : I think pizza is the best food ever !
Claim 1: No check - worthy claims available .
Statement : Let ‚Äôs assume there is a highest prime number and consider
its implications on number theory .
Claim 1: No check - worthy claims available .
Figure 11: The prompt for statement decomposition using Llama-3-8b-Instruct .
20

=== Page 21 ===
** Finding Supportive Premises **
Is the following statement true or false ? If it is true , list several
supportive premises for it.
** Important Rules :**
1. Each premise should be clearly stated and directly relevant to the
target statement . Avoid ambiguity and ensure that the connection to the
target statement is evident
2. Do not use pronouns in generated premises . Ensure each premise can
be understood clearly without any context . For each generated premise ,
you should always use the full name of each person , event , object , etc .
** Input / Output Format **:
Your output should be organized as follows .
Judgement : <True or False >
Premise 1: <the first premise >
Premise 2: <the second premise >
...
In contrast , if the statement is false , you simly output :
Judgement : False
Premise 1: No supportive premises applicable .
** Examples :**
Target statement : Renewable energy sources will lead to a decrease in
global greenhouse gas emissions .
Judgement : True
Premise 1: Renewable energy sources produce electricity without
emitting carbon dioxide .
Premise 2: Increasing the adoption of renewable energy reduces reliance
on fossil fuels , which are the primary source of industrial carbon
dioxide emissions .
Target statement : Eating carrots improves night vision .
Judgement : False
Premise 1: No supportive premises applicable .
Statement : Historical literacy enhances a society ‚Äôs ability to make
informed decisions .
Judgement : True
Premise 1: Understanding historical events provides context for current
issues , enabling citizens to make decisions that consider past
outcomes and lessons .
Premise 2: Historical literacy fosters critical thinking skills , which
are crucial in analyzing information and making reasoned decisions .
Premise 3: Societies with high levels of historical awareness can
recognize and avoid the repetition of past mistakes .
Figure 12: The prompt for generation of supportive premises.
21

=== Page 22 ===
** Finding Contradictory Premises **
Is the following statement true or false ? If it is false , list several
contradictory premises for it.
** Important Rules :**
1. Each premise should be clearly stated and directly relevant to the
target statement . Avoid ambiguity and ensure that the connection to the
target statement is evident
2. Do not use pronouns in generated premises . Ensure each premise can
be understood clearly without any context . For each generated premise ,
you should always use the full name of each person , event , object , etc .
** Input / Output Format **:
Your output should be organized as follows .
Judgement : <True or False >
Premise 1: <the first premise >
Premise 2: <the second premise >
...
In contrast , if the statement is false , you simply output :
Judgement : True
Premise 1: No contradictory premises applicable .
** Examples :**
Target statement : Renewable energy sources will lead to a decrease in
global greenhouse gas emissions .
Judgement : True
Premise 1: No contradictory premises applicable .
Target statement : Eating carrots improves night vision .
Judgement : False
Premise 1: The belief that eating carrots improves night vision stems
from World War II propaganda , not from scientific evidence .
Premise 2: While carrots are rich in vitamin A, which is necessary for
maintaining healthy vision , they do not enhance night vision beyond
normal levels .
Statement : The introduction of invasive species does not impact native
biodiversity .
Judgement : False
Premise 1: Invasive species often compete with native species for
resources , leading to a decline in native populations .
Premise 2: Studies show that invasive species can alter the natural
habitats of native species , negatively affecting their survival rates .
Premise 3: The introduction of the invasive zebra mussel in North
American waterways has led to significant declines in the populations
of native mussels .
Figure 13: The prompt for generation of contradictory premises.
22

=== Page 23 ===
Given the following claim , your tasks include :
1. Identify the key pieces of information critical for fact -
checking to determine its truthfulness .
2. Create a masked version of the claim by masking these key pieces
of information
3. Generate a question asking for the key pieces of information
Rules :
1. Do not mask the grammatical subject of the sentence -- the actor ,
entity , or object that performs the action in the sentence ‚Äôs main
clause . Also , following the format of the below examples in your output
.
** Examples :**
Statement : Bitcoin was created in 2009 by an anonymous entity known as
Satoshi Nakamoto .
Masked statement : Bitcoin was created in 2009 by an anonymous entity
known as [ which person ].
Question : Who created Bitcoin in 2009?
Statement : The iPhone was first released by Apple in 2007.
Masked statement : The iPhone was first released by Apple in [ what year
].
Question : Was the iPhone first released by Apple in 2007?
Statement : The speed of light in a vacuum is approximately 299 ,792
kilometers per second .
Masked statement : ‚ÄôRomeo and Juliet ‚Äô was written by Shakespeare in [
what time period ].
Question : What is the speed of light in a certain medium ?
Statement : " Attention Is All You Need " is a paper written by Ashish
Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan
N. Gomez , Lukasz Kaiser , Illia Polosukhin .
Masked statement : " Attention Is All You Need " is a paper written by [
whom ]
Question : Who was / were the authors of the paper " Attention Is All You
Need "?
Statement : The Great Wall of China is visible from space .
Masked statement : The Great Wall of China is [ visible or invisible ]
from space .
Question : Is the Great Wall of China visible from space ?
Statement : The headquarters of the United Nations is located in New
York City
Masked statement : The headquarters of the United Nations is located in
[ which city ].
Question : Where is the headquarters of the United Nations located ?
Figure 14: Step 1 in statement correction: question generation. This prompt is used for
gpt-3.5-turbo-0125 .
23

=== Page 24 ===
Given a statement , your task is to identify a general question that can
be used to check the truthfulness of the statement . The question
should directly address the claim to confirm or refute it without
seeking additional detailed information .
** Examples :**
Statement : Bitcoin was created in 2009 by an anonymous entity known as
Satoshi Nakamoto .
Question : Who created Bitcoin in 2009?
Statement : The iPhone was first released by Apple in 2007.
Question : When was iPhone first relased by Apple ?
Statement : ‚ÄôRomeo and Juliet ‚Äô was written by Shakespeare in the late 16
th century .
Question : When was ‚ÄôRomeo and Juliet ‚Äô was written by Shakespeare ?
Statement : " Attention Is All You Need " is a paper written by Ashish
Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan
N. Gomez , Lukasz Kaiser , Illia Polosukhin .
Question : Who was / were the authors of the paper " Attention Is All You
Need "?
Statement : The Eiffel Tower is located in Paris
Question : Where is the Eiffel Tower located ?
Statement : Albert Einstein was a physicist .
Question : What profession was Albert Einstein ?
Figure 15: Step 1 in statement correction: question generation. This prompt is used for
Llama-3-8b-Instruct . We adjust the prompt since the masked statement is not used in the
prompt for gpt-3.5-turbo-0125 .
** Background Knowledge **: { the model answer in step 2}
Leverage the above provided knowledge and your own knowledge to review
the correctness of following statement :
** Statement **: { statement }
Instruction :
- If the statement is correct , output it unchanged .
- If the statement is ** not mentioned in the background knowledge
and its correctness cannot be determined **, you should also directly
output the statement ** unchanged **.
- If the statement is wrong , revise only the parts of the statement
that are incorrect , to align with the background knowledge . Do not add
any additional sentences or details .
** Output Format :**
Format your output as:
Revised Answer : <Display the original statement if it is correct or not
mentioned in the background knowledge ; display the revised statement
if it is inaccurate >
Figure 16: Step 3 in statement correction: statement revision.
24

=== Page 25 ===
** Task : Choose the Best Strategy for Premise Generation **
We need to generate several premises for a given target statement .
These premises could either support or contradict the target statement .
Particularly , we have 2 techniques for premise generation :
1. Logical Relationships : This involves creating premises based on
entailment or contradiction . You can generate premises that either
support or contradict the target statement .
2. Statement Perturbation : Create variations of the statement by
altering key details to form contradictory premises .
Given these techniques , your task is to select the most suitable
technique given a particular statement . Follow the guidelines below to
select the most suitable technique .
** Important Guidelines **:
1. Prioritize logical relationships . The logical relation strategy is
broadly applicable , as long as it is straightforward to find supportive
or contradictory premises .
2. If a statement contains particular names , numbers , timestamps , or
other conditions that can be varied to generate contradictory premises ,
consider statement perturbation .
3. If both two methods are applicable , select them together and output
" both ".
** Output Format :**
Your selection should be one of " Logical Relation ", " Statement
Perturbation ", and " both ".
Target statement : Eating a balanced diet improves overall health .
Output : Logical Relation
Target statement : ‚ÄôWar and Peace ‚Äô was a book written by Leo Tolstoy .
Output : Statement Perturbation
Target statement : Water boils at 100 degrees Celsius at sea level
Output : both
Target statement : Mount Everest is 8 ,848 meters tall .
Output : Statement Perturbation
Target statement : Artificial intelligence will replace most human jobs
in the future
Output : Logical Relation
Figure 17: Prompt for strategy selection ( gpt-3.5-turbo-0125 ).
25

=== Page 26 ===
** Instruction for Choosing the Best Strategy for Premise Generation **
When tasked with generating premises for a given target statement , the
choice of strategy - Logical Relationships or Statement Perturbation -
should be determined based on the nature of the statement and the
desired objectives . Use the following guidelines to route the choice :
### When to Use Logical Relationships
Opt for Logical Relationships when :
- ** The Statement is Abstract or Principled **: Ideal for statements
that explore broad principles , ethics , or abstract concepts . This
method helps in drawing deep logical entailments or contradictions .
- ** Complex Relationships or Conditions **: When the statement involves
complex logical or conditional relationships , using this method
clarifies or challenges these intricacies .
- ** Requirement for Detailed Analysis **: For statements needing precise
and formal argumentation , especially in academic or technical
discussions .
### When to Use Statement Perturbation
Choose Statement Perturbation when :
- ** The Statement is Specific and Concrete **: Best for statements with
explicit details like scenarios , dates , locations , names , numbers or
timestamps . Altering these elements generates varied premises .
- ** Exploration of Counterfactuals or Hypotheticals **: Useful for
creating imaginative or scenario - based premises by modifying key
details to envision different outcomes .
- ** Sensitivity to Detail Changes **: When minor modifications in the
statement can significantly alter its implications or truth value .
** Output Format :**
Your selection should be one of " Logical Relation ", " Statement
Perturbation ", and " both ". Here " both " means both two methods are
applicable .
** Examples :**
Target statement : Eating a balanced diet improves overall health .
Output : Logical Relation
Target statement : Countries with higher investment in education
consistently rank higher in global innovation indexes
Output : Logical Relation
Target statement : ‚ÄôWar and Peace ‚Äô was a book written by Leo Tolstoy .
Output : Statement Perturbation
Target statement : Water boils at 100 degrees Celsius at sea level
Output : both
Target statement : Mount Everest is 8 ,848 meters tall .
Output : Statement Perturbation
Target statement : The bridge will remain intact even if 75 cars drive
on it simultaneously if the cars are lightweight
Output : Statement Perturbation
Target statement : Artificial intelligence will replace most human jobs
in the future
Output : Logical Relation
Figure 18: Prompt for strategy selection ( Llama-3-8b-Instruction ).
26
