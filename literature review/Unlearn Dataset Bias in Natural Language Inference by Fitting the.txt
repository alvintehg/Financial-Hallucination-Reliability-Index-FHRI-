=== Page 1 ===
Unlearn Dataset Bias in Natural Language Inference by Fitting the
Residual
He He1;2and Sheng Zha1and Haohan Wang3
1Amazon Web Services,2New York University,3Carnegie Mellon University
fhehea,zhasheng g@amazon.com, haohanw@cs.cmu.edu
Abstract
Statistical natural language inference (NLI)
models are susceptible to learning dataset
bias: superﬁcial cues that happen to asso-
ciate with the label on a particular dataset, but
are not useful in general, e.g., negation words
indicate contradiction. As exposed by sev-
eral recent challenge datasets, these models
perform poorly when such association is ab-
sent, e.g., predicting that “ I love dogs. ” con-
tradicts “ I don’t love cats. ”. Our goal is to
design learning algorithms that guard against
known dataset bias. We formalize the con-
cept of dataset bias under the framework of
distribution shift and present a simple debias-
ing algorithm based on residual ﬁtting, which
we call DRiFt. We ﬁrst learn a biased model
that only uses features that are known to re-
late to dataset bias. Then, we train a debi-
ased model that ﬁts to the residual of the bi-
ased model, focusing on examples that cannot
be predicted well by biased features only. We
use DRiFt to train three high-performing NLI
models on two benchmark datasets, SNLI and
MNLI. Our debiased models achieve signiﬁ-
cant gains over baseline models on two chal-
lenge test sets, while maintaining reasonable
performance on the original test sets.1
1 Introduction
Machine learning models have surpassed human-
performance on multiple language understanding
benchmarks. However, transferring the success to
real-world applications has been much slower due
to the brittleness of these systems. For example,
McCoy et al. (2019) show that models blindly pre-
dict the entailment relation for two sentences with
high word overlap even if they have very different
meanings, e.g., “ The man hit a dog ” and “ The dog
hit a man ”. Jia and Liang (2017) show that reading
1Code is available at https://github.com/
hhexiy/debiased .
g(x)
<latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit><latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit><latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit><latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit>y<latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit>b(x)
<latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit><latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit><latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit><latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit>a<latexit sha1_base64="b7/vCs5ze5KtVd66W3yyALYBfbk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipSQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/DXYzl</latexit><latexit sha1_base64="b7/vCs5ze5KtVd66W3yyALYBfbk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipSQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/DXYzl</latexit><latexit sha1_base64="b7/vCs5ze5KtVd66W3yyALYBfbk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipSQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/DXYzl</latexit><latexit sha1_base64="b7/vCs5ze5KtVd66W3yyALYBfbk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipSQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/DXYzl</latexit>Semantics:
P: The little girl is sad.
H: The girl is not sad.Word choice:
“not”Label:
contradictionBias cause: annotation strategyg(x)
<latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit><latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit><latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit><latexit sha1_base64="5MdSH+jBhLb9upJWOdxkrEde8/M=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5NKo/nQ+qNbfhzoFWiVeQGhRoDapf/WFMUkGlIRxr3fPcxAQZVoYRTmeVfqppgskEj2jPUokF1UE2v3WGzqwyRFGsbEmD5urviQwLracitJ0Cm7Fe9nLxP6+Xmug6yJhMUkMlWSyKUo5MjPLH0ZApSgyfWoKJYvZWRMZYYWJsPBUbgrf88ippXzQ8t+HdX9aaN0UcZTiBU6iDB1fQhDtogQ8ExvAMr/DmCOfFeXc+Fq0lp5g5hj9wPn8AbeaN0g==</latexit>y<latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit>b(x)
<latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit><latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit><latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit><latexit sha1_base64="6bfZhmIPs+auSOFP/BVfCvkg+rY=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWMG2hDWWz3bRLdzdhdyOW0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmhQln2rjut1NaW9/Y3CpvV3Z29/YPqodHbR2nilCfxDxW3RBrypmkvmGG026iKBYhp51wcpv7nUeqNIvlg5kmNBB4JFnECDa5FNafzgfVmttw50CrxCtIDQq0BtWv/jAmqaDSEI617nluYoIMK8MIp7NKP9U0wWSCR7RnqcSC6iCb3zpDZ1YZoihWtqRBc/X3RIaF1lMR2k6BzVgve7n4n9dLTXQdZEwmqaGSLBZFKUcmRvnjaMgUJYZPLcFEMXsrImOsMDE2nooNwVt+eZW0Lxqe2/DuL2vNmyKOMpzAKdTBgytowh20wAcCY3iGV3hzhPPivDsfi9aSU8wcwx84nz9mQ43N</latexit>Semantics:
P: The little girl is sad.
H: The girl is not happy.Word choice:
“not”Label:
entailmentUnknown causea0
<latexit sha1_base64="c9NesRyxyqsx3mTie9T9rgIQLPo=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbRU0mkoMeiF49V7Ae0oWy2m3bpZhN2J0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilB3reL1fcqjsHWSVeTiqQo9Evf/UGMUsjrpBJakzXcxP0M6pRMMmnpV5qeELZmA5511JFI278bH7plJxZZUDCWNtSSObq74mMRsZMosB2RhRHZtmbif953RTDaz8TKkmRK7ZYFKaSYExmb5OB0JyhnFhCmRb2VsJGVFOGNpySDcFbfnmVtC6rnlv17muV+k0eRxFO4BQuwIMrqMMdNKAJDEJ4hld4c8bOi/PufCxaC04+cwx/4Hz+ACPNjRY=</latexit><latexit sha1_base64="c9NesRyxyqsx3mTie9T9rgIQLPo=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbRU0mkoMeiF49V7Ae0oWy2m3bpZhN2J0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilB3reL1fcqjsHWSVeTiqQo9Evf/UGMUsjrpBJakzXcxP0M6pRMMmnpV5qeELZmA5511JFI278bH7plJxZZUDCWNtSSObq74mMRsZMosB2RhRHZtmbif953RTDaz8TKkmRK7ZYFKaSYExmb5OB0JyhnFhCmRb2VsJGVFOGNpySDcFbfnmVtC6rnlv17muV+k0eRxFO4BQuwIMrqMMdNKAJDEJ4hld4c8bOi/PufCxaC04+cwx/4Hz+ACPNjRY=</latexit><latexit sha1_base64="c9NesRyxyqsx3mTie9T9rgIQLPo=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbRU0mkoMeiF49V7Ae0oWy2m3bpZhN2J0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilB3reL1fcqjsHWSVeTiqQo9Evf/UGMUsjrpBJakzXcxP0M6pRMMmnpV5qeELZmA5511JFI278bH7plJxZZUDCWNtSSObq74mMRsZMosB2RhRHZtmbif953RTDaz8TKkmRK7ZYFKaSYExmb5OB0JyhnFhCmRb2VsJGVFOGNpySDcFbfnmVtC6rnlv17muV+k0eRxFO4BQuwIMrqMMdNKAJDEJ4hld4c8bOi/PufCxaC04+cwx/4Hz+ACPNjRY=</latexit><latexit sha1_base64="c9NesRyxyqsx3mTie9T9rgIQLPo=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbRU0mkoMeiF49V7Ae0oWy2m3bpZhN2J0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilB3reL1fcqjsHWSVeTiqQo9Evf/UGMUsjrpBJakzXcxP0M6pRMMmnpV5qeELZmA5511JFI278bH7plJxZZUDCWNtSSObq74mMRsZMosB2RhRHZtmbif953RTDaz8TKkmRK7ZYFKaSYExmb5OB0JyhnFhCmRb2VsJGVFOGNpySDcFbfnmVtC6rnlv17muV+k0eRxFO4BQuwIMrqMMdNKAJDEJ4hld4c8bOi/PufCxaC04+cwx/4Hz+ACPNjRY=</latexit>Training
TestingFigure 1: An example of dataset bias in NLI. On the
training data, the biased feature (“ not”) is affected by
crowd workers’ strategy of negating the premise to cre-
ate a contradicting pair. However, at test time the word
choice is affected by unknown sources, thus “ not” may
not be associated with the label “contradiction”. A
model relying on the negation word to predict “con-
tradiction” would fail on the shown test example.
comprehension models are easily distracted by ir-
relevant sentences containing key phrases from the
question. Similar failures have also been observed
on paraphrase identiﬁcation (Zhang et al., 2019c)
and story cloze test (Schwartz et al., 2017).
A common problem behind these failures is dis-
tribution shift. Our training data is often not a rep-
resentative sample of real-world data due to their
different data-generating processes, thus models
are susceptible to learning simple cues (e.g., lex-
ical overlap) that work well on the majority of
training examples but fail on more challenging
test examples. Consider generating a contradicting
pair of sentences for natural language inference
(NLI) in Figure 1. Crowd workers tend to mechan-
ically negate the premise sentence to save time, in-
troducing an association between negation words
(e.g., “ not”) and the contradiction label. However,
at test time, such association may not exist as data
is now generated by end users. Thus, a model thatarXiv:1908.10763v2  [cs.CL]  25 Nov 2019

=== Page 2 ===
heavily relies on the biased feature “ not” would
fail. In this paper, we formalize dataset bias (Tor-
ralba and Efros, 2011) under the label shift as-
sumption: the conditional distribution of the la-
bel given biased features changes at test time. Our
goal is to design learning algorithms that are ro-
bust to dataset bias with a focus on NLI, i.e. pre-
dicting whether the premise sentence entails the
hypothesis sentence.
Typical debiasing approaches aim to remove bi-
ased features (e.g., gender and image texture) in
the learned representation (Wang et al., 2019b,a).
However, biased features in textual data often con-
ﬂate useful semantic information and superﬁcial
cues, thus completely removing them might sig-
niﬁcantly hurt prediction performance. Even when
we are conﬁdent that the bias is irrelevant to pre-
diction (e.g., gender), Gonen and Goldberg (2019)
show that existing bias removal methods are insuf-
ﬁcient.
Instead of debiasing the data representation, our
method (along with the concurrent work of Clark
et al. (2019)) accounts for label shift given biased
features by focusing on “hard” examples that can-
not be predicted well using only biased features.
We train a model in two steps. First, we train a
biased model using insufﬁcient features such as
overlapping words between the premise and the
hypothesis. Next, we train a debiased model by ﬁt-
ting to the residuals of the biased model. This step
“unlearns” the bias by taking additional negative
gradient updates on examples with low loss under
the biased model (Section 3.2).2At test time, only
the debiased model is used for prediction. We call
this learning algorithm DRiFt ( Debias by Residual
Fitting).
We use DRiFt to train three high-performing
NLI models on two benchmark datasets,
SNLI (Bowman et al., 2015) and MNLI (Williams
et al., 2017). Compared to baseline models
trained by maximum likelihood estimation, our
debiased models improve performance on several
challenge datasets with only slight degradation on
the original test sets.
2 Problem Statement
Dataset bias. Letx2X be the input and y2Y
be the label we want to predict. Given training
2Note that dataset bias is ﬂagged by good performance
despite insufﬁcient input, e.g., a high-accuracy hypothesis-
only classiﬁer (Gururangan et al., 2018).examples (x;y)drawn from a distribution P, we
deﬁne dataset bias as (partial) representation of
xthat exhibits label shift (Lipton et al., 2018;
Scholkopf et al., 2012) on the test distribution Q.
Formally, assume that xcan be represented by two
components b(x)andg(x)conditionally indepen-
dent giveny. We have
p(x;y) =p(b(x);g(x);y) (1)
=p(g(x)jy)p(yjb(x))p(b(x)):(2)
Letg(x)be the true effect of ysuch that their re-
lationship does not change normally, i.e. p(g(x)j
y) =q(g(x)jy). Letb(x)bebiased features that
happen to be predictive of yonP. For example, in
Figure 1,g(x)represents semantics of the premis
and hypothesis sentences, whereas b(x)represents
speciﬁc word choices affected by varying sources.
In the training data, the word “ not” has a strong as-
sociation with “contradiction” due to crowd work-
ers’ writing strategies. Consequently, a model
learned on the training data distribution Pwould
degrade when such association no longer exists.
Formally, both training and testing examples may
exhibit biased features: p(b(x)) =q(b(x)), but
dependence between these features and the label
can change: p(yjb(x))6=q(yjb(x)).
In a typical supervised learning setting with
dataset bias, we do not observe examples from
Qthusb(x)is unknown. Without additional in-
formation, achieving good performance on Qis
impossible. Fortunately, oftentimes we do have
domain-speciﬁc knowledge on what b(x)might
be, e.g., the word overlapping heuristic in NLI.
Therefore, our goal is to correct the model trained
onPto perform well on Qgiven known dataset
bias.
Bias in NLI data. Dataset bias in SNLI (Bow-
man et al., 2015) and MNLI (Williams et al., 2017)
are largely due to the crowdsourcing process. Both
are created by asking crowd workers to write three
sentences (hypotheses) that are entailed by, neutral
with, or contradict a given sentence drawn from a
corpus (the premise). Gururangan et al. (2018);
Poliak et al. (2018) show that certain words in the
hypothesis have high pointwise mutual informa-
tion with class labels regardless of the premise,
which could be artifacts of speciﬁc annotation
strategies. For example, one can create a neu-
tral sentence by adding a cause (“ because ”) to
the premise and create a contradicting sentence
by negating (“ no”, “never ”) the premise. As a

=== Page 3 ===
result, the majority of training examples can be
solved without much reasoning about sentence
meanings. Subsequently, McCoy et al. (2019) re-
port that models rely on high word overlap to pre-
dict entailment; Glockner et al. (2018); Naik et al.
(2018) demonstrate that models struggle at even
lexical-level inference involving antonyms, hyper-
nyms, etc.
A natural question to ask then is whether there
exist better data collection procedures that guard
against these biases. We argue that this is not
easy because in practice, we almost always have
different data-generating processes during train-
ing (generated from selected corpora and anno-
tators) and test (generated by end users). Then,
can we remove biased features from training ex-
amples? This is also infeasible because sometimes
they contain the necessary information for predic-
tion, e.g., removing words may destroy the sen-
tence meaning. It is not the features that are biased
but their relation with the label. Next, we describe
our approach to mitigating this biased relation.
3 Approach
3.1 Overview
The key idea of our approach is to ﬁrst detect bi-
ased examples given prior knowledge on potential
dataset bias, then focus on learning from unbiased,
hard examples. We describe the two steps in de-
tails below.
Detect biased examples. How do we know if an
example exhibits biased features? Although we
cannot directly measure label shift without access-
ing the test data, we know that NLI models are
unlikely to work well given insufﬁcient features.
When it does work well given only partial seman-
tics of the input, the good performance is likely
due to dataset bias. For example, Gururangan
et al. (2018) exposes annotation artifacts by show-
ing that hypothesis-only models have unexpected
high accuracy. Similarly, we train a biased clas-
siﬁer using insufﬁcient features I(x), e.g., the hy-
pothesis sentence. We assume that examples pre-
dicted well by the biased classiﬁer exhibit dataset
bias, i.e.p(yjI(x))is high butq(yjI(x))is low.
Importantly, while I(x)approximates b(x)
given our prior knowledge, it does not necessar-
ily capture all dataset bias, which depends on the
unknown test distribution. In addition, I(x)may
include useful information. For example, althoughbag-of-words (BOW) features are insufﬁcient to
represent precise sentence meaning, it encodes a
distribution of possible meanings. Thus good per-
formance of a BOW classiﬁer is not fully due to
ﬁtting dataset bias. In practice, as we will see
in the experiments (Section 4.5), good choices of
I(x)capture biased features precisely, resulting in
signiﬁcant performance drop of the biased classi-
ﬁer onQ.
Learn residuals of the biased classiﬁer. Our in-
tuition is that the debiased classiﬁer should cap-
ture information beyond those contained in the bi-
ased classiﬁer. If the biased classiﬁer already has
a small loss on an example, then there is not much
to learn beyond the biased features; otherwise, the
debiased classiﬁer should correct predictions of
the biased classiﬁer.
We implement the idea through a residual ﬁtting
procedure (DRiFt). Let fs:X !Randfd:X !
Rbe the biased and the debiased classiﬁers, and
letLbe the loss function. First, we learn fswith
insufﬁcient features I(x)as the input:
= arg min
EP[L(fs(I(x););y)]:(3)
Letf(x)be the optimal predictor that minimizes
the empirical risk on P. We deﬁne
f(x)def=fs(I(x);) +fd(x;): (4)
Thusfdﬁts the residual of fswith respect to the
targetf. To estimate parameters offd, we ﬁx
parameters of fsand minimize the loss:
min
EP[L(fs(I(x);) +fd(x;);y)]:(5)
At test time, we only use the debiased classiﬁer fd.
Consider the typical empirical risk minimiza-
tion approach that estimates by minimizing
EP[L(fd(x;);y)]. It is susceptible to relying on
biased features when they predict well on the ma-
jority examples. In contrast, DRiFt ﬁrst learns fs
which is intended to ﬁt potential bias in the data. It
then learnsfdthat compensates fswithout ﬁtting
to the bias already captured by it.
Next, we analyze the behavior of DRiFt using
the cross-entropy loss function, which is typically
used for classiﬁcation problems.
3.2 Analysis with the Cross-Entropy Loss
In this section, we show that DRiFt adjusts the gra-
dient on each example depending on how well it is
predicted by the pretrained biased classiﬁer.

=== Page 4 ===
Given the cross-entropy loss, our goal is to
maximize the expected conditional log-likelihood
of the data, EP[logp(yjx)]. A classiﬁer out-
puts a vector of scores for each of the Kclasses,
f(x) = (f1(x);:::;fK(x))2RK, which are
then mapped to a probability distribution p(yjx)
by the softmax function. Given classiﬁers fsand
fd, we have three choices of parametrization of the
conditional probability p(yjx):
ps(yjI(x))/exp (fy
s(I(x);)) (6)
pd(yjx)/exp 
fy
d(x;)
(7)
pa(yjx)/exp 
fy
s(I(x);) +fy
d(x;)
/ps(yjI(x))pd(yjx): (8)
To learn the classiﬁer fd, standard maximum like-
lihood estimation (MLE) uses pd(yjx), whereas
DRiFt uses pa(yjx)given pretrained fswith
ﬁxed parameters.
Let us ﬁrst compare the two learning objectives.
Denoteps(yjI(x);)byp
s(yjI(x)). DRiFt
maximizes
JD() =X
(x;y)Dlogpa(yjx;;) (9)
=C+X
(x;y)D[logpd(yjx;) 
logKX
k=1p
s(kjI(x))pd(kjx;)];(10)
whereDdenotes the training set and C=P
(x;y)Dlogp
s(kjI(x))is a constant. Compare
(10) with the MLE objective:
JMLE() =X
(x;y)Dlogpd(yjx;): (11)
We see thatJD()has an additional regularizer for
each example x:
R(x)def= logKX
k=1p
s(kjI(x))pd(kjx):(12)
Geometrically, it encourages output from the debi-
ased classiﬁer, pd, to have minimal projection on
pspredicted by the biased classiﬁer.
Next, let’s look at the effect of this regularizer
through its gradient. Let Z(x)be the normalizerP
kp
s(kjI(x))pd(kjx). Then, we have
rR(x) = P
kp
s(kjI(x))rpd(kjx)P
kps(kjI(x))pd(kjx)
= X
kpa(kjx)rlogpd(kjx);which is derived by writing rpdaspdrlogpd.
Taking a negative step in the direction of
rlogpd(kjx)corresponds to down-weighting
the probability pd(kjx). Intuitively, the model
tries to reweight the output distribution by the gra-
dient weights pa(kjx). Note that
pa(kjx)/p
s(kjI(x))pd(kjx): (13)
For an example (x;y), large values of p
s(yjI(x))
indicate that I(x)is likely to contain biased fea-
tures. Ifpd(yjx)is also large, the model is
probably picking up the bias since pdhas access
to complete information in xincluding the biased
features, in which case a relatively large negative
step is taken to correct it. In the extreme case
where the biased classiﬁer makes perfect predic-
tion, we have p
s(yjI(x))!1thusrR(x)!
 rlogpd(yjx), canceling the MLE gradient
rlogpd(yjx). As a result, the gradient on this
example is zero, and there is nothing to be learned.
At the other end where I(x)does not provide any
useful information, the biased classiﬁer outputs a
uniform distribution p
s(yjI(x)) = 1=K, thus
pa(yjx) =pd(yjx)and the gradient on this
example is reduced to the MLE gradient.
4 Experiments
We ﬁrst evaluate our method using synthetic bias
to show its effectiveness under different amount
of dataset bias. We then test on two challenge
datasets using different biased classiﬁers. We
show that DRiFt consistently outperforms MLE on
the challenge datasets given different NLI models,
especially when the insufﬁcient features capture
dataset bias exploited by the challenge data.
4.1 Training Data
We evaluate DRiFt on two benchmarking NLI
datasets: SNLI (Bowman et al., 2015) and
MNLI (Williams et al., 2017). Each pair of
premise and hypothesis sentences has a label from
one of “entailment”, “contradiction”, or “neutral”.
Sentences from SNLI are derived from image cap-
tions, whereas MNLI covers a broader range of
styles and topics. Statistics of the two datasets are
shown in Table 1. All MNLI results are on the
matched development set.3
3MNLI has two development sets, one from the same
source as the training data (matched) and one from different
sources (mismatched). We trained two sets of models using
their corresponding development sets for model selection and
obtained similar results. Thus we focus on the “matched”
results.

=== Page 5 ===
Dataset Train Dev Test
SNLI 549,367 9842 9842
MNLI 392,702 9815 -
Table 1: Statistics of training datasets. The test sets of
MNLI are hosted through Kaggle competitions.
4.2 Models and Training Details
DRiFt is a general learning algorithm that works
with any biased/debiased models. Below we
describe the three key components of our ap-
proaches: the learning algorithm, the biased model
with its insufﬁcient features, and the debiased
model.
Learning algorithms. We compare DRiFt with
MLE, as well as a simpler variant of DRiFt: in-
stead of the residual ﬁtting, we remove the ex-
amples predicted correctly by the biased classi-
ﬁer and train on the rest. We call this baseline
RM, which is also conceived by Gururangan et al.
(2018). MLE only trains the debiased model. Both
DRiFt and R Mrely on an additional biased model
that captures potential dataset bias.
Biased models. We consider three insufﬁcient
representations that exploit various NLI dataset bi-
ases reported in prior work.
HYPO is a ﬁnetuned BERT classiﬁer that uses
only the hypothesis sentence.
CBOW is a continuous bag-of-words classiﬁer.
Similar to Mou et al. (2016), we represent both the
premise and the hypothesis as the respective sums
of their word embeddings. We then concatenate
the premise and the hypothesis embeddings, their
difference, and their element-wise product. The
ﬁnal representation is passed through a one-layer
fully connected network with ReLU activation.
HAND is a classiﬁer using handcrafted features
based on error analysis in Naik et al. (2018).
Speciﬁcally, we include tokens in the hypothe-
sis that are also in the premise, tokens unique
to the hypothesis, Jaccard similarity between the
two sentences, whether negation words (“ not” and
“n’t”) are included, and length difference com-
puted byjLp Lhj
Lp+LhwhereLpandLhare numbers
of tokens in the premise and the hypothesis. We
represent the overlapping and the non-overlapping
tokens as the respective sums of their word embed-
dings. The embeddings are then concatenated withthe dense features and passed through a one-layer
fully connected network with ReLU activation.
Debiased models. We choose three high-
performing models of different capability.
DA is the Decomposable Attention model intro-
duced by Parikh et al. (2016), which relies on
the interaction between words in the premise and
the hypothesis. It does not use any word order
information. We used the variant without intra-
sentence attention.4
ESIM is the Enhanced Sequential Inference
Model (Chen et al., 2017). It ﬁrst encodes the
premise and the hypothesis by a bidirectional
LSTM, aligns the contextual word embeddings
similar to Parikh et al. (2016), and uses another
“inference” bidirectional LSTM to aggregate in-
formation. Thus it has access to the non-local con-
text.
BERT is the Bidirectional Encoder Representa-
tions from Transformers (Devlin et al., 2019) that
recently improved performance on MNLI signiﬁ-
cantly. It uses contextual embeddings pretrained
from large corpora.
Hyperparameters. For non-BERT mod-
els, word embeddings are initialized with the
840B.300d pretrained GloVe (Pennington
et al., 2014) word vectors and ﬁnetuned during
training. For DA and ESIM, hyperparame-
ters of the model architecture are the same
as those reported in the original papers. We
ﬁnetune all BERT models from the pretrained
BERT-base-uncased model.5We train all
models using the Adam (Kingma and Ba, 2014)
optimizer with 1= 0:9,2= 0:999, L2 weight
decay of 0.01, learning rate warmup for the ﬁrst
10% of updates and linear decay afterwards. We
use a dropout rate of 0.1 for all models except
ESIM, which has a dropout rate of 0.5. BERT and
non-BERT models are trained with a learning rate
of 2e-5 and 1e-4, respectively. For MLE, we train
BERT for 4 epochs and the rest for 30 epochs.
When training the debiased model in DRiFt, we
ﬁnd that the models converge slowly thus we train
BERT for 8 epochs and the rest for 80 epochs.
4We removed the projection layers of the word embed-
dings as it speeds up training without hurting performance in
our experiments.
5http://gluon-nlp.mxnet.io/model_zoo/
bert/index.html

=== Page 6 ===
model
0.60.70.80.91.0accuracyBERTDAESIM
0.2 0.4 0.6 0.8cheatingrate0.2 0.4 0.6 0.8cheatingrate0.2 0.4 0.6 0.8cheatingrateDRiFt-hypoMLERm-cheatmethodFigure 2: Accuracy on SNLI test set augmented with cheating features, which leak the groundtruth labels on
training data but not on test data. Models trained by MLE degrade signiﬁcantly when a majority of examples are
cheatable, whereas debiased models trained by DRiFt maintain similar accuracies across different cheating rates.
4.3 In-Distribution Performance
We ﬁrst evaluate the models’ in-distribution per-
formance where they are trained and evaluated on
splits from the same dataset. Results of the biased
models are reported in Table 2. All exceeds the
majority-class baseline by a large margin, indicat-
ing that a majority of examples can be solved by
superﬁcial cues.
Results of the debiased models are reported in
Table 3. Baseline results from our implemen-
tations are comparable to prior reported perfor-
mance (row “MLE”). Debiased models trained by
DRiFt show some degradation on in-distribution
data, especially for the less powerful DA and
ESIM models. The accuracy drop is expected
due to two reasons. First, DRiFt assumes distri-
bution shift thus does not optimize performance
on the training distribution P. Second, the effec-
tive training data size is reduced by negative gra-
dients on potentially biased examples; this effect
is exaggerated by R M, which shows signiﬁcant
in-distribution degradation. Similar trade-off be-
tween in-distribution accuracy and robustness on
out-of-distribution data has also been observed in
adversarial training (Zhang et al., 2019b; Tsipras
et al., 2019).
Dataset majority H YPO CBOW H AND
SNLI 34.2 61.8 81.2 76.7
MNLI 35.4 52.5 66.1 65.4
Table 2: Accuracy of biased classiﬁers on SNLI test set
and MNLI development set. All exceeds the majority-
class baseline by a large margin, signaling dataset bias.SNLI MNLI
BERT DA ESIM BERT DA ESIM
MLE 90.8 85.3 88.0 84.5 72.2 78.1
DRiFt-H YPO 89.8 83.9 86.3 84.3 68.6 75.0
DRiFt-CBOW 84.7 62.6 62.3 82.1 56.3 68.8
DRiFt-H AND 86.5 75.0 79.2 81.7 58.8 68.9
RM-HYPO 71.2 67.0 70.3 65.5 57.5 63.0
RM-CBOW 35.8 27.1 22.2 54.9 26.8 27.1
RM-HAND 46.3 37.2 38.1 51.7 34.6 37.4
Table 3: Accuracy of models trained by MLE, DRiFt,
and R Mwith different biased models. Training and
test examples are from the same dataset. Intensity of
thered highlights corresponds to absolute drop in ac-
curacy with respect to the MLE baseline. R Msigniﬁ-
cantly hurts in-distribution performance. DRiFt main-
tains reasonable performance.
4.4 Synthetic Bias
In this section, we evaluate our model under con-
trolled, synthetic dataset bias on SNLI. Recall our
deﬁnition of dataset bias: the conditional distribu-
tion of the label ygiven biased features are dif-
ferent on training and test sets. Therefore, we in-
ject bias into each example by adding a cheating
feature that encodes its label. On training and de-
velopment examples, the cheating feature encodes
the ground truth label with probability pcheat (the
cheating rate), and a random label otherwise. On
test examples, the cheating feature always encodes
a random label. Thus a model relying on the cheat-
ing feature would perform poorly on the test set.
Speciﬁcally, we prepend the hypothesis with
a string “flabelgand” where label2
fentailment, contradiction, neutral g. To simulate

=== Page 7 ===
the fact that we often cannot pinpoint biased fea-
tures until the model fails on some test examples,
we choose H YPO as our biased classiﬁer. That is
to say, we have a rough idea that the bias might
be in the hypothesis but do not know what it is ex-
actly.
We train all three base models (DA, ESIM, and
BERT) using MLE and DRiFt, respectively. Our
results are shown in Figure 2. All MLE models are
reasonably robust to a mild amount of bias. How-
ever, when a majority ( pcheat>0:6) of training ex-
amples contains the bias, their accuracy decreases
signiﬁcantly: about 20% drop atpcheat= 0:9com-
pared to the baseline accuracy when no cheating
features are injected. BERT is slightly more robust
than DA and ESIM, possibly due to the regulariza-
tion effect of pretrained embeddings. In contrast,
our debiased models (DRiFt-H YPO) maintain sim-
ilar accuracies with increasing cheating rates and
have a maximum accuracy drop of about 5%.
Two questions remain, though: (1) Why does
the accuracy of debiased models still drop a bit
at high cheating rates? (2) Why is the baseline
accuracy of DRiFt lower than MLE? We answer
these questions by analyzing the upper bound per-
formance of our method below.
Best-case scenario. In the ideal case, we know
precisely what the bias is. Consider a biased clas-
siﬁer that only uses the cheating feature as its
input. It predicts biased examples perfectly, i.e.
ps(yjb(x)) = 1 andps(kjb(x)) = 08k6=y,
and predicts the rest unbiased examples uniformly
at random. Based on our discussion at the end of
Section 3.2, the biased examples have zero gradi-
ents and unbiased examples have the same gradi-
ents as in MLE. In this case, our method is equiv-
alent to removing biased examples and training a
classiﬁer on the rest, i.e. R M-cheat. In Figure 2,
we see that it completely dominates MLE. The ac-
curacy of R M-cheat still drops when pcheatis large,
because there are fewer “good” examples to learn
from, not due to ﬁtting the bias. Similarly, DRiFt-
HYPO has lower overall accuracy compared to
RM-cheat, because H YPO captures additional (un-
biased) features that cannot be fully learned by the
debiased model.
Worst-case scenario. In the extreme case when
pcheat= 1, all models’ predictions on the test set
are random guesses. For MLE, the biased fea-
tures are no longer differentiable from the gener-alizable ones, thus there is no reason not to use
them. For DRiFt, since the biased model achieves
perfect prediction on all training examples, the de-
biased model receives zero gradient. Therefore,
when strong bias presents on all examples, we
need more information to correct the bias, e.g.,
collecting additional data or augmenting exam-
ples.
methodlexical subseq const
E:E E :E E :E
HYPO 52.6 44.4 54.5 44.3 45.6 16.7
CBOW 63.2 16.0 66.2 33.7 63.2 38.5
HAND 66.7 0.0 66.7 0.0 66.7 0.0
model: BERT
MLE 67.2 7.8 66.7 0.4 68.1 11.9
DRiFt-H YPO 84.7 79.8 69.0 23.7 72.7 40.8
DRiFt-CBOW 80.8 75.2 68.5 29.5 71.5 40.3
DRiFt-H AND 77.4 70.9 71.2 41.2 75.8 61.0
RM-HYPO 67.2 46.0 65.2 36.6 75.5 72.2
RM-CBOW 5.4 66.4 8.5 64.2 34.8 65.3
RM-HAND 10.0 66.0 4.7 66.3 9.1 67.3
model: DA
MLE 66.6 0.5 66.6 0.3 66.5 0.4
DRiFt-H YPO 66.3 1.7 66.9 5.5 66.3 8.4
DRiFt-CBOW 65.3 7.2 66.1 9.6 65.1 9.1
DRiFt-H AND 60.5 27.1 61.4 44.9 55.9 48.3
RM-HYPO 65.1 9.6 66.2 15.0 66.2 18.8
RM-CBOW 0.4 66.6 1.3 66.7 0.8 66.5
RM-HAND 10.3 65.8 8.9 65.7 13.9 64.7
model: ESIM
MLE 65.8 3.2 67.2 4.6 65.5 2.8
DRiFt-H YPO 64.3 10.5 68.3 16.3 68.1 29.3
DRiFt-CBOW 63.2 14.4 66.8 20.1 64.9 22.7
DRiFt-H AND 61.2 19.6 63.7 39.4 64.8 48.3
RM-HYPO 63.3 12.8 64.1 24.8 71.3 46.0
RM-CBOW 4.5 65.7 6.0 65.2 16.9 63.8
RM-HAND 25.8 60.8 18.3 67.3 13.1 65.9
Table 4: F1 scores of the entailment ( E) and non-
entailment (:E) classes on HANS. All models are
trained on MNLI and results are shown on three subsets
targeting at different biases: lexical overlap (lexical),
subsequence overlap (subseq), and constituent overlap
(const). Intensity of the Blue and red highlights cor-
responds to absolute increase and decrease of scores
with respect to MLE. DRiFt signiﬁcantly improves re-
sults on challenging :Eexamples without hurting per-
formance on E, whereas R Mimproves scores on :Eat
the cost of performance on E.
4.5 Word Overlap Bias
We evaluate our method on word overlap bias
in NLI. McCoy et al. (2019) show that models

=== Page 8 ===
trained on MNLI largely rely on word overlap
between the premise and the hypothesis to make
entailment predictions. They created a challenge
dataset (HANS) where premises may not entail
high word-overlapping hypotheses. Speciﬁcally, a
model biased by word overlap would fail on three
types of non-entailment examples: (1) Lexical
overlap, e.g., “ The doctor visited the lawyer. ”;
“The lawyer visited the doctor. ”. (2) Subsequence,
e.g., “ The senator near the lawyer danced. ”;
“The lawyer danced. ”. (3) Constituent, e.g., “ The
lawyers resigned, or the artist slept. ”;“The
artist slept. ”.
We evaluate both biased and debiased models
on the three subsets of HANS and show F1 scores
for each class in Table 4. As expected, mod-
els trained by MLE almost always predict entail-
ment (E), and thus performs poorly for the non-
entailment class (:E). DRiFt improves perfor-
mance on:Ein all cases with little degradation
onE. In contrast, R Mimproves performance on
:Eat the cost of signiﬁcant degradation on E.
Among all biased models, H AND produces the
best debiasing results because it is designed to ﬁt
the word overlap bias, and indeed has zero recall
on:Ewhen tested on HANS. On the contrary, the
improvement from H YPO is lower because it does
not capture any word overlap bias. Correspond-
ingly, its performance drop on HANS is mini-
mal compared to its in-distribution performance.
Among all debiased models, BERT has the best
overall performance. We hypothesize that pre-
training on large data improves model robustness
in addition to the debiasing effect from DRiFt.
4.6 Stress Tests
In addition to the word overlap bias exploited by
HANS, there are other known biases such as nega-
tion words and sentence lengths. Naik et al. (2018)
conduct a detailed error anlaysis on MNLI and
create six stress test sets (S TRESS ) targeting at
each type of error. We focus on the word overlap
and negation stress test sets, which expose dataset
bias as opposed to model weakness according to
Liu et al. (2019). A model biased by word overlap
rate and negation words are expected to have low
accuracy on the entailment class on challenge data.
The complete results are shown in Appendix A.
In Table 5, we show the F1 scores of each class
for all models on S TRESS .6Compared to results
6Since results of R Mare similar to those in Table 4, wemethodNegation Overlap
E C N E C N
HYPO 41.2 52.4 50.5 44.2 52.8 51.7
CBOW 20.1 48.2 53.9 49.7 52.9 55.6
HAND 37.5 45.0 57.3 56.7 50.1 57.8
model: BERT
MLE 2.4 81.1 56.5 19.2 83.3 59.4
DRiFt-H YPO 7.3 80.7 55.6 27.5 81.1 59.1
DRiFt-CBOW 17.9 81.7 55.5 18.3 80.0 56.6
DRiFt-H AND 4.3 80.6 55.5 15.0 81.9 57.4
model: DA
MLE 17.4 47.3 55.3 46.7 60.5 57.8
DRiFt-H YPO 11.8 47.0 51.8 41.6 59.4 55.6
DRiFt-CBOW 28.4 21.4 39.5 35.2 41.7 43.8
DRiFt-H AND 24.7 42.0 46.4 42.2 56.0 49.9
model: ESIM
MLE 12.0 72.7 54.6 27.6 76.4 57.5
DRiFt-H YPO 22.8 67.7 54.0 37.5 73.2 56.7
DRiFt-CBOW 32.7 62.3 46.9 30.4 65.6 49.8
DRiFt-H AND 15.8 64.6 51.8 39.2 70.7 53.9
Table 5: F1 scores of each class on S TRESS . Intensity
of the Blue and red highlights corresponds to ab-
solute increase and decrease of scores with respect to
MLE. DRiFt improves results on E(that exhibits label
shift) with some degradation on other classes for DA
and ESIM.
on HANS, S TRESS sees lower overall improve-
ment from debiasing. One reason is that S TRESS
decreases word overlap rate and injects negation
words by appending distractor phrases, i.e. “ true
is true ” and “ false is not true ”. While this in-
troduces label shift on biased features, it also in-
troduces covariate shift on the input. For exam-
ple, although H AND contains features designed
to use word overlap rate (Jaccard similarity) and
negation words, its does not have big performance
drop on the challenge data compared to its in-
distribution performance, showing that that dis-
tractor phrases may affect the model in other ways.
While all debiased models show improvement
onE, both DA and ESIM suffer from degradation
on the other two classes, especially when trained
by DRiFt-CBOW. We posit two reasons. First,
while CBOW is insufﬁcient to represent complete
sentence meaning, it does encode a distribution
of possible meanings. Thus models debiased by
DRiFt-CBOW might discard useful information.
Second, model capacity limits what is learned be-
yond a BOW representation. DA shows the most
put them in Appendix A.

=== Page 9 ===
degradation since it only uses local word interac-
tion, thus is essentially a BOW model. In contrast,
BERT has little degradation on in-distribution ex-
amples regardless of the biased classiﬁer.
5 Related Work and Discussion
Adversarial data collection. Aside from NLI,
dataset bias has been exposed on benchmarks
for other NLP tasks as well, e.g., paraphrase
identiﬁcation (Zhang et al., 2019c,a), story close
test (Schwartz et al., 2017), reading comprehen-
sion (Kaushik and Lipton, 2018), coreference res-
olution (Zhao et al., 2018a), and visual question
answering (Agrawal et al., 2016). Most bias is
resulted from artifacts in the data selection pro-
cedure and shortcuts taken by crowd workers.
To systematically minimize bias during data col-
lection, adversarial ﬁltering methods (Sakaguchi
et al., 2019; Zellers et al., 2019) have been pro-
posed to discard examples predicted well by a sim-
ple classiﬁer. This is similar to the R Mbaseline,
except that we apply “ﬁltering” at training time.
In general, our debiasing methods are complemen-
tary to adversarial data collection methods.
Debiased representation. Our work is closely
related to the line of work on removing bias in
data representations. Bolukbasi et al. (2016); Zhao
et al. (2018b) learn gender-neutral word embed-
dings by forcing certain dimensions to be free
of gender information. Similarly, Wang et al.
(2019a) construct a biased classiﬁer and project
its representation out of the model’s representa-
tion. For NLI, Belinkov et al. (2019) use adversar-
ial learning to remove hypothesis-related bias in
the sentence representations. However, for some
NLP applications it may not be easy to separate
biased features from useful semantic representa-
tions, thus we correct the conditional distribution
of the class label given these biased features in-
stead of removing them from the input. Concur-
rently, Clark et al. (2019) take the same approach
and further show its effectiveness on additional
tasks including reading comparehension and vi-
sual question answering.
Distribution shift. Covariate shift (Shimodaira,
2000; Ben-David et al., 2006) and label shift (Lip-
ton et al., 2018; Zhang et al., 2013) are two well-
studied settings under distribution shift, which
makes different assumptions on how p(x;y)
changes. However, most works in these settingsassume access to unlabeled data from the tar-
get distribution. Our objective is more related
to distributionally robust optimization (Duchi and
Namkoong, 2018; Hu et al., 2018), which does
not assume access to target data and optimizes the
worst-case performance under unknown , bounded
distribution shift. In contrast, we leverage prior
knowledge on potential dataset bias.
Data augmentation. An effective way to tackle
the challenge datasets is to train or ﬁnetune
on similar examples (McCoy et al., 2019; Liu
et al., 2019; Jia and Liang, 2017), which explic-
itly correct the training data distribution. How-
ever, constructing challenge examples often rely
on handcrafted rules that target a speciﬁc type
of bias, e.g., swapping male and female enti-
ties (Zhao et al., 2018a, 2019), synonym/antonym
substitution (Glockner et al., 2018), and syntactic
rules (McCoy et al., 2019; Ribeiro et al., 2018),
and may require human veriﬁcation (Zhang et al.,
2019c; Jia and Liang, 2017). Data augmentation
provides a way to encode our prior knowledge on
the task, e.g., swapping genders does not affect
coreference resolution result, and syntactic trans-
formations may affect sentence meanings. There-
fore, a related direction is to develop generic aug-
mentation techniques with linguistic priors (An-
dreas, 2019; Karpukhin et al., 2019).
6 Conclusion
Across all different dataset biases, the fundamen-
tal problem is that the majority training examples
are not representative of the real-world data dis-
tribution (including the challenge data), thus min-
imizing the average training loss no longer accu-
rately describes our objective. In this paper, we
tackle the problem by adapting the learning ob-
jective to focus on examples that cannot be easily
solved by biased features. We show that our de-
biasing method improves model performance on
challenge data given known dataset bias. However,
current improvements largely rely on task-speciﬁc
prior knowledge, thus an important next step is to
develop more general methods that tackle different
types of biases.
Acknowledgments
Yanchao Ni worked on an earlier version of this
project while he was at New York University. We
thank the GluonNLP community for their support
on reproducing prior results.

=== Page 10 ===
References
A. Agrawal, D. Batra, and D. Parikh. 2016. Analyzing
the behavior of visual question answering models.
InEmpirical Methods in Natural Language Process-
ing (EMNLP) .
J. Andreas. 2019. Good-enough compositional data
augmentation. arXiv .
Y . Belinkov, A. Poliak, S. M. Shieber, B. V . Durme,
and A. M. Rush. 2019. Don’t take the premise for
granted: Mitigating artifacts in natural language in-
ference. In Association for Computational Linguis-
tics (ACL) .
S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira.
2006. Analysis of representations for domain adap-
tation. In Advances in Neural Information Process-
ing Systems (NeurIPS) , pages 137–144.
T. Bolukbasi, K. Chang, J. Y . Zou, V . Saligrama, and
A. T. Kalai. 2016. Man is to computer programmer
as woman is to homemaker? debiasing word embed-
dings. In Advances in Neural Information Process-
ing Systems (NeurIPS) , pages 4349–4357.
S. Bowman, G. Angeli, C. Potts, and C. D. Manning.
2015. A large annotated corpus for learning natural
language inference. In Empirical Methods in Natu-
ral Language Processing (EMNLP) .
Q. Chen, X. Zhu, Z. Ling, S. Wei, H. Jiang, and
D. Inkpen. 2017. Enhanced LSTM for natural lan-
guage inference. In Association for Computational
Linguistics (ACL) .
C. Clark, M. Yatskar, and L. Zettlemoyer. 2019. Don’t
take the easy way out: Ensemble based methods for
avoiding known dataset biases. In Empirical Meth-
ods in Natural Language Processing (EMNLP) .
J. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019.
Bert: Pre-training of deep bidirectional transformers
for language understanding. In North American As-
sociation for Computational Linguistics (NAACL) .
J. Duchi and H. Namkoong. 2018. Learning models
with uniform performance via distributionally robust
optimization. arXiv preprint arXiv:1810.08750 .
M. Glockner, V . Shwartz, and Y . Goldberg. 2018.
Breaking NLI systems with sentences that require
simple lexical inferences. In Association for Com-
putational Linguistics (ACL) .
H. Gonen and Y . Goldberg. 2019. Lipstick on a pig:
Debiasing methods cover up systematic gender bi-
ases in word embeddings but do not remove them.
arXiv preprint arXiv:1903.03862 .
S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz,
S. R. Bowman, and N. A. Smith. 2018. Annota-
tion artifacts in natural language inference data. In
North American Association for Computational Lin-
guistics (NAACL) .W. Hu, G. Niu, I. Sato, and M. Sugiyama. 2018. Does
distributionally robust supervised learning give ro-
bust classiﬁers? In International Conference on Ma-
chine Learning (ICML) .
R. Jia and P. Liang. 2017. Adversarial examples for
evaluating reading comprehension systems. In Em-
pirical Methods in Natural Language Processing
(EMNLP) .
V . Karpukhin, O. Levy, J. Eisenstein, and
M. Ghazvininejad. 2019. Training on syn-
thetic noise improves robustness to natural noise in
machine translation. arXiv .
D. Kaushik and Z. C. Lipton. 2018. How much read-
ing does reading comprehension require? a criti-
cal investigation of popular benchmarks. In Em-
pirical Methods in Natural Language Processing
(EMNLP) .
D. Kingma and J. Ba. 2014. Adam: A method
for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Z. C. Lipton, Y . Wang, and A. J. Smola. 2018. De-
tecting and correcting for label shift with black box
predictors. In International Conference on Machine
Learning (ICML) .
N. F. Liu, R. Schwartz, and N. A. Smith. 2019. Inocu-
lation by ﬁne-tuning: A method for analyzing chal-
lenge datasets. In North American Association for
Computational Linguistics (NAACL) .
R. T. McCoy, E. Pavlick, and T. Linzen. 2019. Right
for the wrong reasons: Diagnosing syntactic heuris-
tics in natural language inference. arXiv preprint
arXiv:1902.01007 .
L. Mou, R. Men, G. Li, Y . Xu, L. Zhang, R. Yan, and
Z. Jin. 2016. Natural language inference by tree-
based convolution and heuristic matching. In Asso-
ciation for Computational Linguistics (ACL) .
A. Naik, A. Ravichander, N. Sadeh, C. Rose, and
G. Neubig. 2018. Stress test evaluation for natural
language inference. In International Conference on
Computational Linguistics (COLING) .
A. Parikh, O. T ¨ackstr ¨om, D. Das, and J. Uszkoreit.
2016. A decomposable attention model for natural
language inference. In Empirical Methods in Natu-
ral Language Processing (EMNLP) .
J. Pennington, R. Socher, and C. D. Manning. 2014.
GloVe: Global vectors for word representation. In
Empirical Methods in Natural Language Processing
(EMNLP) , pages 1532–1543.
A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger,
and B. V . Durme. 2018. Hypothesis only base-
lines in natural language inference. arXiv preprint
arXiv:1805.01042 .

=== Page 11 ===
M. T. Ribeiro, S. Singh, and C. Guestrin. 2018. Se-
mantically equivalent adversarial rules for debug-
ging NLP models. In Association for Computational
Linguistics (ACL) .
K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi.
2019. WINOGRANDE: An adversarial wino-
grad schema challenge at scale. arXiv preprint
arXiv:1907.10641 .
B. Scholkopf, D. Janzing, J. Peters, E. Sgouritsa,
K. Zhang, and J. Mooij. 2012. On causal and anti-
causal learning. In International Conference on Ma-
chine Learning (ICML) .
R. Schwartz, M. Sap, Y . Konstas, L. Zilles, Y . Choi,
and N. A. Smith. 2017. The effect of different writ-
ing tasks on linguistic style: A case study of the
ROC story cloze task. In Computational Natural
Language Learning (CoNLL) .
H. Shimodaira. 2000. Improving predictive inference
under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Infer-
ence, 90:227–244.
A. Torralba and A. Efros. 2011. Unbiased look at
dataset bias. In Computer Vision and Pattern Recog-
nition (CVPR) .
D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and
A. Madry. 2019. Robustness may be at odds with
accuracy. In International Conference on Learning
Representations (ICLR) .
H. Wang, Z. He, Z. C. Lipton, and E. P. Xing. 2019a.
Learning robust representations by projecting super-
ﬁcial statistics out. In International Conference on
Learning Representations (ICLR) .
T. Wang, J. Zhao, M. Yatskar, K. Chang, and V . Or-
donez. 2019b. Balanced datasets are not enough:
Estimating and mitigating gender bias in deep im-
age representations. In International Conference on
Computer Vision (ICCV) .
A. Williams, N. Nangia, and S. R. Bowman. 2017.
A broad-coverage challenge corpus for sentence
understanding through inference. arXiv preprint
arXiv:1704.05426 .
R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and
Y . Choi. 2019. HellaSwag: Can a machine really
ﬁnish your sentence? In Association for Computa-
tional Linguistics (ACL) .
G. Zhang, B. Bai, J. Liang, K. Bai, S. Chang, M. Yu,
C. Zhu, and T. Zhao. 2019a. Selection bias explo-
rations and debias methods for natural language sen-
tence matching datasets. In Association for Compu-
tational Linguistics (ACL) .
H. Zhang, Y . Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and
M. I. Jordan. 2019b. Theoretically principled trade-
off between robustness and accuracy. arXiv preprint
arXiv:1901.08573 .K. Zhang, B. Schlkopf, K. Muandet, and Z. Wang.
2013. Domain adaptation under target and condi-
tional shift. In International Conference on Machine
Learning (ICML) .
Y . Zhang, J. Baldridge, and L. He. 2019c. PAWS: Para-
phrase adversaries from word scrambling. In North
American Association for Computational Linguis-
tics (NAACL) .
J. Zhao, T. Wang, M. Yatskar, R. Cotterell, V . Ordonez,
and K. Chang. 2019. Gender bias in contextualized
word embeddings. In North American Association
for Computational Linguistics (NAACL) .
J. Zhao, T. Wang, M. Yatskar, V . Ordonez, and
K. Chang. 2018a. Gender bias in coreference reso-
lution:evaluation and debiasing methods. In North
American Association for Computational Linguis-
tics (NAACL) .
J. Zhao, Y . Zhou, Z. Li, W. Wang, and K. Chang.
2018b. Learning gender-neutral word embeddings.
InEmpirical Methods in Natural Language Process-
ing (EMNLP) .

=== Page 12 ===
A Results on MNLI Stress Test
In Table 6, we show the complete results on MNLI
Stress Test (Naik et al., 2018). In addition to
Overlap and Negation, which is intended to test
dataset bias, we also include two tests that eval-
uate model performance on minority examples.
Our debiased models have some improvement on
Antonym, possibly as a by-product of focusing on
challenge examples that cannot be solved by su-
perﬁcial cues. However, DRiFt did not improve
performance on Length.

=== Page 13 ===
Negation Overlap Antonym Length
E C N E C N E C N E C N
HYPO
MLE41.2 52.4 50.5 44.2 52.8 51.7 - 40.5 - 55.1 52.5 51.5
CBOW 20.1 48.2 53.9 49.7 52.9 55.6 - 19.0 - 21.9 55.5 49.4
HAND 37.5 45.0 57.3 56.7 50.1 57.8 - 28.2 - 66.6 65.0 60.7
BERTMLE 2.4 81.1 56.5 19.2 83.3 59.4 - 66.0 - 83.8 83.6 77.4
DRiFt-H YPO 7.3 80.7 55.6 27.5 81.1 59.1 - 75.4 - 84.1 83.2 76.3
DRiFt-CBOW 17.9 81.7 55.5 18.3 80.0 56.6 - 75.3 - 82.4 82.3 74.6
DRiFt-H AND 4.3 80.6 55.5 15.0 81.9 57.4 - 76.0 - 81.4 82.5 74.9
RM-HYPO 32.1 55.9 39.9 44.4 63.8 43.0 - 69.3 - 72.9 70.9 52.4
RM-CBOW 33.6 61.6 42.7 29.4 65.2 44.7 - 85.1 - 69.7 60.8 55.7
RM-HAND 20.7 49.7 40.0 30.9 54.7 39.6 - 83.8 - 57.2 52.6 46.5
DAMLE 17.4 47.3 55.3 46.7 60.5 57.8 - 59.8 - 69.5 66.0 61.9
DRiFt-H YPO 11.8 47.0 51.8 41.6 59.4 55.6 - 57.4 - 66.4 63.7 55.3
DRiFt-CBOW 28.4 21.4 39.5 35.2 41.7 43.8 - 57.8 - 64.3 39.4 53.9
DRiFt-H AND 24.7 42.0 46.4 42.2 56.0 49.9 - 72.4 - 48.4 57.6 51.2
RM-HYPO 14.9 39.9 45.3 52.0 52.6 46.0 - 56.6 - 63.9 62.2 32.0
RM-CBOW 3.8 23.9 38.0 2.6 17.1 41.5 - 53.1 - 4.4 33.5 29.9
RM-HAND 31.6 26.4 36.2 40.6 37.6 29.6 - 57.8 - 40.0 27.6 33.1
ESIMMLE 12.0 72.7 54.6 27.6 76.4 57.5 - 75.1 - 77.6 76.8 68.8
DRiFt-H YPO 22.8 67.7 54.0 37.5 73.2 56.7 - 75.5 - 75.9 74.3 66.3
DRiFt-CBOW 32.7 62.3 46.9 30.4 65.6 49.8 - 67.0 - 68.5 60.2 60.1
DRiFt-H AND 15.8 64.6 51.8 39.2 70.7 53.9 - 74.7 - 68.8 70.9 61.6
RM-HYPO 29.6 54.4 45.3 47.3 63.6 46.1 - 60.4 - 70.6 68.2 31.1
RM-CBOW 31.8 32.0 28.9 18.1 33.2 32.7 - 68.3 - 26.6 18.0 40.8
RM-HAND 26.0 35.1 40.7 29.2 43.3 34.0 - 57.4 - 31.4 35.0 35.3
Table 6: Complete results on MNLI Stress Test.
