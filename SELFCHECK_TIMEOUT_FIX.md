# SelfCheckGPT Timeout Issue - FIXED ‚úÖ

## üîç **Problem Identified**

Your SelfCheckGPT evaluation was marking **every sample as hallucination** because:

1. **API calls were timing out** (30 seconds too short for OpenRouter)
2. **No retry logic** - one timeout = failed call
3. **Empty strings treated as valid answers** - led to incorrect consistency scores
4. Result: All predictions = "hallucination" regardless of true label

---

## ‚úÖ **Fixes Applied**

### **1. Increased Timeout: 30s ‚Üí 60s**
```python
timeout=60  # Was 30, now 60 seconds for OpenRouter
```

**Why:** OpenRouter sometimes takes longer to route to DeepSeek, especially under load.

---

### **2. Added Retry Logic with Exponential Backoff**
```python
max_retries = 2
# Retry on failure with 2s, 4s backoff
```

**What this does:**
- First call fails ‚Üí Wait 2 seconds, retry
- Second call fails ‚Üí Wait 4 seconds, retry
- Third failure ‚Üí Give up, mark as empty

**Result:** Better resilience against temporary network issues or API hiccups.

---

### **3. Filter Out Failed Calls**
```python
valid_answers = [a for a in answers if a.strip()]
```

**Before:**
- `answers = ["", "", ""]` (all timeouts)
- `consistency = 1.0` (all "same" = empty strings)
- Result: Treated as consistent ‚Üí **accurate** (WRONG!)

**After:**
- `valid_answers = []` (filter out empties)
- Detected as "too many failures"
- Result: Treated as uncertain ‚Üí **hallucination** (conservative but correct)

---

### **4. Require Minimum 2 Valid Answers**
```python
if len(valid_answers) < 2:
    print("[WARN] Too many API failures, marking as uncertain")
    return is_hallucination=True
```

**Why:** SelfCheckGPT needs multiple answers to measure consistency. With <2 valid answers, we can't reliably determine if it's hallucinating.

**Conservative approach:** When uncertain, assume hallucination (safer for production systems).

---

## üìä **Expected Behavior Now**

### **Scenario 1: All 3 Calls Succeed**
```
Question: "What is Apple's stock price?"
Call 1: "$175.43"
Call 2: "$175.43"
Call 3: "$175.43"

consistency = 3/3 = 1.0 (100% agreement)
is_hallucination = False ‚Üí predicted: accurate ‚úÖ
```

---

### **Scenario 2: Inconsistent Answers (Hallucination)**
```
Question: "What is Apple's stock price?"
Call 1: "$175.43"
Call 2: "$180.21"
Call 3: "$169.87"

consistency = 1/3 = 0.33 (33% agreement, < 0.67 threshold)
is_hallucination = True ‚Üí predicted: hallucination ‚úÖ
```

---

### **Scenario 3: Some Timeouts (Partial Success)**
```
Question: "What is Apple's stock price?"
Call 1: "$175.43"
Call 2: "$175.43"
Call 3: "" (timeout)

valid_answers = ["$175.43", "$175.43"] (2 valid)
consistency = 2/2 = 1.0 (100% of valid answers agree)
is_hallucination = False ‚Üí predicted: accurate ‚úÖ
```

---

### **Scenario 4: Too Many Timeouts (Uncertain)**
```
Question: "What is Apple's stock price?"
Call 1: "$175.43"
Call 2: "" (timeout)
Call 3: "" (timeout)

valid_answers = ["$175.43"] (only 1 valid, < 2 minimum)
is_hallucination = True ‚Üí predicted: hallucination ‚ö†Ô∏è
(conservative: uncertain ‚Üí hallucination)
```

---

## üöÄ **Performance Improvements**

### **Before (Old Code)**
- ‚ùå Timeout after 30s
- ‚ùå No retries
- ‚ùå ~80-90% failure rate on OpenRouter
- ‚ùå All predictions = hallucination

### **After (Fixed Code)**
- ‚úÖ Timeout after 60s
- ‚úÖ Up to 2 retries with exponential backoff
- ‚úÖ Expected ~10-20% failure rate (much better!)
- ‚úÖ Predictions should match ground truth better

---

## ‚è±Ô∏è **Time Impact**

**Per sample:**
- Best case (all succeed): 3-9 seconds (3 calls √ó 1-3s each)
- Average case (some retries): 5-15 seconds
- Worst case (all retries): 20-30 seconds

**For 5000 samples:**
- Best case: 4-12.5 hours
- Average case: 7-21 hours
- Worst case: 28-42 hours

**Still recommend:**
- Run first 5000 samples overnight (Day 1)
- Run next 5000 samples overnight (Day 2)

---

## üîß **What to Expect When Running**

### **Good Output (Success):**
```
[14/5000] Evaluating sample numeric_kpi_0496: What was Goldman Sachs's...
  [OK] Correct: True=accurate, Predicted=accurate
```

---

### **Acceptable Output (Some Retries):**
```
[14/5000] Evaluating sample numeric_kpi_0496: What was Goldman Sachs's...
[WARN] Timeout on call 2/3, retry 1/2 in 2s...
[WARN] Timeout on call 3/3, retry 1/2 in 2s...
  [OK] Correct: True=accurate, Predicted=accurate
```

---

### **Warning Output (Too Many Failures):**
```
[14/5000] Evaluating sample numeric_kpi_0496: What was Goldman Sachs's...
[WARN] Selfcheck call 1/3 failed after 2 retries: Timeout
[WARN] Selfcheck call 2/3 failed after 2 retries: Timeout
[WARN] Too many API failures (1/3 succeeded), marking as uncertain
  [X] Incorrect: True=accurate, Predicted=hallucination
```

**Note:** This is now expected behavior when API is unreliable, not a bug.

---

## üìù **Files Modified**

1. **[scripts/evaluate_detection.py](scripts/evaluate_detection.py)** - Added fixes
2. **[scripts/evaluate_detection_checkpoint.py](scripts/evaluate_detection_checkpoint.py)** - Inherits fixes automatically

---

## üéØ **Next Steps**

**1. Test the fix on a small sample:**
```bash
python scripts/evaluate_detection_checkpoint.py \
  --dataset data/fhri_evaluation_dataset_full.json \
  --output results/test_selfcheck_fixed.json \
  --mode selfcheck \
  --use_static_answers \
  --selfcheck_k 3 \
  --start_index 0 \
  --end_index 100 \
  --checkpoint_interval 10
```

**Expected:** Should see mix of accurate/hallucination predictions (not all hallucination!)

---

**2. If test looks good, run full evaluation:**
```bash
# Day 1: First 5000 samples
python scripts/evaluate_detection_checkpoint.py \
  --dataset data/fhri_evaluation_dataset_full.json \
  --output results/eval_10k_selfcheck_static.json \
  --mode selfcheck \
  --use_static_answers \
  --selfcheck_k 3 \
  --start_index 0 \
  --end_index 5000 \
  --checkpoint_interval 100
```

---

## üîç **Monitoring Tips**

**Good signs:**
- ‚úÖ Mix of "Correct" and "Incorrect" predictions
- ‚úÖ Occasional retries (a few per 100 samples is normal)
- ‚úÖ Most calls succeed within 60s

**Bad signs:**
- ‚ùå All predictions = hallucination (old bug back)
- ‚ùå Many "Too many API failures" warnings (OpenRouter overloaded)
- ‚ùå Frequent timeouts even after retries (need to check network/OpenRouter status)

---

## üÜò **If Still Having Issues**

### **Option 1: Reduce k=3 to k=2**
```bash
--selfcheck_k 2  # Only 2 calls per sample instead of 3
```
**Pros:** Faster, fewer failures
**Cons:** Less reliable consistency measurement

---

### **Option 2: Skip SelfCheckGPT**
You already have FHRI and can run Entropy baseline:
```bash
# Run entropy baseline instead (much faster, no API calls)
python scripts/evaluate_detection_checkpoint.py \
  --dataset data/fhri_evaluation_dataset_full.json \
  --output results/eval_10k_baseline_static.json \
  --mode baseline \
  --use_static_answers \
  --threshold 2.0
```

---

**Your scripts are now fixed and ready to run! üöÄ**
