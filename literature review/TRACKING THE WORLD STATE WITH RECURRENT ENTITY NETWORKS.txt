=== Page 1 ===
arXiv:1612.03969v3  [cs.CL]  10 May 2017Published as a conference paper at ICLR 2017
TRACKING THE WORLD STATE WITH
RECURRENT ENTITY NETWORKS
Mikael Henaff1,2, Jason Weston1, Arthur Szlam1, Antoine Bordes1and Yann LeCun1,2
1Facebook AI Research
2Courant Institute, New York University
{mbh305}@nyu.edu ,{jase,aszlam,abordes,yann }@fb.com
ABSTRACT
We introduce a new model, the Recurrent Entity Network (EntN et). It is equipped
with a dynamic long-term memory which allows it to maintain a nd update a rep-
resentation of the state of the world as it receives new data. For language un-
derstanding tasks, it can reason on-the-ﬂy as it reads text, not just when it is
required to answer a question or respond as is the case for a Me mory Network
(Sukhbaatar et al., 2015). Like a Neural Turing Machine or Di fferentiable Neural
Computer (Graves et al., 2014; 2016) it maintains a ﬁxed size memory and can
learn to perform location and content-based read and write o perations. However,
unlike those models it has a simple parallel architecture in which several memory
locations can be updated simultaneously. The EntNet sets a n ew state-of-the-art
on the bAbI tasks, and is the ﬁrst method to solve all the tasks in the 10k training
examples setting. We also demonstrate that it can solve a rea soning task which
requires a large number of supporting facts, which other met hods are not able to
solve, and can generalize past its training horizon. It can a lso be practically used
on large scale datasets such as Children’s Book Test, where i t obtains competitive
performance, reading the story in a single pass.
1 I NTRODUCTION
The essence of intelligence is the ability to predict. An int elligent agent must be able to predict
unobserved facts about their environment from limited perc epts (visual, auditory, textual, or other-
wise), combined with their knowledge of the past. In order to reason and plan, they must be able to
predict how an observed event or action will affect the state of the world. Arguably, the ability to
maintain an estimate of the current state of the world, combi ned with a forward model of how the
world evolves, is a key feature of intelligent agents.
A natural way for an agent to represent the world is to maintai n a set of high-level concepts or entities
together with their properties, which are updated as new inf ormation is received. For example, if
a percept is the textual description of an event, such as “Joh n walks out of the kitchen”, the agent
should learn to update its estimate of John’s location, as we ll as the list (and number) of people
present in each room. If John was carrying a bag, the location of the bag and the list of objects in
the kitchen must also be updated. When we read a story, each se ntence we read or hear causes us to
update our internal representation of the current state of t he world within the story. The ﬂow of the
story is captured by the evolution of this state of the world.
At any given time, an agent typically receives limited infor mation about the state of the world, and
should therefore be able to infer new information through pa rtial observation. In this paper, we
investigate this problem through a simple story understand ing scenario, in which the agent is given
a sequence of textual statements and events, and then given a nother series of statements about the
ﬁnal state of the world. If the second series of statements is given in the form of questions about the
ﬁnal state of the world together with their correct answers, the agent should be able to learn from
them and its performance can be measured by the accuracy of it s answers.
1

=== Page 2 ===
Published as a conference paper at ICLR 2017
Even with this weak form of supervision, the system may learn basic dynamical constraints about the
world. For example, it may learn that a person or object canno t be in two locations at the same time,
or may learn simple update rules such as incrementing and dec rementing the number of persons
or objects in a room. It may also learn basic rules of approxim ate (logical) inference, such as the
fact that objects belonging to the same category tend to have similar properties (light objects can be
carried over from rooms to rooms for instance).
We propose to handle this scenario with a new kind of memory-a ugmented neural network that
uses a distributed memory and processor architecture: the R ecurrent Entity Network (EntNet). The
model consists of a ﬁxed number of dynamic memory cells, each containing a vector key wjand
a vector value (or content) hj. Each cell is associated with its own “processor”, a simple g ated
recurrent network that may update the cell value given an inp ut. If each cell learns to represent a
concept or entity in the world, one can imagine a gating mecha nism that, based on the key and con-
tent of the memory cells, will only modify the cells that conc ern the entities mentioned in the input.
In the current version of the model, there is no direct intera ction between the memory cells, hence
the system can be seen as multiple identical processors func tioning in parallel, with distributed lo-
cal memory. Alternatively, the EntNet can be seen as a bank of gated RNNs (all sharing the same
parameters), whose hidden states correspond to latent conc epts and attributes, and whose parame-
ters describe the laws of the world according to which the att ributes of objects are updated. The
sharing of these parameters reﬂects an invariance of these l aws across object instances, similarly to
how the weight tying scheme in a CNN reﬂects an invariance of i mage statistics across locations.
Their hidden state is updated only when new information rele vant to their concept is received, and
remains otherwise unchanged. The keys used in the addressin g/gating mechanism also correspond
to concepts or entities, but are modiﬁed only during learnin g, not during inference.
The EntNet is able to solve all 20 bAbI question-answering ta sks (Weston et al., 2015), a popular
benchmark of story understanding, which to our knowledge se ts a new state-of-the-art. Our experi-
ments also indicate that the model indeed maintains an inter nal representation of the simpliﬁed world
in which the stories take place, and that the model does not li mit itself to storing the aspects of the
world required to answer a speciﬁc question. We also introdu ce a new reasoning task which, unlike
the bAbI tasks, requires a model to use a large number of suppo rting facts to answer the question, and
show that the EntNet outperforms both LSTMs and Memory Netwo rks (Sukhbaatar et al., 2015) by
a signiﬁcant margin. It is also able to generalize to sequenc es longer than those seen during training.
Finally, our model also obtains competitive results on the C hildrens Book Test (Hill et al., 2016),
and performs best among models that read the text in a single p ass before receiving knowledge of
the question.
2 M ODEL
Our model is designed to process data in sequential form, and consists of three main parts: an input
encoder, a dynamic memory and an output layer, which we now de scribe in detail. We developed it
in the context of question answering on short stories where t he inputs are word sequences, but the
model could be adapted to many other contexts.
2.1 I NPUT ENCODER
The encoding layer summarizes an element of the input sequen ce with a vector of ﬁxed length.
Typically the input element at time tis a sequence of words, e.g. a sentence or window of words.
One is free to choose the encoding module to be any standard se quence encoder, which is an active
area of research. Typical choices include a bag-of-words (B oW) representation or the ﬁnal state of a
recurrent neural net (RNN) run over the sequence. In this wor k, we use a simple encoder consisting
of a learned multiplicative mask followed by a summation. Mo re precisely, let the input at time tbe
a sequence of words with embeddings {e1,...,ek}. The vector representation of this input is then:
st=/summationdisplay
ifi⊙ei (1)
The same set of vectors {f1,...,fk}are used at each time step and are learned jointly with the oth er
parameters of the model. Note that the model can choose to ado pt a standard BoW representation
2

=== Page 3 ===
Published as a conference paper at ICLR 2017
Figure 1: Diagram of the Recurrent Entity Network’s dynamic memory. Update equations 1 and 2
are represented by the module fθ, whereθis the set of trainable parameters. Equations 3 and 4 are
represented by the gate, since they fullﬁll a similar functi on.
by setting all weights in the multiplicative mask to 1, or can choose a positional encoding model as
used in (Sukhbaatar et al., 2015).
2.2 D YNAMIC MEMORY
The dynamic memory is a gated recurrent network with a (parti ally) block structured weight tying
scheme. We divide the hidden states of the network into block sh1,...,hm; the full hidden state is
the concatenation of the hj. In the experiments below, mis of the order of 5to20, and each block
hjis of the order of 20to100units.
At each time step t, the content of the hidden states {hj}(which we will call the jth memory) are
updated using a set of key vectors {wj}and the encoded input st. In its most general form, the
update equations of our model are given by:
gj←σ(sT
thj+sT
twj) (2)
˜hj←φ(Uhj+Vwj+Wst) (3)
hj←hj+gj⊙˜hj (4)
hj←hj
||hj||(5)
Hereσrepresents a sigmoid, gjis a gating function which determines how much the jthmemory
should be updated, and ˜hjis the new candidate value of the memory to be combined with th e
existing memory hj. The function φcan be chosen from any number of activation functions, in
our experiments we use either parametric ReLU non-linearit ies (He et al., 2015) or the identity. The
matricesU,V,W are typically trainable parameters of the model, and are sha red between all the
blocks. They can also be ﬁxed to certain values, such as the id entity or zero, to yield a simpler
model which we use in some of our experiments.
3

=== Page 4 ===
Published as a conference paper at ICLR 2017
The gating function gjcontains two terms: a “content” term sT
thjwhich causes the gate to open
for memory slots whose content matches the input, and a “loca tion” term sT
twjwhich causes the
gate to open for memory slots whose key matches the input. The ﬁnal normalization step allows
the model to forget previous information. To see this, note t hat since the memories lie on the unit
sphere, all information is contained in their phase. Adding any vector to a given memory (other than
the memory itself) will decrease the cosine distance betwee n the original memory and the updated
one. Therefore, as new information is added, old informatio n is forgotten.
2.3 O UTPUT MODULE
Whenever the model is required to produce an output, it is pre sented with a query vector q. Speciﬁ-
cally, the output is computed using the following equations :
pj=Softmax(qThj)
u=/summationdisplay
jpjhj
y=Rφ(q+Hu)(6)
The matrices HandRare additional trainable parameters of the model. The outpu t module can
be viewed as a one-hop Memory Network (Sukhbaatar et al., 201 5) with an additional non-linearity
φbetween the internal state and the decoder matrix. If the mem ory slots correspond to speciﬁc
words (as we will describe in the following section) which co ntain the answer, pcan be viewed as
a distribution over potential answers and can be used to make a prediction directly or fed into a loss
function, removing the need for the last two steps.
The entire model (all three components described above) is t rained via backpropagation through
time, receiving gradients from any time steps where the read er is required to produce an output,
which are then propagated through the unrolled network.
3 M OTIVATING EXAMPLE OF OPERATION
We now describe a motivating example of how our model can perf orm reasoning on-the-ﬂy as it is
ingesting input sequences. Let us suppose our model is readi ng a story, so the inputs are natural
language sentences, and then it is required to answer questi ons about the story it has just read.
Our model is free to learn the key vectors wjfor each memory j. One choice the model could
make is to associate a single memory (via the key) with each en tity in the story. The memory
slot corresponding to a person could encode that person’s lo cation, the objects they are carrying,
or the people they are with, depending on what information is relevant for the task at hand. As
new information is received indicating that objects are acq uired or discarded, or the person changes
location, their memory slot will change accordingly. Simil arly useful updates can be made for
memories corresponding to object and location entities as w ell.
In fact, we could encode this choice of memories directly int o our model, which we consider as a
type of prior knowledge. By tying the weights of the key vecto rs with the embeddings of speciﬁc
words, we can encourage the model to record information abou t certain words occuring in the text
which we believe to be important. For example, given a list of named entities (which could be
produced by a standard tagger), we could make the model have a separate memory slot for each
entity. We consider this “tied” variant in our experiments. Since the list of entities is independent
of the training data, this variant can handle entities not se en in the training set, as long as their
embeddings can be initialized in a reasonable way (such as pr e-training on a larger corpus).
Now, consider that the model reads the following two sentenc es, and the desired behavior of the
gating function and update function at each memory as they ar e seen:
•Mary picked up the ball.
•Mary went to the garden.
4

=== Page 5 ===
Published as a conference paper at ICLR 2017
As the ﬁrst sentence stis ingested, and assuming memories encode entities, we woul d like the gates
of the memories corresponding to both “Mary” and “ball” to ac tivate. This is possible due to the
location addressing term sT
twjwhich uses the key wj. We expect that a well trained model would
learn to do this. The model would hence modify both the entry c orresponding to “Mary” to indicate
that she is now carrying the ball, and also the entry correspo nding to “ball”, to indicate that it is
being carried by Mary. When the second sentence is seen, we wo uld like the model to again modify
the “Mary” entry to indicate that she is now in the garden, and also modify the “ball” entry to reﬂect
its new location as well. Assuming the information for “Mary ” is contained in the “ball” memory
as described before, the gate corresponding to “ball” can ac tivate due to the content addressing
termsT
thj, even though the word “ball” does not occur in the second sent ence. As before, the gate
corresponding to the “Mary” entry can open due to the second t erm.
If the gating function and update function have weights such that the steps above are executed, then
the memory will be in a state where questions such as “Where is the ball?” or “Where is Mary?” can
be answered from the values of relevant memories, without th e need for further complex reasoning.
4 R ELATED WORK
The EntNet is related to gated recurrent models such as the LS TM (Hochreiter & Schmidhuber,
1997) and GRU (Cho et al., 2014), which also use gates to ﬁx or m odify the information stored in
the hidden state. However, these models use scalar memory ce lls with full interactions between
them, whereas ours has separate memory slots which could be s een as groups of hidden units with
tied weights in the gating and update functions. Another imp ortant difference is the content-based
matching term between the input and hidden state, which is no t present in these models.
Our model also shares some similarities with the DNC/NTM fra mework of (Graves et al., 2014;
2016). There, as in our model, a block of hidden states acts as a set of read-writeable memories. On
the other hand, the DNC has a relatively sophisticated contr oller network (such as an LSTM) which
reads an input and outputs a number of interface vectors (suc h as keys and weightings) which are
then combined via a softmax to read from and write to the exter nal memory matrix. In contrast, our
model can be viewed as a set of separate recurrent models whos e hidden states store the memory
slots. These hidden states are either ﬁxed by the gates, or mo diﬁed through a simple RNN-style
update. The bulk of the reasoning is thus performed by these p arallel recurrent models, rather than
through a central controller. Moreover, instead of using a s oftmax, our model uses an independent
gate for writing to each memory.
Our model is similar to a Memory Network and its variants (Wes ton et al., 2014; Sukhbaatar et al.,
2015; Chandar et al., 2016; Miller et al., 2016) in the way it p roduces an output using a softmax over
blocks of hidden states, and our encoding layer is inspired b y techniques used in those works. How-
ever, Memory Networks explicitly store the entire input seq uence in memory, and then sequentially
update a controller’s hidden state via a softmax gating over the memories. In contrast, our model
keeps a ﬁxed number of blocks of hiddens as memories and updat es each block with an independent
gated RNN. The Dynamic Memory Network of (Xiong et al., 2016) also performs updates via a re-
current model, however it links memories to input tokens and updates them sequentially rather than
in parallel.
The weight tying scheme and the parallel gated RNNs recall th e gated graph network of (Li et al.,
2015). If we interpret our work in that context, the “graph” i s just a set of vertices with no edges;
our gating mechanism is also somewhat different than the one they use. The CommNN model of
(Sukhbaatar et al., 2016), the Interaction Network of ( ?), the Neural Physics Engine of ( ?) and the
model of ( ?) also use a set of parallel recurrent models with tied weight s, but differ from our model
in their use of inter-network communication and the lack of a gating mechanism.
Finally, there is another class of recent models that have a w riteable memory arranged as (un-
bounded) stacks, linked lists or queues (Joulin & Mikolov, 2 015; Grefenstette et al., 2015). Our
model is different from these in that we use a key-value pair a rray instead of a stack, and in the
experiments in this work, the array is of ﬁxed size.
5

=== Page 6 ===
Published as a conference paper at ICLR 2017
Model T= 10T= 20T= 40
MemN2N 0.09 0.633 0.896
LSTM 0 0.157 0.226
EntNet 0 0 0
(a)T20 30 40 50 60 70 80
Error 0 0 0 0.01 0.03 0.05 0.08
(b)
Table 1: a) Error of different models on the World Model Task. b) Generalization of an EntNet
trained up to T= 20 . All errors range from 0 to 1.
5 E XPERIMENTS
In this section we evaluate our model on three different data sets. Training details common to all
experiments can be found in Appendix A.
5.1 S YNTHETIC WORLD MODEL TASK
We ﬁrst study our model’s properties on a toy task designed to measure the ability to keep a world
model in memory. In this task two agents are initially placed randomly on an 10×10grid, and at each
time step a randomly chosen agent either changes direction o r moves ahead. After a certain number
of time steps, the model is required to provide the locations of each of the agents, thus revealing
its internal world model (details can be found in Appendix B) . This task is challenging because the
model must combine up to T−2supporting facts in order to answer the question correctly, and must
also keep the locations of both agents in memory and update th em at different times.
We compared the performance of a MemN2N, LSTM and EntNet. For the MemN2N, we set the
number of hops equal to T−2and the embedding dimension to d= 20 . The EntNet had embedding
dimension d= 20 and 5 memory slots, and the LSTM had 50hidden units which resulted in it having
signiﬁcantly more parameters than the other two models. For each model, we repeated the experi-
ment with 5 different initializations and reported the best performance. All models were trained with
ADAM (Kingma & Ba, 2014) with initial learning rates set by gr id search over{0.1,0.01,0.001}
and divided by 2 every 10,000 updates. Table 1a shows the resu lts. The MemN2N has the worst
performance, which degrades quickly as the length of the seq uence increases. The LSTM performs
better, but still loses accuracy as the length of the sequenc e increases. In contrast, the EntNet is able
to solve the task in all cases.
The ability to generalize to sequences longer than those see n during training is a desirable property,
which suggests that the network has learned the dynamics of t he world it is trying to model. It also
means the model can be trained less expensively. To study thi s, we trained an EntNet on variable
length sequences between 1 and 20, and evaluated it on differ ent length sequences longer than 20.
Results are shown in Table 1b. We see that the model is able to a chieve good performance several
times past its training horizon.
5.2 BABI TASKS
We next evaluate our model on the bAbI tasks, which are a colle ction of 20 synthetic question-
answering datasets ﬁrst introduced in (Weston et al., 2015) designed to test a wide variety of rea-
soning abilities. They have since become a benchmark for mem ory-augmented neural networks and
most of the related methods described in Section 4 have been t ested on them. Performance is mea-
sured using two metrics: the average error across all tasks, and the number of failed tasks (more than
5%error). We used version 1.2 of the dataset with 10k samples.1
Training Details We used a similar training setup as (Sukhbaatar et al., 2015) . All models were
trained with ADAM using a learning rate of η= 0.01, which was divided by 2 every 25 epochs until
200 epochs were reached. Copying previous works (Sukhbaata r et al., 2015; Xiong et al., 2016),
the capacity of the memory was limited to the most recent 70 se ntences, except for task 3 which
was limited to 130 sentences. Due to the high variance in mode l performance for some tasks, for
1Code to reproduce these experiments can be found at
https://github.com/facebook/MemNN/tree/master/EntNe t-babi .
6

=== Page 7 ===
Published as a conference paper at ICLR 2017
Table 2: Results on bAbI Tasks with 10k training samples.
Task NTM D-NTM MemN2N DNC DMN+ EntNet
1: 1 supporting fact 31.5 4.4 0 0 0 0
2: 2 supporting facts 54.5 27.5 0.3 0.4 0.3 0.1
3: 3 supporting facts 43.9 71.3 2.1 1.8 1.1 4.1
4: 2 argument relations 0 0 0 0 0 0
5: 3 argument relations 0.8 1.7 0.8 0.8 0.5 0.3
6: yes/no questions 17.1 1.5 0.1 0 0 0.2
7: counting 17.8 6.0 2.0 0.6 2.4 0
8: lists/sets 13.8 1.7 0.9 0.3 0.0 0.5
9: simple negation 16.4 0.6 0.3 0.2 0.0 0.1
10: indeﬁnite knowledge 16.6 19.8 0 0.2 0 0.6
11: basic coreference 15.2 0 0.0 0 0.0 0.3
12: conjunction 8.9 6.2 0 0 0.2 0
13: compound coreference 7.4 7.5 0 0 0 1.3
14: time reasoning 24.2 17.5 0.2 0.4 0.2 0
15: basic deduction 47.0 0 0 0 0 0
16: basic induction 53.6 49.6 51.8 55.1 45.3 0.2
17: positional reasoning 25.5 1.2 18.6 12.0 4.2 0.5
18: size reasoning 2.2 0.2 5.3 0.8 2.1 0.3
19: path ﬁnding 4.3 39.5 2.3 3.9 0.0 2.3
20: agent’s motivation 1.5 0 0 0 0 0
Failed Tasks ( >5%error): 16 9 3 2 1 0
Mean Error: 20.1 12.8 4.2 3.8 2.8 0.5
each task we conducted 10 runs with different initializatio ns and picked the best model based on
performance on the validation set, as it has been done in prev ious work. In all experiments, our
model had embedding dimension size d= 100 and 20 memory slots.
In Table 2 we compare our model to various other state-of-the -art models in the literature: the larger
MemN2N reported in the appendix of (Sukhbaatar et al., 2015) , the Dynamic Memory Network of
(Xiong et al., 2016), the Dynamic Neural Turing Machine (Gul cehre et al., 2016), the Neural Turing
Machine (Graves et al., 2014) and the Differentiable Neural Computer (Graves et al., 2016). Our
model is able to solve all the tasks, outperforming the other models in terms of both the number of
solved tasks and the average error.
To analyze what kind of representations our model can learn, we conducted an additional experi-
ment on Task 2 using a simple BoW sentence encoding and key vec tors which were tied to entity
embeddings. This was designed to make the model more interpr etable, since the weight tying forces
memory slots to encode information about speciﬁc entities.2After training, we ran the model over
a story and computed the cosine distance between φ(Hhj)and each row riof the decoder matrix
R. This gave us a score which measures the afﬁnity between a giv en memory slot and each word
in the vocabulary. Table 3 shows the nearest neighboring wor ds for each memory slot (which itself
corresponds to an entity). We see that the model has indeed st ored locations of all of the objects and
characters in its memory slots which reﬂect the ﬁnal state of the story. In particular, it has the correct
answer readily stored in the memory slot of the entity being i nquired about (the milk). It also has
correct location information about all other non-location entities stored in the appropriate memory
slots. Note that it does not store useful or correct informat ion in the memory slots corresponding to
2For most tasks including this one, tying key vectors did not s igniﬁcantly change performance, although it
hurt in a few cases (see Appendix C). Therefore we did not appl y it in Table 2
7

=== Page 8 ===
Published as a conference paper at ICLR 2017
Table 3: On the left, the network’s ﬁnal “world model” after r eading the story on the right. First and
second nearest neighbors from each memory slot are shown, al ong with their cosine distance.
Key 1-NN 2-NN
football hallway (0.135) dropped (0.056)
milk garden (0.111) took (0.011)
john kitchen (0.501) dropped (0.027)
mary garden (0.442) took (0.034)
sandra hallway (0.394) kitchen (0.121)
daniel hallway (0.689) to (0.076)
bedroom hallway (0.367) dropped (0.075)
kitchen kitchen (0.483) daniel (0.029)
garden garden (0.281) where (0.026)
hallway hallway (0.475) left (0.060)Story
mary got the milk there
john moved to the bedroom
sandra went back to the kitchen
mary travelled to the hallway
john got the football there
john went to the hallway
john put down the football
mary went to the garden
john went to the kitchen
sandra travelled to the hallway
daniel went to the hallway
mary discarded the milk
where is the milk ?
answer: garden
locations, most likely because this task does not contain qu estions about locations (such as “who is
in the kitchen?”).
5.3 C HILDREN ’SBOOK TEST(CBT)
We next evaluated our model on the Children’s Book Test (Hill et al., 2016), which is a semantic
language modeling (sentence completion) benchmark built f rom children’s books that are freely
available from Project Gutenberg3. Models are required to read 20 consecutive sentences from a
given story and use this context to ﬁll in a missing word from t he 21st sentence. More speciﬁcally,
each sample consists of a tuple (S,q,C,a)whereSis the story consisting of 20 sentences, Qis the
21st sentence with one word replaced by a special blank token ,Cis a set of 10 candidate answers
of the same type as the missing word (for example, common noun s or named entities), and ais the
true answer (which is always contained in C).
It was shown in (Hill et al., 2016) that methods with limited m emory such as LSTMs perform well
on more frequent, syntax based words such as prepositions an d verbs, being similar to human per-
formance, but poorly relative to humans on more semanticall y meaningful words such as named
entities and common nouns. Therefore, most recent methods h ave been evaluated on the Named En-
tity and Common Noun subtasks, since they better test the abi lity of a model to make use of wider
contextual information.
Training Details We adopted the same window memory approach used in (Hill et al ., 2016), where
each input corresponds to a window of text from {w(i−b−1/2)...wi...w(i+(b−1)/2)}centered at a can-
didatewi∈C. In our experiments we set b= 5. All models were trained using standard stochastic
gradient descent (SGD) with a ﬁxed learning rate of 0.001. We used separate input encodings for the
update and gating functions, and applied a dropout rate of 0.5to the word embedding dimensions.
Key embeddings were tied to the embeddings of the candidate w ords, resulting in 10 hidden blocks,
one per member of C. Due to the weight tying, we did not need a decoder matrix and u sed the
distribution over candidates to directly produce a predict ion, as described in Section 3.
We found that a simpler version of the model worked best, with U=V= 0,W=Iandφequal
to the identity. We also removed the normalization step in th is simpliﬁed model, which we found
to hurt performance. This can be explained by the fact that th e maximum frequency baseline model
in (Hill et al., 2016) has performance which is signiﬁcantly higher than random, and including the
normalization step hides this useful frequency-based info rmation.
Results We draw a distinction between two setups: the single-pass se tup, where the model must read
the story and query in order and immediately produce an outpu t, and the multi-pass setup, where
the model can use the query to perform attention over the stor y. The ﬁrst setup is more challenging
3www.gutenberg.org
8

=== Page 9 ===
Published as a conference paper at ICLR 2017
Table 4: Accuracy on CBT test set. Single-pass models encode the document before seeing the
query, multi-pass models have access to the query at read tim e.
Model Named Entities Common Nouns
Single PassKneser-Ney Language Model + cache 0.439 0.577
LSTMs (context + query) 0.418 0.560
Window LSTM 0.436 0.582
EntNet (general) 0.484 0.540
EntNet (simple) 0.616 0.588
Multi PassMemNN 0.493 0.554
MemNN + self-sup. 0.666 0.630
Attention Sum Reader (Kadlec et al., 2016) 0.686 0.634
Gated-Attention Reader (Bhuwan Dhingra & Salakhutdinov, 2 016) 0.690 0.639
EpiReader (Trischler et al., 2016) 0.697 0.674
AoA Reader (Cui et al., 2016) 0.720 0.694
NSE Adaptive Computation (Munkhdalai & Yu, 2016) 0.732 0.714
because the model does not know beforehand which query it wil l be presented with, and must learn
to retain information which is useful for a wide variety of po tential queries. For this reason it can be
viewed as a test of the model’s ability to construct a general -purpose representation of the current
state of the story. The second setup leverages all available information, and allows the model to use
knowledge of which question will be asked when it reads the st ory.
In Table 4, we show the performance of the general EntNet, the simpliﬁed EntNet, as well as other
single-pass models taken from (Hill et al., 2016). The gener al EntNet performs better than the
LSTMs and n-gram model on the Named Entities Task, but lags behind on the Common Nouns
task. The simpliﬁed EntNet outperforms all other single-pa ss models on both tasks, and also per-
forms better than the Memory Network which does not use the se lf-supervision heuristic. However,
there is still a performance gap when compared to more sophis ticated machine comprehension mod-
els, many of which perform multiple layers of attention over the story using query knowledge. The
fact that the simpliﬁed EntNet is able to obtain decent perfo rmance is encouraging since it indicates
that the model is able to build an internal representation of the story which it can then use to answer
a relatively diverse set of queries.
6 C ONCLUSION
Two closely related challenges in artiﬁcial intelligence a re designing models which can maintain an
estimate of the state of a world with complex dynamics over lo ng timescales, and models which can
predict the forward evolution of the state of the world from p artial observation. In this paper, we
introduced the Recurrent Entity Network, a new model that ma kes a promising step towards the ﬁrst
goal. Our model is able to accurately track the world state wh ile reading text stories, which enables
it to set a new state-of-the-art on the bAbI tasks, the compet itive benchmark of story understanding,
by being the ﬁrst model to solve them all. We also showed that o ur model is able to capture simple
dynamics over long timescales, and is able to perform compet itively on a real-world dataset.
Although our model was able to solve all the bAbI tasks using 1 0k training samples, we found that
performance dropped considerably when using only 1k sample s (see Appendix). Most recent work
on the bAbI tasks has focused on the 10k samples setting, and w e would like to emphasize that
solving them in the 1k samples setting remains an open proble m which will require improving the
sample efﬁciency of reasoning models, including ours.
Recent works have made some progress towards the second goal of forward modeling, for instance
in capturing simple physics (Lerer et al., 2016), predictin g future frames in video (Mathieu et al.,
2015) or responses in dialog (Weston, 2016). Although we hav e only applied our model to tasks
9

=== Page 10 ===
Published as a conference paper at ICLR 2017
with textual inputs in this work, the architecture is genera l and future work should investigate how
to combine the EntNet’s tracking abilities with such predic tive models.
REFERENCES
Bhuwan Dhingra, Hanxiao Liu, William Cohen and Salakhutdin ov, Ruslan. Gated-
attention readers for text comprehension. CoRR , abs/1606.01549, 2016. URL
http://arxiv.org/abs/1606.01549 .
Chandar, Sarath, Ahn, Sungjin, Larochelle, Hugo, Vincent, Pascal, Tesauro, Gerald, and Bengio,
Yoshua. Hierarchical memory networks. arXiv preprint arXiv:1605.07427 , 2016.
Cho, Kyunghyun, van Merrienboer, Bart, Bahdanau, Dzmitry, and Bengio, Yoshua. On
the properties of neural machine translation: Encoder-dec oder approaches. In Pro-
ceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Sem antics and Structure
in Statistical Translation, Doha, Qatar, 25 October 2014 , pp. 103–111, 2014. URL
http://aclweb.org/anthology/W/W14/W14-4012.pdf .
Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Clment . Torch7: A matlab-like environment
for machine learning, 2011.
Cui, Yiming, Chen, Zhipeng, Wei, Si, Wang, Shijin, Liu, Ting , and Hu, Guoping. Attention-
over-attention neural networks for reading comprehension .CoRR , abs/1607.04423, 2016. URL
http://arxiv.org/abs/1607.04423 .
Graves, Alex, Wayne, Greg, and Dnihelka, Ivo. Neural Turing Machines, September 2014. URL
http://arxiv.org/abs/1410.5401 .
Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo, Grabska-Barwi´ nska,
Agnieszka, Colmenarejo, Sergio G´ omez, Grefenstette, Edw ard, Ramalho, Tiago, Agapiou, John,
et al. Hybrid computing using a neural network with dynamic e xternal memory. Nature , 2016.
Grefenstette, Edward, Hermann, Karl Moritz, Suleyman, Mus tafa, and Blunsom, Phil. Learning to
transduce with unbounded memory. In Advances in Neural Information Processing Systems , pp.
1828–1836, 2015.
Gulcehre, Caglar, Chandar, Sarath, Cho, Kyunghyun, and Ben gio, Yoshua. Dynamic neural tur-
ing machines with soft and hard addressing schemes. CoRR , abs/1607.00036, 2016. URL
http://arxiv.org/abs/1607.00036 .
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. D elving deep into rectiﬁers: Surpass-
ing human-level performance on imagenet classiﬁcation. CoRR , abs/1502.01852, 2015.
Hill, Felix, Bordes, Antoine, Chopra, Sumit, and Weston, Ja son. The goldilocks principle: Read-
ing children’s books with explicit memory representations . In Proceedings of the International
Conference on Learning Representations . 2016.
Hochreiter, Sepp and Schmidhuber, J¨ urgen. Long short-ter m memory. Neural Comput. , 9(8):
1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/ne co.1997.9.8.1735. URL
http://dx.doi.org/10.1162/neco.1997.9.8.1735 .
Joulin, Armand and Mikolov, Tomas. Inferring algorithmic p atterns with stack-augmented recurrent
nets. arXiv preprint arXiv:1503.01007 , 2015.
Kadlec, Rudolf, Schmid, Martin, Bajgar, Ondrej, and Kleind ienst, Jan. Text under-
standing with the attention sum reader network. CoRR , abs/1603.01547, 2016. URL
http://arxiv.org/abs/1603.01547 .
Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochas tic optimization. CoRR ,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980 .
10

=== Page 11 ===
Published as a conference paper at ICLR 2017
Lerer, Adam, Gross, Sam, and Fergus, Rob. Learning physical intuition of block tow-
ers by example. In Proceedings of the 33nd International Conference on Machin e Learn-
ing, ICML 2016, New York City, NY, USA, June 19-24, 2016 , pp. 430–438, 2016. URL
http://jmlr.org/proceedings/papers/v48/lerer16.html .
Li, Yujia, Tarlow, Daniel, Brockschmidt, Marc, and Zemel, R ichard S. Gated graph sequence neural
networks. CoRR , abs/1511.05493, 2015. URL http://arxiv.org/abs/1511.05493 .
Mathieu, Micha¨ el, Couprie, Camille, and LeCun, Yann. Deep multi-scale video
prediction beyond mean square error. CoRR , abs/1511.05440, 2015. URL
http://arxiv.org/abs/1511.05440 .
Miller, Alexander, Fisch, Adam, Dodge, Jesse, Karimi, Amir -Hossein, Bordes, Antoine, and We-
ston, Jason. Key-value memory networks for directly readin g documents. arXiv preprint
arXiv:1606.03126 , 2016.
Munkhdalai, Tsendsuren and Yu, Hong. Reasoning with memory augmented neu-
ral networks for language comprehension. CoRR , abs/1610.06454, 2016. URL
https://arxiv.org/abs/1610.06454 .
Sukhbaatar, Sainbayar, szlam, arthur, Weston, Jason, and F ergus, Rob. End-
to-end memory networks. In Cortes, C., Lawrence, N. D., Lee, D. D.,
Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Pro-
cessing Systems 28 , pp. 2440–2448. Curran Associates, Inc., 2015. URL
http://papers.nips.cc/paper/5846-end-to-end-memory- networks.pdf .
Sukhbaatar, Sainbayar, Szlam, Arthur, and Fergus, Rob. Lea rning multiagent
communication with backpropagation. CoRR , abs/1605.07736, 2016. URL
http://arxiv.org/abs/1605.07736 .
Trischler, Adam, Ye, Zheng, Yuan, Xingdi, and Suleman, Kahe er. Natural lan-
guage comprehension with the epireader. CoRR , abs/1606.02270, 2016. URL
http://arxiv.org/abs/1606.02270 .
Weston, Jason. Dialog-based language learning. CoRR , abs/1604.06045, 2016. URL
http://arxiv.org/abs/1604.06045 .
Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory n etworks. CoRR , abs/1410.3916,
2014. URL http://arxiv.org/abs/1410.3916 .
Weston, Jason, Bordes, Antoine, Chopra, Sumit, and Mikolov , Tomas. Towards ai-complete
question answering: A set of prerequisite toy tasks. CoRR , abs/1502.05698, 2015. URL
http://arxiv.org/abs/1502.05698 .
Xiong, Caiming, Merity, Stephen, and Socher, Richard. Dyna mic memory networks for visual and
textual question answering. In ICML , 2016.
A T RAINING DETAILS
All models were implemented using Torch (Collobert et al., 2 011). In all experiments, we initialized
our model by drawing weights from a Gaussian distribution wi th mean zero and standard deviation
0.1, except for the PReLU slopes and encoder weights which we re initialized to 1. Note that the
PReLU initialization is related to two of the heuristics use d in (Sukhbaatar et al., 2015), namely
starting training with a purely linear model, and adding non -linearities to half of the hidden units.
Our initialization allows the model to choose when and how mu ch to enter the non-linear regime.
Initializing the encoder weights to 1 corresponds to beginn ing with a BoW encoding, which the
model can then choose to modify. The initial values of the mem ory slots were initialized to the key
values, which we found to help performance. Optimization wa s done with SGD or ADAM using
minibatches of size 32, and gradients with norm greater than 40 were clipped to 40. A null symbol
whose embedding was constrained to be zero was used to pad all sentences or windows to a ﬁxed
size.
11

=== Page 12 ===
Published as a conference paper at ICLR 2017
B D ETAILS OF WORLD MODEL EXPERIMENTS
Two agents are initially placed at random on a 10×10grid with 100 distinct locations
{(1,1),(1,2),...(9,10),(10,10)}. At each time step an agent is chosen at random. There are two
types of actions: the agent can face a given direction, or can move a number of steps ahead. Actions
are sampled until a legal action is found by either choosing t o change direction or move with equal
probability. If they change direction, the direction is cho sen between north, south, east and west with
equal probability. If they move, the number of steps is rando mly chosen between 1 and 5. A legal
action is one which does not place the agent off the grid. Stor ies are given to the network in textual
form, an example of which is below. The ﬁrst action after each agent is placed on the grid is to face
a given direction. Therefore, the maximum number of actions made by one agent is T−2. The
network learns word embeddings for all words in the vocabula ry such as locations, agent identiﬁers
and actions. At question time, the model must predict the cor rect answer (which will always be a
location) from all the tokens in the vocabulary.
agent1 is at (2,8)
agent1 faces-N
agent2 is at (9,7)
agent2 faces-N
agent2 moves-2
agent2 faces-E
agent2 moves-1
agent1 moves-1
agent2 faces-S
agent2 moves-5
Q1: where is agent1 ?
Q2: where is agent2 ?
A1: (2,9)
A2: (10,4)
C A DDITIONAL RESULTS ON B ABI TASKS
We provide some additional experiments on the bAbI tasks, in order to better understand the inﬂu-
ence of architecture, weight tying, and amount of training d ata. Table 5 shows results when a simple
BoW encoding is used for the inputs. Here, the EntNet still pe rforms better than a MemN2N which
uses the same encoding scheme, indicating that the architec ture has an important effect. Tying the
key vectors to entities did not help, and hurt performance fo r some tasks. Table 6 shows results when
using only 1k training samples. In this setting, the EntNet p erforms worse than the MemN2N.
Table 7 shows results for the EntNet and the DNC when models ar e trained on all tasks jointly.
We report results for the mean performance across different random seeds (20 for the DNC, 5 for
the EntNet), as well as the performance for the single best se ed (measured by validation error).
The DNC results for mean performance were taken from the appe ndix of Graves et al. (2016). The
DNC has better performance in terms of the best seed, but also exhibits high variation across seeds,
indicating that many different runs are required to achieve good performance. The EntNet exhibits
less variation across runs and is able to solve more tasks con sistently. Note that Table 2 reports DNC
results with joint training, since results when training on each task separately were not available.
12

=== Page 13 ===
Published as a conference paper at ICLR 2017
Table 5: Error rates on bAbI Tasks with inputs are encoded usi ng BoW. “Tied” refers to the case
where key vectors are tied with entity embeddings.
Task MemN2N EntNet-tied EntNet
1: 1 supporting fact 0 0 0
2: 2 supporting facts 0.6 3.0 1.2
3: 3 supporting facts 7 9.6 9.0
4: 2 argument relations 32.6 33.8 31.8
5: 3 argument relations 10.2 1.7 3.5
6: yes/no questions 0.2 0 0
7: counting 10.6 0.5 0.5
8: lists/sets 2.6 0.1 0.3
9: simple negation 0.3 0 0
10: indeﬁnite knowledge 0.5 0 0
11: basic coreference 0 0.3 0
12: conjunction 0 0 0
13: compound coreference 0 0.2 0.4
14: time reasoning 0.1 6.2 0.1
15: basic deduction 11.4 12.5 12.1
16: basic induction 52.9 46.5 0
17: positional reasoning 39.3 40.5 40.5
18: size reasoning 40.5 44.2 45.7
19: path ﬁnding 74.4 75.1 74.0
20: agent’s motivation 0 0 0
Failed Tasks ( >5%): 9 8 6
Mean Error: 15.6 13.7 10.9
13

=== Page 14 ===
Published as a conference paper at ICLR 2017
Table 6: Results on bAbI Tasks with 1k samples.
Task MemN2N EntNet
1: 1 supporting fact 0 0.7
2: 2 supporting facts 8.3 56.4
3: 3 supporting facts 40.3 69.7
4: 2 argument relations 2.8 1.4
5: 3 argument relations 13.1 4.6
6: yes/no questions 7.6 30.0
7: counting 17.3 22.3
8: lists/sets 10.0 19.2
9: simple negation 13.2 31.5
10: indeﬁnite knowledge 15.1 15.6
11: basic coreference 0.9 8.0
12: conjunction 0.2 0.8
13: compound coreference 0.4 9.0
14: time reasoning 1.7 62.9
15: basic deduction 0 57.8
16: basic induction 1.3 53.2
17: positional reasoning 51.0 46.4
18: size reasoning 11.1 8.8
19: path ﬁnding 82.8 90.4
20: agent’s motivation 0 2.6
Failed Tasks ( >5%): 11 15
Mean Error: 13.9 29.6
14

=== Page 15 ===
Published as a conference paper at ICLR 2017
Table 7: Results on bAbI Tasks with 10k samples and joint trai ning on all tasks.
All Seeds Best Seed
Task DNC EntNet DNC EntNet
1: 1 supporting fact 9.0±12.6 0±0.1 0 0.1
2: 2 supporting facts 39.2±20.5 15.3±15.70.4 2.8
3: 3 supporting facts 39.6±16.4 29.3±26.31.8 10.6
4: 2 argument relations 0.4±0.7 0.1±0.1 0 0
5: 3 argument relations 1.5±1.0 0.4±0.3 0.8 0.4
6: yes/no questions 6.9±7.5 0.6±0.8 0 0.3
7: counting 9.8±7.0 1.8±1.1 0.6 0.8
8: lists/sets 5.5±5.9 1.5±1.2 0.3 0.1
9: simple negation 7.7±8.3 0±0.1 0.2 0
10: indeﬁnite knowledge 9.6±11.4 0.1±0.2 0.2 0
11: basic coreference 3.3±5.7 0.2±0.2 0 0
12: conjunction 5.0±6.3 0±0 0 0
13: compound coreference 3.1±3.6 0±0.1 0 0
14: time reasoning 11.0±7.5 7.3±4.5 0.4 3.6
15: basic deduction 27.2±20.1 3.6±8.1 0 0
16: basic induction 53.6±1.9 53.3±1.2 55.1 52.1
17: positional reasoning 32.4±8.0 8.8±3.8 12.0 11.7
18: size reasoning 4.2±1.8 1.3±0.9 0.8 2.1
19: path ﬁnding 64.6±37.4 70.4±6.1 3.9 63.0
20: agent’s motivation 0.0±0.1 0±0 0 0
Failed Tasks ( >5%): 11.2±5.45±1.2 2 4
Mean Error: 16.7±7.69.7±2.6 3.8 7.38
15
