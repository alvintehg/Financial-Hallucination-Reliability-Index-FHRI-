=== Page 1 ===
FINQA: A Dataset of Numerical Reasoning over Financial Data
Zhiyu Chen1, Wenhu Chen1, Charese Smiley2, Sameena Shah2,
Iana Borova1,Dylan Langdon1,Reema Moussa1,Matt Beane1,Ting-Hao Huang3,
Bryan Routledge4andWilliam Yang Wang1
1University of California, Santa Barbara
2J.P. Morgan
3Pennsylvania State University
4Carnegie Mellon University
{zhiyuchen,william}@cs.ucsb.edu
Abstract
The sheer volume of Ô¨Ånancial statements
makes it difÔ¨Åcult for humans to access and an-
alyze a business‚Äôs Ô¨Ånancials. Robust numeri-
cal reasoning likewise faces unique challenges
in this domain. In this work, we focus on
answering deep questions over Ô¨Ånancial data,
aiming to automate the analysis of a large cor-
pus of Ô¨Ånancial documents. In contrast to ex-
isting tasks on general domain, the Ô¨Ånance do-
main includes complex numerical reasoning
and understanding of heterogeneous represen-
tations. To facilitate analytical progress, we
propose a new large-scale dataset, FINQA,
with Question- Answering pairs over Financial
reports, written by Ô¨Ånancial experts. We also
annotate the gold reasoning programs to en-
sure full explainability. We further introduce
baselines and conduct comprehensive experi-
ments in our dataset. The results demonstrate
that popular, large, pre-trained models fall far
short of expert humans in acquiring Ô¨Ånance
knowledge and in complex multi-step numer-
ical reasoning on that knowledge. Our dataset
‚Äî the Ô¨Årst of its kind ‚Äî should therefore en-
able signiÔ¨Åcant, new community research into
complex application domains. The dataset and
code are publicly available1.
1 Introduction
Financial analysis is a critical means of assessing
business performance, and the consequences of
poor analysis can involve costs of billions of dol-
lars (Jerven, 2013; MacKenzie, 2008). To facilitate
high quality, timely decision making, profession-
als ‚Äî such as analysts or investors ‚Äî perform
complex quantitative analysis to select informa-
tion from Ô¨Ånancial reports. Such analysis demands
advanced expertise in reasoning among heteroge-
neous (structured and unstructured) data sources
and performing complex numerical reasoning, for
example, comparing Ô¨Ånancial ratios of proÔ¨Åtabil-
ity or growth. These challenges are compounded
1https://github.com/czyssrs/FinQAby an exponentially expanding collection of com-
pany Ô¨Ånancial documents (MacKenzie et al., 2012;
Lange et al., 2016) such that it is genuinely unclear
whether dedicated human effort can produce Ô¨Åscal
analysis of sufÔ¨Åcient quality for current decision
making. This poses an interesting question: can we
automate such deep analysis of Ô¨Ånancial data?
A few NLP studies in Question Answering
(QA) explored the numerical reasoning capabilities
needed to answer questions correctly. For exam-
ple, the DROP dataset (Dua et al., 2019) focused
on Wikipedia-based questions that require numer-
ical reasoning, e.g., ‚ÄúWhere did Charles travel to
Ô¨Årst, Castile or Barcelona?‚Äù needs a comparison
between the times of two events. However, most
prior work only targeted the general domain, where
the questions involve much less calculation (mostly
one-step calculation) than that of the Ô¨Ånancial do-
main. Financial QA is more challenging than clas-
sic QA (Rajpurkar et al., 2018; Yang et al., 2018)
because it requires the system to spot relevant in-
formation across heterogeneous sources, such as
tables and unstructured texts, and then create a
numerical reasoning path to connect all the infor-
mation. It also takes substantial knowledge to ask
meaningful Ô¨Ånancial questions. It is not clear how
well the large language models, which performed
well for general-domain QA, can be adapted to
answer realistic, complex Ô¨Ånancial questions.
This paper introduces FINQA, a expert-
annotated dataset that contains 8,281 Ô¨Ånancial QA
pairs, along with their numerical reasoning pro-
cesses. Eleven Ô¨Ånance professionals collectively
constructed FINQAbased on the earnings reports
of S&P 500 companies (Zheng et al., 2021). The
questions in FINQA, such as ‚ÄúConsidering the
weighted average fair value of options, what was
the change of shares vested from 2005 to 2006?‚Äù
(Figure 1) and ‚ÄúWhat was the net change in tax
positions in 2014?‚Äù, require information from both
tables and unstructured texts to answer. The reason-arXiv:2109.00122v3  [cs.CL]  7 May 2022

=== Page 2 ===
200620052004Weighted average fair value of options granted$20.01$9.48$7.28Expected volatility0.35340.32240.3577Distribution yield1.00%0.98%1.30%Expected life of options in years6.36.36.3Risk-free interest rate5%4%4%Page 91 from the annual reports of GRMN (Garmin Ltd.)The fair value for these options was estimated at the date of grant using a Black-Scholes option pricing model with the following weighted-average assumptions for 2006, 2005 and 2004:
‚Ä¶ The total fair value of shares vested during 2006, 2005, and 2004 was $9,413, $8,249, and $6,418 respectively. The aggregate intrinsic values of options outstanding and exercisable at December 30, 2006 were $204.1 million and $100.2 million, respectively. ( ‚Ä¶ abbreviate 10 sentences ... )Question: Considering the weighted average fair value of options , what was the change of shares vested from 2005 to 2006?Answer: - 400Calculations:divide ( 9413, 20.01 )divide ( 8249, 9.48 )substract ( #0, #1 )941320.0182499.48(())-=- 400Program:Figure 1: An example from FINQA: The system needs to learn how to calculate the number of shares, then select relevant
numbers from both the table and the text to generate the reasoning program to get the answer.
ing processes answering these questions are made
of many common calculations in Ô¨Ånancial analysis,
such as addition, comparison, and table aggrega-
tion. To the best of our knowledge, FINQAis the
Ô¨Årst dataset of its kind to tackle complicated QA
tasks based on the real-world Ô¨Ånancial documents.
We propose a retriever-generator QA framework
to Ô¨Årst retrieve supporting facts from Ô¨Ånancial re-
ports, then to generate executable reasoning pro-
grams to answer the questions. Equipped with pre-
trained language models, such as BERT (Devlin
et al., 2019) and RoBERTa (Liu et al., 2019), our
proposed approach outperforms all other baselines
and achieves an execution accuracy of 65.05%.
Although our system outperforms the non-expert
crowd (50.68%), the signiÔ¨Åcant accuracy gap be-
tween the model and human experts (91.16%) mo-
tivates the need for future research.
The main contribution of this work is three-fold:
‚Ä¢We propose the task of QA over Ô¨Ånancial data
to assist Ô¨Ånancial analysis. The task empha-
sizes an important phenomenon for the NLP
community to study and analyze how the cur-
rent pre-trained models perform on complex
and specialized domains.
‚Ä¢We construct a new large-scale dataset,
FINQA, with 8,281 examples written by Ô¨Ånan-
cial experts, with fully annotated numerical
reasoning programs.
‚Ä¢We experiment on various baselines and Ô¨Ånd
that the models are still far behind expert per-
formance, strongly motivating future research.2 Related Work
Questions Answering. There have been several
QA datasets involving numerical understandings
and calculations. The major source is from struc-
tured tables or knowledge bases, owning the nature
to succinctly organize numerical information. Pop-
ular datasets include ComplexWebQuestions (Tal-
mor and Berant, 2018), WikiTableQuestions (Pa-
supat and Liang, 2015), Spider (Yu et al., 2018),
TabFact (Chen et al., 2020b), etc. For reading com-
prehension, the dataset most related to ours is the
DROP dataset (Dua et al., 2019), which applies
simple calculations over texts. The top methods on
DROP typically use speciÔ¨Åc prediction heads for
each kind of calculation. HybridQA (Chen et al.,
2020c) targets QA over both the table and the text,
but not with the focus of numerical reasoning. All
these existing datasets are built upon the general do-
main (mostly based on Wikipedia). In contrast, our
dataset focus on the Ô¨Ånance domain, which demon-
strates much more complex nature in numerical
reasoning questions, combining both the structured
tables and unstructured texts. Another kind of QA
datasets related to ours is the math word problem
datasets, like MaWPS (Koncel-Kedziorski et al.,
2016), MathQA (Amini et al., 2019). The task is to
generate the solution programs given a short input
math problem. Existing models include (Kim et al.,
2020; Chen et al., 2020a,d), etc.
Financial NLP. Financial NLP has become one
of the major application domains attracting grow-
ing attentions. Previous works in Ô¨Ånance domain
include risk management to detect fraud (Han et al.,
2018; Wang et al., 2019; Nourbakhsh and Bang,
2019), sentiment analysis to assist market predic-
tion (Day and Lee, 2016; Wang et al., 2013; Akhtar

=== Page 3 ===
et al., 2017), opinionated Question Answering (Liu
et al., 2020), such as the FiQA2dataset built from
forums and social media. Recent works attempt to
develop pre-trained models specialized for Ô¨Ånance
domain (Yang et al., 2020; Araci, 2019), and the
downstream tasks are mostly sentiment classiÔ¨Åca-
tions. To the best of our knowledge, there is no
previous work and dataset on building QA systems
of numerical reasoning on Ô¨Ånancial reports.
3 Task DeÔ¨Ånition
Problem Formulation. Presented with a Ô¨Ånan-
cial report consisting of textual contents Eand
structured table T, given a question Q, the
task is to generate the reasoning program G=
fw0; w1; :::w ng, where wiis the program tokens
deÔ¨Åned by domain speciÔ¨Åc language (DSL), then it
is executed to get the answer A:
P(AjT; E; Q ) =X
P(GijT; E; Q )(1)
WherefGigis all the correct programs to evaluate
to the answer. For Ô¨Ånancial tables, there is typi-
cally a description header (blue header in Figure 1),
which often gives the timing information; and each
row has its name on the left. Some of the Ô¨Ånancial
tables may demonstrate more complicated layouts,
e.g., nested structures. As a Ô¨Årst step for this di-
rection, in this paper we only focus on the regular
layout cases for simplicity.
Domain SpeciÔ¨Åc Language. In this work, we
use DSL consisting of mathematical operations
and table operations as executable programs. The
program consists of a sequence of operations:
op1[args1];op2[args2]:::;opn[argsn] (2)
Each operation takes a list of arguments
args n. On consulting with Ô¨Ånancial experts,
as most of the accounting and Ô¨Ånancial val-
uation theory primarily include linear algebra,
we include 10 common types of operations in
our dataset. There are 6 mathematical opera-
tions: add,subtract ,multiply ,divide ,
greater ,exp, and 4 table aggregation opera-
tions table-max ,table-min ,table-sum ,
table-average , that apply aggregation opera-
tions on table rows. The mathematical operations
take arguments of either numbers from the given
reports, or a numerical result from a previous step;
2https://sites.google.com/view/Ô¨Åqa/homeThe table operations take arguments of table row
names. We use the special token #nto denote the
result from the nth step. For example, in Figure 1,
the program consists of 3 steps; The Ô¨Årst and the
second division steps take arguments from the table
and the text, respectively, then the third step sub-
tracts the results from the two previous steps. Refer
to Appendix A for more details of the operations
and the grammars.
Evaluations. Previous studies on QA with nu-
merical reasoning only evaluate the execution ac-
curacy, i.e., the Ô¨Ånal results from the generated
programs, such as DROP (Dua et al., 2019) and
MathQA (Amini et al., 2019). However, the ap-
plications for the Ô¨Ånance domain generally pose
much higher requirements of explainability and
transparency. Therefore, we also provide the gold
programs for our dataset. Besides execution accu-
racy, we also propose to evaluate the accuracy of
the generated programs. SpeciÔ¨Åcally, we replace all
the arguments in a program with symbols, and then
we evaluate if two symbolic programs are mathe-
matically equivalent . For example, the following
two programs are equivalent programs:
add(a1; a2);add(a3; a4);subtract (#0;#1)
add(a4; a3);add(a1; a2);subtract (#1;#0)
Note that execution accuracy tends to overestimate
the performance because sometimes the model just
hit the correct answer by chance; While program ac-
curacy tends to produce false negatives since some
questions may have multiple correct programs.
4 The F INQA Dataset
4.1 Data Preparation
Data Source. We develop FINQAbased on the
publicly available earnings reports of S&P 500
companies from 1999 to 2019, collected in the
FinTabNet dataset (Zheng et al., 2021). An earn-
ings report is a set of pages in a PDF Ô¨Åle that out-
lines the Ô¨Ånancials of a company, which usually
contains tables and texts. The FinTabNet dataset
has annotated the tables in each report.
Data Filtering. Realistic earnings reports con-
tain many tables not suitable for numerical reason-
ing tasks. Equipped with the table annotations in
FinTabNet, we Ô¨Ålter the data as follows: First, we
extract the pages in earnings reports with at most
one table. Second, we exclude the tables with over
20 rows, over 2 description headers, or with other

=== Page 4 ===
complex nested structures. We also exclude the ta-
bles with tedious contents, such as catalogs, which
is common in FinTabNet. As stated in ¬ß3, these
over-complicated tables are out of the scope of this
work. Finally, for the tables with 2 description
headers, we merge them into a single header to
simplify the representations. As a result, a total of
12,719 pages were selected for further annotation.
4.2 Annotation Procedure
Recruiting Expert Annotators. We post job ads
on UpWork3and hire eleven US-based experts with
professional Ô¨Ånance backgrounds (CPAs, MBAs,
etc.) Each hire is interviewed using four exam-
ple report pages and asked to compose example
Q&A pairs. After hiring, each annotator Ô¨Årst goes
through a training session to learn the task and the
annotation interface (Appendix D). When the work-
ers fully master the annotation process, we launch
the ofÔ¨Åcial batches for them to work on.
An annotator can compose up to two questions
for each given report page or skip if it is hard to
compose any meaningful question. We pay around
$2.0 for each question, which leads to an average
hourly wage of $35.0. The whole data collection
took around eight weeks.
We do not use popular micro-task platforms,
such as Amazon Mechanical Turk (MTurk), be-
cause our preliminary studies show that many
MTurk workers can not perform this task effec-
tively. Our experiment with MTurk workers in ¬ß 4.3
further echo this observation. As most existing QA
datasets were constructed by MTurk workers (Yang
et al., 2018; Dua et al., 2019; Chen et al., 2020c),
it requires substantial domain-speciÔ¨Åc knowledge
to compose meaningful questions that are hard for
computers to answer.
Annotation Task Design. For each page se-
lected in ¬ß4.1, the annotators are asked to (i)write
a meaningful Ô¨Ånancial question, (ii)compose a rea-
soning program to answer the question, and (iii)to
annotate the supporting fact. Each page is assigned
to one or two experts for annotation. We detail
each part as follows. (I) Financial question: For
a given page of earnings reports, the annotators are
asked Ô¨Årst to compose a question that is ‚Äúmeaning-
ful for Ô¨Ånancial analysis or learning insights of the
company Ô¨Ånancial reports‚Äù and require numerical
calculations to answer. We encourage the experts
3UpWork (www.upwork.com) is a platform where re-
questers can recruit skilled freelancers.to write questions that require the information from
both the text and the table to answer. (II) Reason-
ing program: After providing the question, the
annotators are then asked to elaborate the oper-
ation steps to answer the question. SpeciÔ¨Åcally,
they compose a maximum of 5 steps of operation,
where each operation has four slots: ‚Äúoperation‚Äù,
‚Äúargument1‚Äù, ‚Äúargument2‚Äù, and ‚Äúresult‚Äù. The ‚Äúop-
eration‚Äù is one of the ten predeÔ¨Åned operations
described in ¬ß3. An ‚Äúargument‚Äù is a number or a
table‚Äôs row name, either from the report or a previ-
ous step‚Äôs result. For operations that only use one
argument, such as table aggregation, workers can
leave argument2 blank. The annotation interface
(see Appendix D) automatically validates the in-
puts to ensure correctness. (III) Supporting fact:
We also ask the annotators to mark all the sentences
in the text and the table rows that contain the infor-
mation needed to answer the question.
4.3 Data Quality Assessment
External experts answer F INQA questions with
a high accuracy and a high inter-annotator
agreement. To validate the quality of the anno-
tations, as well as to set up human expert perfor-
mance upper bound, we hire another two Ô¨Ånancial
professionals on UpWork. We randomly sample
200 examples from our dataset, and ask the pro-
fessionals to answer the questions as well as write
the operation steps, following the same procedure
as in the dataset construction. The payment is
$2.0 per question. For execution accuracy, they
reach 92.25% and 90.06%, respectively (mean =
91.16%). For program accuracy, they reach 89.44%
and 85.53% (mean = 87.49%). The agreements be-
tween the two annotators are 92.65% for execution
accuracy, and 86.76% for program accuracy.
Non-expert crowd workers answer F INQA
questions with a low accuracy. We also test
how well non-expert MTurk workers can answer
FINQAquestions. We distribute the samples to
MTurk4and take the similar process to distribute
each example to two workers. We end up with
an average execution accuracy of 50.68% and a
program accuracy of 48.17%, which is far below
the expert performance; the agreement rate is only
around 60%. These results echo our preliminary
study‚Äôs observations for MTurk workers in ¬ß4.2.
4Three built-in worker qualiÔ¨Åcations are used: HIT Ap-
proval Rate ( 95%), Number of Approved HITs ( 3000 ),
and Locale (US Only) QualiÔ¨Åcation. We do not select any
profession constraints. We pay $2.0 for each question.

=== Page 5 ===
Examples (Q&A pairs with program, fact) 8,281
Report pages 2,789
V ocabulary 22.3k
Avg. # sentences in input text 24.32
Avg. # tokens in input text 628.11
Avg. # rows in input table 6.36
Avg. # tokens in input table 59.42
Avg. # tokens in all inputs (text & table) 687.53
Max. # tokens in all inputs (text & table) 2,679
Avg. question length 16.63
Table 1: Statistics of F INQA.
4.4 Data Analysis
FINQAcontains 8,281 examples. The data is re-
leased as training (6,251), validation (883), and
test (1,147) following an 75%/10%/15% split. The
three sets do not have overlapping input reports.
We quantitatively analyze some key properties of
FINQA. Table 1 shows the general statistics.
Statistics of Supporting Facts. InFINQA,
23.42% of the questions only require the informa-
tion in the text to answer; 62.43% of the questions
only require the information in the table to answer;
and 14.15% need both the text and table to an-
swer. Meanwhile, 46.30% of the examples have
one sentence or one table row as the fact; 42.63%
has two pieces of facts; and 11.07% has more than
two pieces of facts. For the examples with more
than one piece of fact, we also calculate the max-
imum distances between all the same example‚Äôs
facts. 55.48% has a maximum distance of 3 or less
sentences5; 24.35% has a maximum distance of 4-6
sentences; and 20.17% has over 6 sentences.
Statistics of Reasoning Programs. In the pro-
grams, the most frequent operations, add,
subtract ,multiply , and divide , have the
distributions of 14.98%, 28.20%, 5.82%, and
45.29%, respectively. The operation division
has the highest frequency, as calculating ratios is
common in Ô¨Ånancial analysis. In FINQA, 59.10%
of the programs have 1 step, 32.71% have 2 steps,
and the rest 8.19% have 3 or more steps.
5 Baseline Systems
In this section, we Ô¨Årst describe our main base-
line framework FinQANet in ¬ß5.1, and then we
introduce other baselines in ¬ß5.2.
5For tables, we consider one row as one ‚Äúsentence‚Äù.
The fair value for these options was estimated at the date of grant using a Black-Scholes option pricing model with the following weighted-average assumptions for 2006, 2005 and 2004:
The total fair value of shares vested during 2006, 2005 was $9,413, $8,249 respectively. The aggregate intrinsic values of options outstanding and exercisable at December 30, 2006 were $204.1 million and $100.2 million, respectively.The total fair value of shares vested during 2006, 2005 was $9,413, $8,249 respectively. Financial ReportRetrieved FactsFigure 2: The retriever retrieves supporting facts (text sen-
tences or table rows) from the input Ô¨Ånancial report.
5.1 The FinQANet Framework
As a preliminary attempt on FINQA, we propose
FinQANet , with a retriever to Ô¨Årst retrieve the sup-
porting facts from the input Ô¨Ånancial report, then a
generator to generate the program to get the answer.
Retriever The full page of the Ô¨Ånancial report
can go beyond 2,000 tokens, which cannot be
coped with the current popular QA models (De-
vlin et al., 2019). Therefore we Ô¨Årst retrieve the
supporting facts from the input report. For the
tables, we use templates to turn each row into sen-
tences. For example, the last row of the table in
Figure 1 is represented as ‚Äòthe risk-free interest
rate of 2006 is 5%; ...‚Äô. We concatenate each sup-
porting fact with the question and train a classiÔ¨Åer
using pre-trained LMs like BERT (Devlin et al.,
2019). Then we take the top n retrieved facts, re-
ordered as they appear in the input report. This
set of retriever results will serve as the input to the
second phase. Figure 2 illustrates the retrieving
procedure. Another common strategy is sliding
window (Alberti et al., 2019). We take the sliding
window of a Ô¨Åxed size with a stride to go through
the report, then the windows containing all the sup-
porting facts are marked as positive. However, we
observe in the experiments that the length of the
input to the program generator in the second phase
greatly inÔ¨Çuences the performance. The perfor-
mance of using sliding window falls far behind the
previous method.
Program Generator Given the retrieved sup-
porting facts from the retriever, the program gen-
erator aims to generate the executable program to
answer the question. Figure 3 gives an overview of
the program generator. The generated tokens come
from 3 sources: 1) The input passage (retriever out-
put) and the question tokens feig, like the numbers
or the table row names. 2) The special tokens fsig
from the DSL, like the function names, predeÔ¨Åned

=== Page 6 ===
Input encoderStep memory embeddings
9413add()8249#0divide(Step memory embeddings
)8249Step memory embeddings#0#1......was$9413add()...Special token embeddingsInput embeddingsAttentionsConcat............was$9413Output spacePredicted token#0#1...#0#1...Update memoryLSTM decoderFigure 3: The program generator. The retriever results and the question are Ô¨Årst encoded using pre-trained LMs. At each
decoding step, the model can generate from the numbers or table row names from the input, the special tokens in the DSL, or the
step memory tokens. At the end of the generation of each operation step, we update the step memory token embeddings.
constants, etc. 3) The step memory tokens fmig
to denote the results from previous steps, like #0,
#1, etc. We Ô¨Årst use pre-trained LMs to encode
feig, denote the output embeddings as fhe
ig. The
embeddings of the special tokens and the step mem-
ory tokens are randomly initialized and denoted as
fhs
igandfhm
igrespectively. Denote all the token
embeddings H= [he
i;hs
i;hm
i].
An LSTM is used for decoding. At each decod-
ing step T, the program token embeddings Hare
fed as the input; The decoder output hTis used
to calculate the attention vector attpandatthover
the input and the decoding history. Then a context
vector cTcombines all the contextual information:
cT=Wc[attp;atth;hT] (3)
Meanwhile, another attention vector att0
pover the
input is applied to all the token embeddings:
H0
T=Wh[H;Hatt0
p] (4)
Different from other program tokens, the step mem-
ory tokensfmigimply the reasoning path of the
program. To make use of such structure informa-
tion, at each decoding step indicating the end of
oneoperation [args]unit, i.e., the step to generate
the ending parentheses in our DSL, we compute
another context vector aT:
aT=Wa[attp;atth;hT] (5)
Then the step memory token embedding corre-
sponding to the current step is updated as aT.
The Ô¨Ånal prediction is calculated with:
wT=softmax (H0
TcT) (6)
During inference time, based on the grammar of
the DSL, we use masks at each decoding step to
ensure the structural correctness of the generated
programs. In the retriever phase, we take the topn retrieved results as the input to the program gen-
erator. Therefore, for the training of the program
generator, we use the retriever result on the training
set (combined with the gold facts if there is any
wrong prediction) as the input.
5.2 Other Baselines
TF-IDF + Single Op. We use TF-IDF to retrieve
the top 2 sentences from the input report. Since the
most common case in our dataset is one-step pro-
gram and the most common operation is division,
we take the Ô¨Årst number from each sentence and
apply the division operation.
Retriever + Direct Generation. To demonstrate
the necessity of generating the reasoning programs,
we keep the architecture the same as our model, but
directly generating the Ô¨Ånal results.
Retriever + Seq2seq. We use a Seq2seq architec-
ture for the generator, similar to the Seq2seq base-
line in the MathQA dataset (Amini et al., 2019). A
bi-LSTM is used for encoding the input, and then
an LSTM is used for decoding with attention.
Retriever + NeRd. The Neural Symbolic
Reader(NeRd) (Chen et al., 2020d) is also a pointer-
generator based model for program generation,
with the state of the art results on the MathQA
dataset (Amini et al., 2019). Different from ours,
it directly learns the program with nested format
as a sequence, i.e., without the step memory to-
kens. This way the model is able to learn the pro-
gram structures as patterns from very large-scale
data (~40k for MathQA), but may fail on learning
the reasoning paths. We keep the retriever part
the same and compare with the generator part to
demonstrate the usefulness of structure learning.
Pre-Trained Longformer. There are also works
on modeling very long documents with thousands
of characters, with the attention mechanism that

=== Page 7 ===
scales linearly with sequence length, like the Long-
former (Beltagy et al., 2020). To demonstrate the
necessity of breaking up into the pipeline of re-
triever and program generator, we remove the re-
triever and directly use the pre-trained Longformer
as the input encoder in the program generator, and
encode the whole report. The table rows are lin-
earized similar as in ¬ß5.1.
6 Experimental Results
Experiment Setups. For the retriever, we use
BERT-base as the classiÔ¨Åer (other pre-trained mod-
els perform similarly). Since most of the examples
in our dataset have 1 or 2 facts, and we Ô¨Ånd that
longer inputs lower the performance of the pro-
gram generator, we take the top 3 ranked facts as
the retriever results. For the program generator, we
experiment on using BERT (Devlin et al., 2019),
RoBERTa (Liu et al., 2019), and FinBert (Araci,
2019) as the encoder, to test the performances of
popular large pre-trained models. For all models,
we use the Adam optimizer (Kingma and Ba, 2015).
Check Appendix B for more details of training and
parameter settings.
6.1 QA Model Performance
Table 2 presents the results for all the baseline sys-
tems. We evaluate the execution accuracy (exe acc)
and program accuracy (prog acc) as explained in
¬ß3. For the BERT-based retriever, we have 89.66%
recall for the top 3 retrieved facts and 93.63% re-
call for the top 5. Using TF-IDF results in 82.91%
recall for the top 5 facts. We use the same retriever
results for all retriever-generator based models.
Directly generating the execution results gives near-
zero scores, which indicates the necessity of gen-
erating the reasoning programs. If without using
the retriever-generator pipeline, but directly apply-
ing an end-to-end pre-trained Longformer model,
the performance falls far behind. Because longer
inputs have more numbers which put more confu-
sions on the program generator and thus make it
harder to learn. Generally, the program generators
using pre-trained models perform much better than
the Seq2seq baseline, as there is language model-
ing knowledge that can also be used for the Ô¨Ånance
domain. And larger pre-trained models give better
performance, as they tend to see more Ô¨Ånancial
corpus during their pre-training. FinBert (Araci,
2019) is a pre-trained model for the Ô¨Ånance domain;
its main downstream tasks are sentiment analysis.Baselines Exe Acc Prog Acc
TF-IDF + Single Op 1.01 0.90
Retriever + Direct Generation 0.30 -
Pre-Trained Longformer (base) 21.90 20.48
Retriever + Seq2seq 19.71 18.38
Retriever + NeRd (BERT-base) 48.57 46.76
FinQANet (FinBert) 50.10 47.52
FinQANet (BERT-base) 50.00 48.00
FinQANet (BERT-large) 53.52 51.62
FinQANet (RoBERTa-base) 56.10 54.38
FinQANet (RoBERTa-large) 61.24 58.86
FinQANet-Gold (RoBERTa-large) 70.00 68.76
Human Expert Performance 91.16 87.49
General Crowd Performance 50.68 48.17
Table 2: The execution accuracy (Exe Acc) and program
accuracy (Prog Acc) for all the models. Although our best
system (61.24%) outperforms the non-expert crowd (50.68%),
the signiÔ¨Åcant accuracy gap between the model and human
experts (91.16%) motivates the need for future research.
The performance of using FinBert is no better than
BERT-large, mostly because its pre-training cor-
pus is limited (~30M words from news articles).
Comparing FinQANet with the retriever + NeRd
baseline (Chen et al., 2020d), it shows the improve-
ments from learning the logical structure of the pro-
grams. We also run the program generator using
the gold retriever result, shown as FinQANet-Gold.
Another interesting observation is the comparisons
with human performances. While there is still a
large gap from the human expert upper bound, the
best performing model already surpasses the gen-
eral crowd performance.
6.2 Performance Breakdown
We conduct a set of performance breakdowns using
the FinQANet (RoBERTa-large) model. Table 3
shows all the results.
Necessity of using both table and text. We run
inferences taking facts only from a single source
from the retriever. Inferences on individual source
(table-only: 45.81%, text-only: 15.80%) are both
far behind the full results (61.24%).
The model performs the best on the table-only
questions. The model performs the best on table-
only questions (67.38%). Tables tend to have more
uniÔ¨Åed structures and might be easier for the model
to learn. Table 3 also shows that the questions
involving both tables and texts are the most chal-

=== Page 8 ===
Gold program: subtract(746, 554)Predicted program: multiply(554, const_1000000)Gold supporting facts: text sentence(s)Question: what is the amount of credit lines that has been drawn in millions as of year-end 2016?[1] additionally , we have other committed and uncommitted credit lines of $ 746 million with major international banks and financial institutions to support our general global funding needs , including with respect to bank supported letters of credit, performance bonds and guarantees .[2] approximately $ 554 million of these credit lines were available for use as of year-end 2016 .sharesweighted average grant-date fair valuenon-vested at may 31 200976242non-vested at may 31 201071342Gold supporting facts: table row(s)Question: what is the percentage change in the total fair value of non-vested shares from 2009 to 2010?Gold program: multiply(762, 42), multiply(713, 42), subtract(#1, #0), divide(#2, #0)Predicted program: subtract(713, 762), divide(#0, 762)Gold supporting facts: text sentence(s)Question: what is the estimated percentage of revolving credit facility in relation with the total senior credit facility in millions?Gold program: multiply(1.4, const_1000), divide(945.5, #0)Predicted program: divide(945.5, const_1000)[1] we maintained a $ 1.4 billion senior credit facility with various financial institutions , including the $ 420.5 million term loan and a $ 945.5 million revolving credit facility .Error case (1)Error case (2)Error case (3)Figure 4: Error cases. In these examples, the retriever results all correctly cover the gold facts; thus we only present the gold facts,
gold program, and the predicted program to study the errors of the program generator. We give more error cases in Appendix
C, including the cases for the retriever errors. Example 1 : The Ô¨Ånancial knowledge to calculate the ‚Äòcredit lines that has been
drawn‚Äô. Example 2 : Complex reasoning of 4 steps. Example 3 : Number unit conversion between ‚Äòbillion‚Äô and ‚Äòmillion‚Äô.
Methods Exe Acc Prog Acc
full results 61.24 58.86
Necessity of table and text
table-only inference 45.81 43.62
text-only inference 15.80 15.33
Performances on table and text
table-only questions 67.38 64.48
text-only questions 54.86 53.70
table-text questions 43.80 41.61
Performances regarding program steps
1 step programs 67.61 65.28
2 step programs 59.08 56.37
>2 step programs 22.78 21.52
Programs with constants 43.88 39.80
Table 3: Performance breakdown of FinQANet (RoBERTa-
large). The model beneÔ¨Åts from using both table and text, as
inferences on individual source yield much lower performance.
FinQANet is better at answering table-only questions, and the
questions that require more steps to solve are indeed more
challenging to the model.
lenging ones for the model (43.80%).
Questions that need more than two steps to an-
swer are challenging. The model has a low ac-
curacy (22.78%) on the questions that need three
or more steps. Meanwhile, not surprisingly, the
questions that require only one step are the easiest.
Constants in programs. Many programs in
FINQAcontain constants as arguments. A constant
is often used to convert an English number word
to another. For example, we need Ô¨Årst to use theconstant ‚Äú1,000‚Äù to convert ‚Äú1.5 billion‚Äù to ‚Äú1,500
million‚Äù so that it can be added with ‚Äú50 million‚Äù.
A constant is also used to explicate the implicit
numbers hidden in the language. For example, to
calculate ‚Äúthe average for the year 2012, 2013, and
2014‚Äù, the program needs to use the constant ‚Äú3‚Äù as
the denominator, which is not mentioned explicitly
in the text. As shown in Table 3, the programs with
constants yield great challenges for our model, as
the performance (43.88%) is much lower than that
of the whole set (61.24%).
6.3 Error Analysis
We sample 50 error cases from the results of the
FinQANet (RoBERTa-large) model and analyze
them manually. 15% of the errors are caused by
the retriever, e.g., missing facts. Half of the rest are
due to the lack of Ô¨Ånancial knowledge, such as the
meaning of some terminology. And the rest half
are primarily numerical reasoning errors, includ-
ing complex programs with multiple steps, numeri-
cal unit conversions, or resolving the ordering and
matching of the numbers and the years. Many error
cases involve both the numerical reasoning prob-
lems and misunderstandings of Ô¨Ånancial knowl-
edge. We show three representative error cases in
Figure 4.
7 Conclusion and Future Work
This paper introduces FINQA, a new expert-
annotated QA dataset that aims to tackle numerical
reasoning over real-world Ô¨Ånancial data. The ques-
tions in FINQApose great challenge for existing
models to resolve domain-speciÔ¨Åc knowledge, as
well as to acquire complex numerical reasoning

=== Page 9 ===
abilities. We propose baseline frameworks and con-
duct comprehensive experiments and analysis. The
results show that current large pre-trained models
still fall far behind the human expert performance.
This encourages potential future work on develop-
ing pre-training tasks for such realistic, complex
application domains. We believe FINQAshould
serve as a valuable resource for the research com-
munity.
8 Ethical Considerations
Data Access and Licensing. We develop
FINQAbased on the publicly available earnings
reports of S&P 500 companies from 1999 to 2019,
collected in the FinTabNet dataset (Zheng et al.,
2021). The FinTabNet dataset is publicly available
under the CDLA-Permissive6license, which
permits us to create additional annotations on top
of the data (‚ÄúEnhanced Data‚Äù, ¬ß1.5 of CDLA)
and publish the annotations (‚ÄúPublish‚Äù, ¬ß1.9 of
CDLA).
Dataset Collection Process and Conditions.
For the annotation of our FINQAdataset on Up-
work, we Ô¨Årst launch interviews of the task intro-
duction with 4 example questions, which is paid as
$30, for them to try a few examples to get informed
and familiar with the task. Then based on their con-
sents to continue working on the large-scale job,
we discuss with the workers to reach agreements
on the compensation before starting the large-scale
job. We pay around $2.0 per question, and the
hourly rates are discussed and agreed upon with
both sides based on the working speed of differ-
ent workers. Among all eleven US-based hires,
the average hourly rate is $35.0, and the minimum
and maximum hourly rates are $20 and $50, re-
spectively. The evaluation tasks follow the similar
procedure, and each question is paid as $2.0.
IRB (Institutional Review Board) Approval.
This project is approved by our Institutional Review
Board (IRB). The systems trained using our dataset
are primarily intended to be used as augmenting
human decision-making in Ô¨Ånancial analysis, but
not as a replacement of human experts.
Acknowledgment
We thank the anonymous reviewers for their
thoughtful comments. This research was supported
6CDLA-Permissive: https://cdla.dev/sharing-1-0/by the J.P. Morgan Faculty research award. The au-
thors are solely responsible for the contents of the
paper and the opinions expressed in this publication
do not reÔ¨Çect those of the funding agencies.
References
Md. Shad Akhtar, Abhishek Kumar, Deepanway
Ghosal, Asif Ekbal, and Pushpak Bhattacharyya.
2017. A multilayer perceptron based ensemble
technique for Ô¨Åne-grained Ô¨Ånancial sentiment anal-
ysis. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September 9-
11, 2017 , pages 540‚Äì546. Association for Computa-
tional Linguistics.
Chris Alberti, Kenton Lee, and Michael Collins. 2019.
A BERT baseline for the natural questions. CoRR ,
abs/1901.08634.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. Mathqa: Towards interpretable math
word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers) , pages 2357‚Äì2367. Association for Computa-
tional Linguistics.
Dogu Araci. 2019. Finbert: Financial sentiment
analysis with pre-trained language models. CoRR ,
abs/1908.10063.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
CoRR , abs/2004.05150.
Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul
Smolensky, Kenneth D. Forbus, and Jianfeng Gao.
2020a. Mapping natural-language problems to
formal-language solutions using structured neural
representations. In Proceedings of the 37th Inter-
national Conference on Machine Learning, ICML
2020, 13-18 July 2020, Virtual Event , volume 119 of
Proceedings of Machine Learning Research , pages
1566‚Äì1575. PMLR.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020b. Tabfact: A large-scale
dataset for table-based fact veriÔ¨Åcation. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net.
Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan
Xiong, Hong Wang, and William Yang Wang. 2020c.
Hybridqa: A dataset of multi-hop question answer-
ing over tabular and textual data. In Proceedings of

=== Page 10 ===
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing: Findings, EMNLP 2020,
Online Event, 16-20 November 2020 , pages 1026‚Äì
1036. Association for Computational Linguistics.
Xinyun Chen, Chen Liang, Adams Wei Yu, Denny
Zhou, Dawn Song, and Quoc V . Le. 2020d. Neu-
ral symbolic reader: Scalable integration of dis-
tributed and symbolic representations for reading
comprehension. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Min-Yuh Day and Chia-Chou Lee. 2016. Deep learn-
ing for Ô¨Ånancial sentiment analysis on Ô¨Ånance news
providers. In 2016 IEEE/ACM International Confer-
ence on Advances in Social Networks Analysis and
Mining, ASONAM 2016, San Francisco, CA, USA,
August 18-21, 2016 , pages 1127‚Äì1134. IEEE Com-
puter Society.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers) , pages 4171‚Äì4186. Association for Computa-
tional Linguistics.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
DROP: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL-
HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,
Volume 1 (Long and Short Papers) , pages 2368‚Äì
2378. Association for Computational Linguistics.
Jingguang Han, Utsab Barman, Jer Hayes, Jinhua Du,
Edward Burgin, and Dadong Wan. 2018. Nextgen
AML: distributed deep learning based language tech-
nologies to augment anti money laundering inves-
tigation. In Proceedings of ACL 2018, Melbourne,
Australia, July 15-20, 2018, System Demonstrations ,
pages 37‚Äì42. Association for Computational Lin-
guistics.
Morten Jerven. 2013. Poor numbers: how we are mis-
led by African development statistics and what to do
about it . Cornell University Press.
Bugeun Kim, Kyung Seo Ki, Donggeon Lee, and Gah-
gene Gweon. 2020. Point to the expression: Solv-
ing algebraic word problems using the expression-
pointer transformer model. In Proceedings of the
2020 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 3768‚Äì3779. Associ-
ation for Computational Linguistics.Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,
Nate Kushman, and Hannaneh Hajishirzi. 2016.
MAWPS: A math word problem repository. In
NAACL HLT 2016, The 2016 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, San Diego California, USA, June 12-17, 2016 ,
pages 1152‚Äì1157. The Association for Computa-
tional Linguistics.
Ann-Christina Lange, Marc Lenglet, and Robert
Seyfert. 2016. Cultures of high-frequency trading:
Mapping the landscape of algorithmic developments
in contemporary Ô¨Ånancial markets. Economy and
Society , 45(2):149‚Äì165.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR , abs/1907.11692.
Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,
and Jun Zhao. 2020. Finbert: A pre-trained Ô¨Ånan-
cial language representation model for Ô¨Ånancial text
mining. In Proceedings of the Twenty-Ninth Inter-
national Joint Conference on ArtiÔ¨Åcial Intelligence,
IJCAI 2020 , pages 4513‚Äì4519. ijcai.org.
Donald MacKenzie. 2008. An engine, not a camera:
How Ô¨Ånancial models shape markets . Mit Press.
Donald MacKenzie, Daniel Beunza, Yuval Millo, and
Juan Pablo Pardo-Guerra. 2012. Drilling through
the allegheny mountains: Liquidity, materiality and
high-frequency trading. Journal of cultural econ-
omy, 5(3):279‚Äì296.
Armineh Nourbakhsh and Grace Bang. 2019. A
framework for anomaly detection using language
modeling, and its applications to Ô¨Ånance. CoRR ,
abs/1908.09156.
Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
Proceedings of the 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 1: Long Papers , pages 1470‚Äì
1480. The Association for Computer Linguistics.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don‚Äôt know: Unanswerable ques-
tions for squad. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2018, Melbourne, Australia, July 15-
20, 2018, Volume 2: Short Papers , pages 784‚Äì789.
Association for Computational Linguistics.

=== Page 11 ===
Alon Talmor and Jonathan Berant. 2018. The web as
a knowledge-base for answering complex questions.
InProceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2018, New Orleans, Louisiana, USA,
June 1-6, 2018, Volume 1 (Long Papers) , pages 641‚Äì
651. Association for Computational Linguistics.
Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and Chin-
Ting Chang. 2013. Financial sentiment analysis
for risk prediction. In Sixth International Joint
Conference on Natural Language Processing, IJC-
NLP 2013, Nagoya, Japan, October 14-18, 2013 ,
pages 802‚Äì808. Asian Federation of Natural Lan-
guage Processing / ACL.
Weikang Wang, Jiajun Zhang, Qian Li, Chengqing
Zong, and Zhifei Li. 2019. Are you for real? de-
tecting identity fraud via dialogue interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019 , pages 1762‚Äì
1771. Association for Computational Linguistics.
Yi Yang, Mark Christopher Siy Uy, and Allen Huang.
2020. Finbert: A pretrained language model for Ô¨Å-
nancial communications. CoRR , abs/2006.08097.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4,
2018 , pages 2369‚Äì2380. Association for Computa-
tional Linguistics.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang, and
Dragomir R. Radev. 2018. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-sql task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, Brussels,
Belgium, October 31 - November 4, 2018 , pages
3911‚Äì3921. Association for Computational Linguis-
tics.
Xinyi Zheng, Doug Burdick, Lucian Popa, Peter
Zhong, and Nancy Xin Ru Wang. 2021. Global ta-
ble extractor (gte): A framework for joint table iden-
tiÔ¨Åcation and cell structure recognition using visual
context. Winter Conference for Applications in Com-
puter Vision (WACV) .
Appendix A: Operation DeÔ¨Ånitions
We describe all the operations in Table 4.Appendix B: Experiment Details
All the validation results of the baselines are shown
in Table 5. The trainings of all models are con-
ducted on TITAN RTX GPUs. All the implementa-
tion and pre-trained models are based on the hug-
gingface transformers library. We use the Adam
optimizer (Kingma and Ba, 2015). The parameter
settings are the following:
Retriever The learning rate is set as 3e-5, with
batch size of 16.
TF-IDF + Single Op We use the TF-IDF from the
Scikit-learn library.
FinQANet The learning rate is set as 1e-5. For
Bert-base, Roberta-base, and Ô¨ÅnBert we use batch
size of 32; For Bert-large and RoBerta-large we use
batch size of 16 due to GPU memory constraints.
Retriever + Seq2seq A bidirectional LSTM is
used for encoding the input, then an LSTM is used
for decoding with attention. Learning rate is set as
1e-3, hidden size as 100.
Retriever + NeRd The parameter settings are the
same as FinQANet.
Pre-Trained Longformer We truncate the maxi-
mum input length as 2,000. The learning rate is set
as 2e-5, with batch size of 16 due to GPU memory
constraints.
For more modeling details refer to our released
code.
Appendix C: Case Studies
Here we provide more case studies with the full in-
put reports. For all the examples the gold evidence
is highlighted in blue.
Appendix D: Annotation Interface
We use Turkle7to build our annotation platform,
which is a Django-based web application that can
run in a local server. Figure 7 and Figure 8 show
our annotation interface. After the annotators Ô¨Ånish
one example, they will use the validation check
button to automatically check the validity of their
inputs.
7https://github.com/hltcoe/turkle

=== Page 12 ===
Name Arguments Output Description
add number1, number2 number add two numbers: number 1 +number 2
subtract number1, number2 number subtract two numbers: number 1 number 2
multiply number1, number2 number multiply two numbers: number 1number 2
divide number1, number2 number multiply two numbers: number 1=number 2
exp number1, number2 number exponential: number 1number 2
greater number1, number2 bool comparison: number 1> number 2
table-sum table header number the summation of one table row
table-average table header number the average of one table row
table-max table header number the maximum number of one table row
table-min table header number the minimum number of one table row
Table 4: DeÔ¨Ånitions of all operations
BaselinesExecution
Accuracy (%)Program
Accuracy (%)
TF-IDF + Single Op 1.65 1.65
Retriever +
Direct Generation0.87 -
Pre-Trained
Longformer (base)23.83 22.56
Retriever + Seq2seq 18.76 17.52
Retriever +
NeRd (BERT-base)47.53 45.37
FinQANet (FinBert) 46.64 44.11
FinQANet (BERT-base) 49.91 47.15
FinQANet (BERT-large) 53.86 50.95
FinQANet (RoBerta-base) 56.27 53.49
FinQANet (RoBerta-large) 61.22 58.05
Table 5: Results on validation set

=== Page 13 ===
Input Report AWK/2014/page_121.pdf 
‚Ä¶ (abbreviate 20 sentences)...  the ppaca effectively changes the tax treatment of federal subsidies paid to sponsors of retiree health benefit plans that provide a benefit 
that is at least actuarially equivalent to the benefits under medicare part d . the acts effectively make the subsidy payments taxable in tax years beginning after december 
31 , 2012 and as a result , the company followed its original accounting for the underfunded status of the other postretirement benefits for the medicare part d adjustment 
and recorded a reduction in deferred tax assets and an increase in its regulatory assets amounting to $ 6348 and $ 6241 at december 31 , 2014 and 2013 , respectively . 
the following table summarizes the changes in the company 2019s gross liability , excluding interest and penalties , for unrecognized tax benefits: .        
balance at january 1 2013 $ 180993 
increases in current period tax position 27229 
decreases in prior period measurement of tax positions -30275 ( 30275 ) 
balance at december 31 2013 $ 177947 
increases in current period tax positions 53818 
decreases in prior period measurement of tax positions -36528 ( 36528 ) 
balance at december 31 2014 $ 195237 
the total balance in the table above does not include interest and penalties of $ 157 and $ 242 as of december 31 , 2014 and 2013 , respectively , which is recorded as a  
component of income tax expense . 
Question:  what was the net change in tax positions in 2014? 
Gold program:  add(53818, -36528), add(#0, 157) 
Retrieved evidence: 
[1] table row: increases in current period tax positions: 27229 ; 
[2] table row: increases in current period tax positions: 53818 ; 
[3] table row: balance at december 31 2014:  $ 195237 ; 
Predicted program:  
subtract(27229, 53818) 
Figure 5: Error case study 1: The net change in the tax position is the sum of the increase and the decrease plus the penalties and
interest. The model lacks this Ô¨Ånance knowledge, thus the retriever fails to retrieve the correct table rows and sentences. Another
challenging point is the table understanding, since in this case, it‚Äôs hard to distinguish the retrieved two table rows for the year
2013 or 2014, using our method that regards each table row as basic unit. The model needs to look at the full table to get this
global information.
Input Report K/2013/page_23.pdf-1‚Ä¶ (abbreviate 12 sentences)... underlying gross margin declined by 180 basis points in 2012 as a result of cost inflation , net of cost savings , and the lower margin structure of the pringles business . underlying sga% ( sga % ) was consistent with 2011 . our underlying gross profit , underlying sga , and underlying operating profit measures are reconciled to the most comparable gaap measure as follows: ( dollars in millions )201320122011reported gross profit ( a )$ 6103$ 5434$ 5152‚Ä¶ abbreviate 10 rows ...underlying operating profit ( d )$ 2098$ 2014$ 2109Question: if 2014 underlying operating profit increases at the same pace as 2013 , what would it be , in millions?Gold program: divide(2098, 2014), multiply(2098, #0)Retrieved evidence:[1] underlying gross margin declined by 110 basis points in 2013 due to the impact of inflation , net of productivity savings , lower operating leverage due to lower sales volume , and the impact of the lower margin structure of the pringles business [2] table row: ( dollars in millions ) The underlying operating profit ( d ) of 2013 is $ 2098 ; The underlying operating profit ( d ) of 2012 is $ 2014 ; The underlying operating profit ( d ) of 2011 is $ 2109 ;[3] during 2013 , we recorded $ 42 million of charges associated with cost reduction initiatives .Predicted program: divide(2098, 2098), multiply(2098, #0)
Figure 6: Error case study 2: Complex numerical reasoning.

=== Page 14 ===
Figure 7: Annotation interface: Display report.
Figure 8: Annotation interface: Annotator input Ô¨Åelds.
